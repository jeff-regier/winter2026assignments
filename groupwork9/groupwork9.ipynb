{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 315, Groupwork 9: Kaggle Galaxy Challenge with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this groupwork assignment, you will continue the task of inferring the number of galaxies in an image. The data is available on Kaggle (link below). Your task is to build a convolutional neural network (CNN) and try to achieve the highest accuracy possible on the test set, the labels for which are hidden from you.\n",
    "\n",
    "To submit your work, please upload the `html` output from executing this notebook to Canvas, **as well as** a `csv` file with your predictions on the test set as a competition submission to Kaggle.\n",
    "\n",
    "The `csv` file should have the following format: `id` for the index of the image (starting with 0), and `label` for the predicted number of galaxies in the image. Code for generating the prediction `csv` is provided in this file.\n",
    "\n",
    "**Kaggle competition link:** [https://www.kaggle.com/t/c7bb7892c2774d61af49f788398b2eec](https://www.kaggle.com/t/c7bb7892c2774d61af49f788398b2eec)\n",
    "\n",
    "**Reference:** [PyTorch CNN Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Import Relevant Packages and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Follow these steps to load the data:\n",
    "\n",
    "1. Download the following files from Kaggle:\n",
    "   - `train_dataset_0.0125.pt`\n",
    "   - `validation_dataset_0.0125.pt`: optional validation set, **not** used for ranking\n",
    "   - `test_images_0.0125.pt`: image-only set used for the competition\n",
    "2. If you're using Google Colab, go to the `Files` tab on the left.\n",
    "3. Create a directory named `data` (or name it something else and change the path below).\n",
    "4. Upload the dataset files to this directory either by clicking the `Upload` button or dragging the files to the directory.\n",
    "\n",
    "Alternatively, you may place the dataset in your Google Drive for persistent storage, and connect to it with the following code:\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "Now the following code block should load the data. You may need to update the file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_counts = torch.load(\"data/train_dataset_0.0125.pt\", weights_only=True)\n",
    "val_images, val_counts = torch.load(\"data/validation_dataset_0.0125.pt\", weights_only=True)\n",
    "\n",
    "print(f\"Training set: {train_images.shape[0]} images\")\n",
    "print(f\"Validation set: {val_images.shape[0]} images\")\n",
    "print(f\"Image dimensions: {train_images.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Since we will be using CNNs, we need to add a channel dimension to the images. PyTorch expects images in the format `(batch, channels, height, width)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.unsqueeze(1).to(device)\n",
    "val_images = val_images.unsqueeze(1).to(device)\n",
    "train_counts = train_counts.to(device)\n",
    "val_counts = val_counts.to(device)\n",
    "\n",
    "print(f\"Training images shape: {train_images.shape}\")\n",
    "print(f\"Validation images shape: {val_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Inspect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Let's display random images from the training and validation sets along with their corresponding galaxy counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(len(train_images), (1,)).item()\n",
    "plt.imshow(train_images[idx].squeeze().cpu(), cmap=\"gray\")\n",
    "plt.title(f\"Training image - Number of galaxies: {train_counts[idx].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(len(val_images), (1,)).item()\n",
    "plt.imshow(val_images[idx].squeeze().cpu(), cmap=\"gray\")\n",
    "plt.title(f\"Validation image - Number of galaxies: {val_counts[idx].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Let's examine the distribution of galaxy counts in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts, frequencies = torch.unique(train_counts, return_counts=True)\n",
    "plt.bar(unique_counts.cpu().numpy(), frequencies.cpu().numpy())\n",
    "plt.xlabel(\"Number of galaxies\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of galaxy counts in training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Problem 1: Define the Model Architecture\n",
    "\n",
    "We will predict the number of galaxies in each image using a CNN. The input to the model will be the image, and the output will be the number of galaxies. Since the galaxy count is a discrete variable (0, 1, 2, ..., 6), we treat this as a classification problem where each count is a class.\n",
    "\n",
    "Design a CNN architecture that you think will work well. Your model must:\n",
    "- Accept input tensors of shape `(batch_size, 1, 50, 50)`\n",
    "- Output tensors of shape `(batch_size, 7)` (logits for 7 classes)\n",
    "\n",
    "Consider using:\n",
    "- Multiple convolutional layers with increasing filter counts\n",
    "- Batch normalization for stable training\n",
    "- Pooling layers to reduce spatial dimensions\n",
    "- Dropout for regularization\n",
    "- Fully connected layers at the end\n",
    "\n",
    "**Hint:** A typical CNN architecture might look like: Conv -> ReLU -> Pool -> Conv -> ReLU -> Pool -> Flatten -> FC -> ReLU -> FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimension and number of classes\n",
    "IMAGE_DIM = 50\n",
    "NUM_CLASSES = 7  # 0 through 6 galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "model = nn.Sequential(\n",
    "    # First conv block: 1 -> 32 channels\n",
    "    nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # 50 -> 25\n",
    "    # Second conv block: 32 -> 64 channels\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # 25 -> 12\n",
    "    # Third conv block: 64 -> 128 channels\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # 12 -> 6\n",
    "    # Flatten and fully connected layers (3 FC layers)\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128 * 6 * 6, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, NUM_CLASSES),\n",
    ").to(device)\n",
    "# END SOLUTION\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_input = torch.randn(4, 1, 50, 50).to(device)\n",
    "test_output = model(test_input)\n",
    "expected_shape = (4, 7)\n",
    "assert test_output.shape == expected_shape, f\"Output shape should be {expected_shape}\"\n",
    "assert model[0].in_channels == 1, \"First conv layer should have 1 input channel\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "test_single = torch.randn(1, 1, 50, 50).to(device)\n",
    "single_output = model(test_single)\n",
    "assert single_output.shape == (1, 7), \"Model should work with batch size 1\"\n",
    "test_batch = torch.randn(32, 1, 50, 50).to(device)\n",
    "batch_output = model(test_batch)\n",
    "assert batch_output.shape == (32, 7), \"Model should work with batch size 32\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Problem 2: Implement the Training Function\n",
    "\n",
    "Implement the training loop. Your training function should:\n",
    "1. Reset the model parameters\n",
    "2. For each epoch:\n",
    "   - Set model to training mode and iterate through the training data\n",
    "   - Compute the loss and perform backpropagation\n",
    "   - Set model to evaluation mode and compute validation loss\n",
    "3. Return the training and validation losses for plotting\n",
    "\n",
    "**Hint:** Use `optimizer.zero_grad()` before computing gradients, `loss.backward()` for backpropagation, and `optimizer.step()` to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model_parameters(model):\n",
    "    \"\"\"Re-initialize all model parameters.\n",
    "\n",
    "    Useful when re-training a model after changing hyperparameters.\n",
    "    \"\"\"\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, \"reset_parameters\"):\n",
    "            module.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, val_dataloader, num_epochs=10):\n",
    "    \"\"\"Train the model and return training/validation losses.\"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    reset_model_parameters(model)\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    # Cosine annealing scheduler for smooth LR decay\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        for images, counts in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, counts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "\n",
    "        train_loss = epoch_train_loss / num_train_batches\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, counts in val_dataloader:\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, counts)\n",
    "                epoch_val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        val_loss = epoch_val_loss / num_val_batches\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"Epoch {epoch:2d} train={train_loss:.3f} val={val_loss:.3f} lr={lr:.5f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    # END SOLUTION\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "import inspect\n",
    "\n",
    "assert callable(train), \"train should be a function\"\n",
    "sig = inspect.signature(train)\n",
    "params = list(sig.parameters.keys())\n",
    "assert \"model\" in params, \"train should accept a model parameter\"\n",
    "assert \"optimizer\" in params, \"train should accept an optimizer parameter\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"train_dataloader\" in params, \"train should accept train_dataloader\"\n",
    "assert \"val_dataloader\" in params, \"train should accept val_dataloader\"\n",
    "assert \"num_epochs\" in params, \"train should accept num_epochs\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Problem 3: Configure Training Hyperparameters\n",
    "\n",
    "Set up your optimizer, batch size, and data loaders. Common optimizer choices include:\n",
    "- `optim.Adam`: Adaptive learning rate, often works well with default settings\n",
    "- `optim.SGD`: Classic gradient descent, may need learning rate tuning\n",
    "\n",
    "The batch size affects:\n",
    "- Training stability (larger batches = more stable gradients)\n",
    "- Memory usage (larger batches = more memory)\n",
    "- Training speed (larger batches = fewer updates per epoch)\n",
    "\n",
    "**Hint:** Start with `Adam`, a learning rate around `1e-3` or `1e-4`, and a batch size of 32 or 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Using SGD with momentum and higher LR works well with cosine annealing\n",
    "learning_rate = 0.02\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "batch_size = 32  # Smaller batch size for more gradient updates\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert optimizer is not None, \"optimizer must be defined\"\n",
    "assert batch_size > 0, \"batch_size must be positive\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(optimizer, \"step\"), \"optimizer must have a step method\"\n",
    "assert hasattr(optimizer, \"zero_grad\"), \"optimizer must have a zero_grad method\"\n",
    "assert batch_size <= 256, \"batch_size should not be too large for memory\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Set up the data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Custom dataset with data augmentation for training\n",
    "class AugmentedGalaxyDataset(Dataset):\n",
    "    \"\"\"Dataset with random augmentations for galaxy images.\"\"\"\n",
    "\n",
    "    def __init__(self, images, labels, *, augment=False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                image = torch.flip(image, dims=[2])\n",
    "            # Random vertical flip\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                image = torch.flip(image, dims=[1])\n",
    "            # Random 90-degree rotation (0, 90, 180, or 270 degrees)\n",
    "            k = torch.randint(0, 4, (1,)).item()\n",
    "            image = torch.rot90(image, k, dims=[1, 2])\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Training set with augmentation, validation set without\n",
    "train_dataset = AugmentedGalaxyDataset(train_images, train_counts, augment=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = AugmentedGalaxyDataset(val_images, val_counts, augment=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Problem 4: Train the Model and Achieve 90% Validation Accuracy\n",
    "\n",
    "Train your model and iterate on your architecture and hyperparameters until you achieve at least **90% accuracy** on the validation set. Watch for:\n",
    "- **Underfitting:** Both training and validation loss remain high\n",
    "- **Overfitting:** Training loss decreases but validation loss increases\n",
    "\n",
    "If your model is not performing well enough, go back to Problems 1-3 and try different:\n",
    "- Model architectures (more/fewer layers, different filter sizes)\n",
    "- Training hyperparameters (learning rate, batch size, number of epochs)\n",
    "- Regularization techniques (dropout rate, weight decay)\n",
    "\n",
    "**Note:** The validation set is not used for ranking, so you can use it to tune your model. The test set is the one used for ranking, where your accuracy might be lower. Think about whether your model is overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "train_losses, val_losses = train(model, optimizer, train_dataloader, val_dataloader, num_epochs=80)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Plot the training and validation losses to diagnose the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Training loss\")\n",
    "plt.plot(val_losses, label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Compute the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions (use batched version if this runs out of memory)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_train_counts = model(train_images)\n",
    "    pred_val_counts = model(val_images)\n",
    "\n",
    "train_accuracy = (pred_train_counts.argmax(dim=1) == train_counts).float().mean().item()\n",
    "val_accuracy = (pred_val_counts.argmax(dim=1) == val_counts).float().mean().item()\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"Validation accuracy: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert train_losses is not None, \"train_losses should be defined after training\"\n",
    "assert val_losses is not None, \"val_losses should be defined after training\"\n",
    "assert len(train_losses) > 0, \"train_losses should not be empty\"\n",
    "assert val_accuracy >= 0.75, f\"Need >= 75% validation accuracy, got {val_accuracy:.2%}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(train_losses) == len(val_losses), \"losses should have same length\"\n",
    "assert all(loss >= 0 for loss in train_losses), \"all losses should be non-negative\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Generate Kaggle Submission\n",
    "\n",
    "Once you are happy with your model's performance, run the code below to generate the submission file for the Kaggle competition.\n",
    "\n",
    "**Note:** You are limited to 3 submissions per day, so be strategic with your submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = torch.load(\"data/test_images_0.0125.pt\", weights_only=True)\n",
    "test_loader = DataLoader(test_images, batch_size=512, shuffle=False)\n",
    "\n",
    "print(f\"Test set: {test_images.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_images in test_loader:\n",
    "        batch_with_channel = batch_images.unsqueeze(1).to(device)\n",
    "        outputs = model(batch_with_channel)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "image_ids = [str(idx) for idx in range(test_images.shape[0])]\n",
    "prediction_df = pd.DataFrame({\"id\": image_ids, \"label\": predictions})\n",
    "\n",
    "prediction_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved to 'submission.csv'\")\n",
    "print(prediction_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
