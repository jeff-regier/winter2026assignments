{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "b70d3b4f643748bd"
   },
   "source": [
    "# Baseline Transformer models for the Galaxy challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "e8367b0e3ca88ea2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "82be9446e732f8a5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "3acf5bcd54ed644e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "62f0643686ff2830",
    "outputId": "431b9c29-2a5a-46d5-d8d8-c83d8c6bb5d8"
   },
   "outputs": [],
   "source": [
    "# cuda setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "3b89b9404ef46565"
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "ddb593a96bd98b2f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Watermarked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "72063fef59f1bdb4"
   },
   "outputs": [],
   "source": [
    "sigma = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "54751a919234788b"
   },
   "outputs": [],
   "source": [
    "offset = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "b79413c69d911125"
   },
   "outputs": [],
   "source": [
    "# alpha = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "7edd29f24ab1b73c"
   },
   "outputs": [],
   "source": [
    "train_images, train_ns = torch.load(\n",
    "    f\"../data/315/watermarked/dataset_train_barsplit_post_{sigma}_{offset}.pt\"\n",
    ")\n",
    "test_images, test_ns = torch.load(\n",
    "    f\"../data/315/watermarked/dataset_test_barsplit_post_{sigma}_{offset}.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "962d6b0e6f289d8f"
   },
   "outputs": [],
   "source": [
    "# train_images, train_ns = torch.load(\n",
    "#     f\"../data/315/watermarked/dataset_train_barsplit_{sigma}_{offset}.pt\"\n",
    "# )\n",
    "# test_images, test_ns = torch.load(\n",
    "#     f\"../data/315/watermarked/dataset_test_barsplit_{sigma}_{offset}.pt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "1acb8ba294d7a5f9"
   },
   "outputs": [],
   "source": [
    "# train_images, train_ns = torch.load(\n",
    "#     f\"../data/315/watermarked/dataset_train_barrandom_{sigma}_{alpha}.pt\"\n",
    "# )\n",
    "# test_images, test_ns = torch.load(\n",
    "#     f\"../data/315/watermarked/dataset_test_barrandom_{sigma}_{alpha}.pt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "ee201d085cd3eeec",
    "outputId": "d1605802-c9eb-48bb-aaaf-41f2b95b2e1c"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    train_images.shape,\n",
    "    train_ns.shape,\n",
    "    test_images.shape,\n",
    "    test_ns.shape,\n",
    ")  # , test_images_kaggle.shape, test_ns_kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "bce0774b3a527543"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.unsqueeze(1)\n",
    "test_images = test_images.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "7b3421b2dc21da09",
    "outputId": "44c9c097-c203-44f2-f1e3-53110565f291"
   },
   "outputs": [],
   "source": [
    "train_images.shape, train_ns.shape, train_images.dtype, test_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "e30ea8bedbdf9d5e"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_images, train_ns)\n",
    "test_dataset = TensorDataset(test_images, test_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "4cd0ce524692f427"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "16623644aacfb714"
   },
   "outputs": [],
   "source": [
    "# kaggle_dataset = TensorDataset(test_images_kaggle, test_ns_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "37e1049f263f0a76"
   },
   "outputs": [],
   "source": [
    "# NOTE: don't shuffle for the kaggle dataset\n",
    "# kaggle_loader = DataLoader(kaggle_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "8c93fca8f718303e",
    "outputId": "60622772-afdb-4ff1-c85a-1ad9d2295368"
   },
   "outputs": [],
   "source": [
    "i = torch.randint(1, 10000)\n",
    "plt.imshow(train_images[i].squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"{train_ns[i]}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "16aa8c63217c8768",
    "outputId": "256bf2f1-743d-4ca0-c782-66f26727237f"
   },
   "outputs": [],
   "source": [
    "i = torch.randint(1, 2500)\n",
    "plt.imshow(test_images[i].squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"{test_ns[i]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "b40eccbe2b6e5475",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "e97af5263cc24b33"
   },
   "outputs": [],
   "source": [
    "dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "6263ea5657e841ea"
   },
   "outputs": [],
   "source": [
    "n_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "c3044e9bf220bd6a"
   },
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "b60944d292800fac"
   },
   "outputs": [],
   "source": [
    "model_path = \"../res/models/galaxy_challenge2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "101b5db25375422e"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs, lr, model_save_path):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=num_epochs,\n",
    "        desc=f\"Epoch: 0/{num_epochs} | Train Loss: N/A | Test Loss: N/A\",\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_running_loss += loss.item()\n",
    "\n",
    "        test_loss = test_running_loss / len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Epoch: {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} \"\n",
    "            f\"| Test Loss: {test_loss:.4f}\"\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = f\"{model_save_path}_epoch_{epoch + 1}.pth\"\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "d29286ef7eb8e501"
   },
   "outputs": [],
   "source": [
    "def plot_losses_and_evaluate(model, train_loader, test_loader, train_losses, test_losses):\n",
    "    # Plot losses\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.plot(test_losses, label=\"test\")\n",
    "    plt.ylim(0, max(train_losses + test_losses))\n",
    "    plt.legend()\n",
    "    plt.title(\"Train vs Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate accuracies\n",
    "    model.eval()\n",
    "\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = (\n",
    "                images.to(next(model.parameters()).device),\n",
    "                labels.to(next(model.parameters()).device),\n",
    "            )\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    test_accuracy = 100 * correct_test / total_test if total_test else 0\n",
    "\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = (\n",
    "                images.to(next(model.parameters()).device),\n",
    "                labels.to(next(model.parameters()).device),\n",
    "            )\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "    train_accuracy = 100 * correct_train / total_train if total_train else 0\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "3d2e197bf3407532"
   },
   "source": [
    "# ChatGPT CNN from previous challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "f1f5e30135ebe841"
   },
   "outputs": [],
   "source": [
    "class StarCounterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input shape: (batch_size, 1, 50, 50)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=16, kernel_size=3, padding=1\n",
    "        )  # (16, 50, 50)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # (16, 25, 25)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # (32, 25, 25)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # (32, 12, 12)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # (64, 12, 12)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # (64, 6, 6)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, 7)  # 7 output classes for 0-6 stars\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "1fac27bde793a83d"
   },
   "outputs": [],
   "source": [
    "model_cgpt = StarCounterCNN()\n",
    "model_cgpt = model_cgpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "db1fd551db86d0a4",
    "outputId": "81f85c36-1ff9-49e4-e7bc-91fec5ba1d7b"
   },
   "outputs": [],
   "source": [
    "train_losses, test_losses = train_model(\n",
    "    model_cgpt,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs,\n",
    "    lr,\n",
    "    f\"{model_path}cgpt_barsplit_post_{sigma}_{offset}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "fc3ab67423a01343"
   },
   "source": [
    "## Split watermark, post-noise $\\sigma = 0.03$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "32bbe81a7ed46644",
    "outputId": "0b843a93-72bc-4358-a84d-52cdc2ff3477"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_cgpt, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "5da6567136d8154d"
   },
   "source": [
    "## Split watermark, post-noise $\\sigma = 0.03$, `offset = 0.05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "ff44afb1c5dada5",
    "outputId": "04bf9943-013f-49f1-9d0e-f862c6838c54"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_cgpt, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "c0aaad689631ce8d"
   },
   "source": [
    "## Split watermark, $\\sigma = 0.04$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "fd2e623340cad624",
    "outputId": "6b68a231-a1fa-4cf9-ef16-6062a73287ca"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_cgpt, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "b68f06fc05149575"
   },
   "source": [
    "## Split watermark, $\\sigma = 0.02$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "63e02bd38a73846e",
    "outputId": "e9de6ea5-b7cb-4a94-c5f5-0306474e057d"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_cgpt, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "5dbe747e-149d-4871-af49-32b99fbe2110"
   },
   "source": [
    "## Bar watermark, $\\sigma = 0.05$, $\\alpha = 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "f59cb44b-ddda-4bc6-bf5b-1b931da8044c",
    "outputId": "3c313391-3742-4eb2-d300-7d9fc9c40c75"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_cgpt, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "2b776b0bea85f4d5"
   },
   "source": [
    "## Bar watermark, $\\sigma = 0.02$, $\\alpha = 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "a5dfb840aa0062fd",
    "outputId": "24adfeda-8877-48b0-9421-224119b38dad"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_cgpt, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "8b05d64e550d6787"
   },
   "source": [
    "## $\\sigma = 0.02$, `offset = 0.005`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "2110bea079c89fd",
    "outputId": "fb89a307-91d1-4c05-b5e9-d6f41b16c4f8"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.ylim(0, max(train_losses + test_losses))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "id": "a34709a159ca2a7b",
    "outputId": "74c62abe-239d-41ff-da67-1b5055175fcf"
   },
   "outputs": [],
   "source": [
    "model_cgpt.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_cgpt(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_cgpt(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct_train / total_train:.2f}%\")\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "430e50f434002bae"
   },
   "source": [
    "## $\\sigma = 0.0125$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "bfef3471577c236e",
    "outputId": "3f215988-8555-441f-b2c4-09e61ba2735c"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.ylim(0, max(train_losses + test_losses))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "483c9096ad720da5",
    "outputId": "a4028847-0e4f-40c1-b9e4-0552a4c5ca16"
   },
   "outputs": [],
   "source": [
    "model_cgpt.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_cgpt(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_cgpt(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct_train / total_train:.2f}%\")\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "id": "5b0b78746c45670"
   },
   "source": [
    "# VGG from prev challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "508bd21229da9b91"
   },
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "id": "f886385d424b33c4"
   },
   "outputs": [],
   "source": [
    "def make_vgg11(device):\n",
    "    model = models.vgg11_bn(pretrained=False)\n",
    "\n",
    "    # Need the first conv layer to accept 1 channel instead of 3.\n",
    "    model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    # Modify the classifier from CIFAR10\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(256, 7)\n",
    "    )\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "id": "8ceec3cba9dbe72f"
   },
   "outputs": [],
   "source": [
    "model_vgg = make_vgg11(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "id": "ea26bced5a90163f",
    "outputId": "90ddc977-9cbc-40ef-c819-4b0436e939a1"
   },
   "outputs": [],
   "source": [
    "train_losses, test_losses = train_model(\n",
    "    model_vgg,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs,\n",
    "    lr=1e-3,\n",
    "    model_save_path=f\"{model_path}vgg11_barsplit_post_{sigma}_{offset}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {
    "id": "eb644ba96144e96f"
   },
   "source": [
    "## Split watermark, post-noise $\\sigma = 0.03$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "id": "34d54b478ca8745d",
    "outputId": "094c2a3e-fbab-429a-cb18-0c73e52bf762"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_vgg, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "de08b22b8259d93e"
   },
   "source": [
    "## Split watermark, $\\sigma = 0.04$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "id": "6f8cec8b0890297c",
    "outputId": "f935a835-3445-4e9d-b72c-caeb2ecb7437"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_vgg, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "id": "d28a6fee22c4e6b6"
   },
   "source": [
    "## Bar watermark, $\\sigma = 0.05$, $\\alpha = 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "id": "20394b35700bd2b9",
    "outputId": "fd3303b1-5813-4ca0-8505-dad09e930890"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_vgg, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "id": "6bad31e03570e853"
   },
   "source": [
    "## Bar watermark, $\\sigma = 0.02$, $\\alpha = 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "id": "38036f2da9bffff6",
    "outputId": "8518ec17-5282-4281-a287-ed4cd668ed14"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_vgg, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "id": "565fcce3bf5fe41"
   },
   "source": [
    "## $\\sigma = 0.0125$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "id": "e7e09677a2d9c2a3",
    "outputId": "c3b6e36e-43ab-4b20-b0d0-632509df9d05"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.ylim(0, max(train_losses + test_losses))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "id": "80a2916045a6b902",
    "outputId": "1d39ec33-ddae-4ad3-e945-7309fb50cac7"
   },
   "outputs": [],
   "source": [
    "model_vgg.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_vgg(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_vgg(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct_train / total_train:.2f}%\")\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "id": "ed2d096aaf7d5fbd"
   },
   "source": [
    "# CNN + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "id": "b58e294e836aa7bc"
   },
   "outputs": [],
   "source": [
    "class GalaxyTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=7, embed_dim=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((16, 16)),\n",
    "        )\n",
    "\n",
    "        self.embedding_proj = nn.Conv2d(64, embed_dim, kernel_size=1)  # -> (B, embed_dim, 16, 16)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.embedding_proj(x)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = x.view(B, C, H * W)\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.mean(dim=0)\n",
    "\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {
    "id": "e764b10067b0ffe6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "id": "71ec3d4880979d02"
   },
   "outputs": [],
   "source": [
    "model_small = GalaxyTransformer(\n",
    "    num_classes=n_classes, embed_dim=64, num_heads=2, num_layers=2, dropout=0.05\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "id": "d9d2b24ce16498d7"
   },
   "outputs": [],
   "source": [
    "model_normal = GalaxyTransformer(\n",
    "    num_classes=n_classes, embed_dim=128, num_heads=4, num_layers=3, dropout=0.025\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "id": "f3fc38a4979be2bb",
    "outputId": "6a7ebcad-7fc8-455c-9889-6e6e0a9df064"
   },
   "outputs": [],
   "source": [
    "train_losses, test_losses = train_model(\n",
    "    model_small,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    100,\n",
    "    lr=3e-4,\n",
    "    model_save_path=f\"{model_path}CNN_transformer__{sigma}_{alpha}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {
    "id": "520bd86a24a2c980"
   },
   "source": [
    "## Split watermark, post-noise $\\sigma = 0.03$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "id": "cac8e1e3e561d5d8",
    "outputId": "519515f0-1a09-44b1-d601-0016974e4a54"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_small, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "8ad56cb556240aaa"
   },
   "source": [
    "## Split watermark, $\\sigma = 0.04$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "id": "74b08d70d4e9355d",
    "outputId": "bab171e4-a772-4414-f6c1-7c135cafa66e"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_normal, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "id": "d3af7cd00075bc66",
    "outputId": "95162b19-8ef7-4523-cdba-4085dbc751bd"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_small, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {
    "id": "28255860f369dd82"
   },
   "source": [
    "## Split watermark, $\\sigma = 0.02$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "id": "110650932776975b",
    "outputId": "90a3d09e-6fb0-43e2-c42b-7df0451f28f8"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_small, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {
    "id": "55041b112e2d9772"
   },
   "source": [
    "## Bar watermark, $\\sigma = 0.05$, $\\alpha = 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "id": "1c91df7bd9c3c8bc",
    "outputId": "50c1da7e-1d84-4b14-a567-1ac9666aac0f"
   },
   "outputs": [],
   "source": [
    "model = GalaxyTransformer(\n",
    "    num_classes=n_classes, embed_dim=128, num_heads=4, num_layers=3, dropout=0.025\n",
    ").to(device)\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{model_path}CNN_transformer_normal_barrandom_{sigma}_{alpha}_epoch_50.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {
    "id": "e5d78b7c330708d3",
    "outputId": "2dae5f61-bbdf-432f-bd09-e728ac944baf"
   },
   "outputs": [],
   "source": [
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = (\n",
    "            images.to(next(model.parameters()).device),\n",
    "            labels.to(next(model.parameters()).device),\n",
    "        )\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * correct_test / total_test if total_test else 0\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = (\n",
    "            images.to(next(model.parameters()).device),\n",
    "            labels.to(next(model.parameters()).device),\n",
    "        )\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "train_accuracy = 100 * correct_train / total_train if total_train else 0\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {
    "id": "36ce7fb0a2331962"
   },
   "source": [
    "## Bar watermark, $\\sigma = 0.02$, $\\alpha = 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "id": "56973ad30fb0a10e",
    "outputId": "978312e6-ab45-42f9-ce7f-e6e0613862c5"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(model_normal, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {
    "id": "ccc1b8027bac1f9"
   },
   "source": [
    "## $\\sigma = 0.02$, `offset = 0.005`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "id": "5f4dc9d05d210e5f",
    "outputId": "9fae2284-0e5a-45f8-b0fb-012ef79a1b68"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.ylim(0, max(train_losses + test_losses))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "id": "af2a29a8898fd6a7"
   },
   "outputs": [],
   "source": [
    "model = GalaxyTransformer(\n",
    "    num_classes=n_classes, embed_dim=128, num_heads=4, num_layers=3, dropout=0.025\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "id": "736be390ff3dab2c",
    "outputId": "80cb877a-8381-483a-817a-adddc4675206"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(f\"{model_path}CNN_transformer_normal2_{sigma}_{offset}_epoch_70.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {
    "id": "c1564b1fd2171f5e",
    "outputId": "74ec558b-e7aa-48fc-8699-d60209a1114e"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct_train / total_train:.2f}%\")\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {
    "id": "e2104d6a6a98cd6b"
   },
   "source": [
    "## $\\sigma = 0.0125$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "id": "e5dbed3e90013112",
    "outputId": "0880c8e3-0ba7-4a98-c196-1d7df0dcb988"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.ylim(0, max(train_losses + test_losses))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "id": "7a558430f86c26f2",
    "outputId": "69764a74-0c76-400c-f4a8-6d2f4b433607"
   },
   "outputs": [],
   "source": [
    "model_small.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_small(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_small(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct_train / total_train:.2f}%\")\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "id": "6fcdbbc8fc936bae"
   },
   "source": [
    "# Pure Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "id": "d9bf5a4e7c158e0"
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=50, patch_size=5, in_chans=1, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class PureTransformerClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=50,\n",
    "        patch_size=5,\n",
    "        in_chans=1,\n",
    "        num_classes=7,\n",
    "        embed_dim=128,\n",
    "        depth=6,\n",
    "        num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Define a learnable class token.\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        # Define positional embeddings for patches + class token.\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create Transformer encoder layers.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.constant_(self.head.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Prepend the class token to the patch embeddings.\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add positional embeddings.\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]\n",
    "        return self.head(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "id": "f9a0e0f6cc22c396"
   },
   "outputs": [],
   "source": [
    "pure_small = PureTransformerClassifier(\n",
    "    img_size=50,\n",
    "    patch_size=8,\n",
    "    in_chans=1,\n",
    "    num_classes=7,\n",
    "    embed_dim=64,\n",
    "    depth=3,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "id": "7a2c777da9cb015e",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "fee342c8-aa7a-41f7-c6a9-ba60294007fe"
   },
   "outputs": [],
   "source": [
    "train_losses, test_losses = train_model(\n",
    "    pure_small,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs=60,\n",
    "    lr=5e-4,\n",
    "    model_save_path=f\"{model_path}pure_transformer_small_barsplit_post_{sigma}_{offset}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {
    "id": "cbb10d2e6a204fa2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {
    "id": "c20ea791e982b3c3"
   },
   "outputs": [],
   "source": [
    "pure_normal = PureTransformerClassifier(\n",
    "    img_size=50,\n",
    "    patch_size=4,\n",
    "    in_chans=1,\n",
    "    num_classes=7,\n",
    "    embed_dim=128,\n",
    "    depth=6,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.05,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "id": "fcce9a1f316d6c07",
    "outputId": "160fd2d8-aa65-4a52-82c2-4f7f8fd3dbb3"
   },
   "outputs": [],
   "source": [
    "train_losses_pure_normal, test_losses_pure_normal = train_model(\n",
    "    pure_normal,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs=60,\n",
    "    lr=5e-4,\n",
    "    model_save_path=f\"{model_path}pure_transformer_normal_barsplit_{sigma}_{offset}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "id": "4e1af06aea1cb409"
   },
   "source": [
    " ## Split watermark, post-noise $\\sigma = 0.03$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {
    "id": "3a2a11eea5d7158b",
    "outputId": "8d171eea-a48d-4409-ff6c-5beac59761f0"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(pure_small, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {
    "id": "89e97b37e7754e98"
   },
   "source": [
    "## Split watermark, $\\sigma = 0.04$, `offset = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {
    "id": "230deb0b435a59b2",
    "outputId": "39d35698-1ad9-4f23-b887-6b3547fd55f0"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(pure_small, train_loader, test_loader, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {
    "id": "618143fa13efd4b4",
    "outputId": "47905b4b-1eb4-48ad-9fae-e74338a51ed0"
   },
   "outputs": [],
   "source": [
    "plot_losses_and_evaluate(\n",
    "    pure_normal,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    train_losses_pure_normal,\n",
    "    test_losses_pure_normal,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {
    "id": "f38c86800d60a1b5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
