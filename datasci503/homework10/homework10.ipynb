{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 503, Homework 10: Neural Networks and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This assignment covers **network architecture** (layers, activation functions, parameter counting), the **softmax function** for multi-class classification, **convolutional neural networks (CNNs)** for image data, and **gradient descent** optimization.\n",
    "\n",
    "**Resources:**\n",
    "- [PyTorch Autograd Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) - Automatic differentiation for gradient computation\n",
    "- [ISLP Chapter 10](https://www.statlearning.com/) - Deep Learning chapter from Introduction to Statistical Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Neural Network Architecture (ISLP 10.1, parts a, b, and d)\n",
    "\n",
    "Consider a neural network with two hidden layers: p = 4 input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output.\n",
    "\n",
    "(10.1a) Draw a picture of the network, similar to Figures 10.1 or 10.4 in ISLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from([r\"$I_1$\", r\"$I_2$\", r\"$I_3$\", r\"$I_4$\"], layer=0)\n",
    "G.add_nodes_from([r\"$H^{(1)}_1$\", r\"$H^{(1)}_2$\"], layer=1)\n",
    "G.add_nodes_from([r\"$H^{(2)}_1$\", r\"$H^{(2)}_2$\", r\"$H^{(2)}_3$\"], layer=2)\n",
    "G.add_node(r\"$O$\", layer=3)\n",
    "\n",
    "edges = [\n",
    "    (r\"$I_1$\", r\"$H^{(1)}_1$\"),\n",
    "    (r\"$I_1$\", r\"$H^{(1)}_2$\"),\n",
    "    (r\"$I_2$\", r\"$H^{(1)}_1$\"),\n",
    "    (r\"$I_2$\", r\"$H^{(1)}_2$\"),\n",
    "    (r\"$I_3$\", r\"$H^{(1)}_1$\"),\n",
    "    (r\"$I_3$\", r\"$H^{(1)}_2$\"),\n",
    "    (r\"$I_4$\", r\"$H^{(1)}_1$\"),\n",
    "    (r\"$I_4$\", r\"$H^{(1)}_2$\"),\n",
    "    (r\"$H^{(1)}_1$\", r\"$H^{(2)}_1$\"),\n",
    "    (r\"$H^{(1)}_1$\", r\"$H^{(2)}_2$\"),\n",
    "    (r\"$H^{(1)}_1$\", r\"$H^{(2)}_3$\"),\n",
    "    (r\"$H^{(1)}_2$\", r\"$H^{(2)}_1$\"),\n",
    "    (r\"$H^{(1)}_2$\", r\"$H^{(2)}_2$\"),\n",
    "    (r\"$H^{(1)}_2$\", r\"$H^{(2)}_3$\"),\n",
    "    (r\"$H^{(2)}_1$\", r\"$O$\"),\n",
    "    (r\"$H^{(2)}_2$\", r\"$O$\"),\n",
    "    (r\"$H^{(2)}_3$\", r\"$O$\"),\n",
    "]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "pos = nx.multipartite_layout(G, subset_key=\"layer\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=2000,\n",
    "    node_color=\"skyblue\",\n",
    "    font_size=13,\n",
    "    font_weight=\"bold\",\n",
    "    arrowstyle=\"-|>\",\n",
    "    arrowsize=10,\n",
    ")\n",
    "plt.title(\"Neural Network Architecture: 2 Hidden Layers\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert (\n",
    "    G.number_of_nodes() == 10\n",
    "), \"Network should have 10 nodes (4 input + 2 hidden1 + 3 hidden2 + 1 output)\"\n",
    "assert G.number_of_edges() == 17, \"Network should have 17 edges\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "input_nodes = [n for n, d in G.nodes(data=True) if d.get(\"layer\") == 0]\n",
    "hidden1_nodes = [n for n, d in G.nodes(data=True) if d.get(\"layer\") == 1]\n",
    "hidden2_nodes = [n for n, d in G.nodes(data=True) if d.get(\"layer\") == 2]\n",
    "output_nodes = [n for n, d in G.nodes(data=True) if d.get(\"layer\") == 3]\n",
    "assert len(input_nodes) == 4, \"Should have 4 input nodes\"\n",
    "assert len(hidden1_nodes) == 2, \"Should have 2 nodes in first hidden layer\"\n",
    "assert len(hidden2_nodes) == 3, \"Should have 3 nodes in second hidden layer\"\n",
    "assert len(output_nodes) == 1, \"Should have 1 output node\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "(10.1b) Write out an expression for $f(X)$, assuming ReLU activation functions. Be as explicit as you can!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "We specify the following notations for clarity:\n",
    "- ReLU Activation Function: $\\text{ReLU}(z) = \\max(0, z)$\n",
    "- Weights: $W^{(1)}, W^{(2)}, W^{(3)}$\n",
    "- Biases: $b^{(1)}, b^{(2)}, b^{(3)}$\n",
    "- Input: $X = [x_1, x_2, x_3, x_4]^T$\n",
    "\n",
    "We then have the following equations for each layer:\n",
    "\n",
    "1. **First Hidden Layer:**\n",
    "   $$z^{(1)} = W^{(1)} X + b^{(1)}$$\n",
    "   $$a^{(1)} = \\text{ReLU}(z^{(1)})$$\n",
    "\n",
    "2. **Second Hidden Layer:**\n",
    "   $$z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}$$\n",
    "   $$a^{(2)} = \\text{ReLU}(z^{(2)})$$\n",
    "\n",
    "3. **Output Layer:**\n",
    "   $$z^{(3)} = W^{(3)} a^{(2)} + b^{(3)}$$\n",
    "   $$f(X) = z^{(3)}$$\n",
    "\n",
    "Combining these, we have:\n",
    "$$f(X) = W^{(3)} \\cdot \\text{ReLU}(W^{(2)} \\cdot \\text{ReLU}(W^{(1)} X + b^{(1)}) + b^{(2)}) + b^{(3)}$$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "(10.1d) How many parameters are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "There are **23 parameters** in total. We can compute this from the dimensions of the weight matrices and bias vectors:\n",
    "\n",
    "- $W^{(1)}$: $4 \\times 2 = 8$ weights, $b^{(1)}$: 2 biases\n",
    "- $W^{(2)}$: $2 \\times 3 = 6$ weights, $b^{(2)}$: 3 biases\n",
    "- $W^{(3)}$: $3 \\times 1 = 3$ weights, $b^{(3)}$: 1 bias\n",
    "\n",
    "Total: $(4 \\times 2) + 2 + (2 \\times 3) + 3 + (3 \\times 1) + 1 = 8 + 2 + 6 + 3 + 3 + 1 = 23$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Softmax Invariance (ISLP 10.2)\n",
    "\n",
    "Consider the softmax function in (10.13) (see also (4.13) on page 145 of ISLP) for modeling multinomial probabilities.\n",
    "\n",
    "(a) In (10.13), show that if we add a constant $c$ to each of the $z_i$, then the probability is unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "From the softmax function:\n",
    "$$\\text{Pr}(Y=m|X) = \\frac{e^{z_m}}{\\sum_{k=1}^K e^{z_k}}$$\n",
    "\n",
    "We consider adding a constant $c$ to each $z_i$ to get $z_i' = z_i + c$. The corresponding softmax function becomes:\n",
    "$$\\text{Pr}(Y=m|X) = \\frac{e^{z_m + c}}{\\sum_{k=1}^K e^{z_k + c}}$$\n",
    "\n",
    "We can factor out $e^c$ from both numerator and denominator:\n",
    "$$\\text{Pr}(Y=m|X) = \\frac{e^c \\cdot e^{z_m}}{e^c \\cdot \\sum_{k=1}^K e^{z_k}} = \\frac{e^{z_m}}{\\sum_{k=1}^K e^{z_k}}$$\n",
    "\n",
    "Therefore, the probability is unchanged.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "(b) In (4.13), show that if we add constants $c_j$, $j = 0,1,\\ldots,p$, to each of the corresponding coefficients for each of the classes, then the predictions at any new point $x$ are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Given the softmax probability:\n",
    "$$\\Pr(Y = k | X = x) = \\frac{e^{\\beta_{0k} + \\beta_{1k}x_1 + \\ldots + \\beta_{pk}x_p}}{\\sum_{l=1}^K e^{\\beta_{0l} + \\beta_{1l}x_1 + \\ldots + \\beta_{pl}x_p}}$$\n",
    "\n",
    "We add new coefficients $\\beta'_{jk} = \\beta_{jk} + c_j$ for all classes $k$. The new function becomes:\n",
    "$$\\Pr(Y = k | X = x) = \\frac{e^{(\\beta_{0k} + c_0) + (\\beta_{1k} + c_1)x_1 + \\ldots + (\\beta_{pk} + c_p)x_p}}{\\sum_{l=1}^K e^{(\\beta_{0l} + c_0) + (\\beta_{1l} + c_1)x_1 + \\ldots + (\\beta_{pl} + c_p)x_p}}$$\n",
    "\n",
    "We can factor out the common terms $e^{c_0 + c_1 x_1 + \\ldots + c_p x_p}$ from both numerator and denominator:\n",
    "$$\\Pr(Y = k | X = x) = \\frac{e^{c_0 + c_1 x_1 + \\ldots + c_p x_p} \\cdot e^{\\beta_{0k} + \\beta_{1k}x_1 + \\ldots + \\beta_{pk}x_p}}{e^{c_0 + c_1 x_1 + \\ldots + c_p x_p} \\cdot \\sum_{l=1}^K e^{\\beta_{0l} + \\beta_{1l}x_1 + \\ldots + \\beta_{pl}x_p}}$$\n",
    "\n",
    "The common factor cancels, yielding the original probability:\n",
    "$$\\Pr(Y = k | X = x) = \\frac{e^{\\beta_{0k} + \\beta_{1k}x_1 + \\ldots + \\beta_{pk}x_p}}{\\sum_{l=1}^K e^{\\beta_{0l} + \\beta_{1l}x_1 + \\ldots + \\beta_{pl}x_p}}$$\n",
    "\n",
    "Therefore, the predictions at any new point $x$ remain unchanged.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "This shows that the softmax function is over-parametrized. However, regularization and SGD typically constrain the solutions so that this is not a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** CNN Parameter Counting (ISLP 10.4)\n",
    "\n",
    "Consider a CNN that takes in 32 x 32 grayscale images and has a single convolution layer with three 5 x 5 convolution filters (without boundary padding).\n",
    "\n",
    "(b) How many parameters are in this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "There are **78 parameters** in this model.\n",
    "\n",
    "Each 5 x 5 filter has $5 \\times 5 = 25$ weights plus 1 bias = 26 parameters.\n",
    "With 3 filters: $26 \\times 3 = 78$ parameters.\n",
    "\n",
    "Equivalently: $(5 \\times 5 \\times 3) + 3 = 75 + 3 = 78$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "(c) Explain how this model can be thought of as an ordinary feed-forward neural network with the individual pixels as inputs, and with constraints on the weights in the hidden units. What are the constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "We can view the CNN as a feed-forward network with the following characteristics:\n",
    "\n",
    "1. **Input layer**: Each pixel of the $32 \\times 32$ image is an individual input unit, giving $32 \\times 32 = 1024$ input units.\n",
    "\n",
    "2. **Hidden layer**: Each unit in the $28 \\times 28$ output feature map corresponds to a hidden neuron. With 3 filters, we have $28 \\times 28 \\times 3 = 2352$ hidden units.\n",
    "\n",
    "3. **Local connectivity constraint**: Each hidden neuron is connected to only a $5 \\times 5$ patch of 25 pixels in the input image (rather than all 1024 inputs).\n",
    "\n",
    "4. **Weight sharing constraint**: The same $5 \\times 5 = 25$ weights are used for every neuron within a single filter's feature map. Each filter's weight matrix is reused 784 times (once for each of the $28 \\times 28$ output positions).\n",
    "\n",
    "These constraints dramatically reduce the number of parameters compared to a fully-connected network.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "(d) If there were no constraints, then how many weights would there be in the ordinary feed-forward neural network in (c)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Without constraints, we would have a fully-connected layer from 1024 inputs to 2352 hidden units.\n",
    "\n",
    "Number of weights: $(32 \\times 32) \\times (28 \\times 28 \\times 3) = 1024 \\times 2352 = 2,408,448$\n",
    "\n",
    "This is over 30,000 times more parameters than the CNN with weight sharing!\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Gradient Descent Visualization (ISLP 10.6)\n",
    "\n",
    "Consider the simple function $R(\\beta) = \\sin(\\beta) + \\beta/10$.\n",
    "\n",
    "(a) Draw a graph of this function over the range $\\beta \\in [-6, 6]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def objective_function(beta):\n",
    "    \"\"\"Compute R(beta) = sin(beta) + beta/10.\"\"\"\n",
    "    return np.sin(beta) + beta / 10\n",
    "\n",
    "\n",
    "beta_values = np.linspace(-6, 6, 400)\n",
    "r_values = objective_function(beta_values)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(beta_values, r_values, label=r\"$R(\\beta) = \\sin(\\beta) + \\frac{\\beta}{10}$\")\n",
    "plt.title(r\"Plot of the Function $R(\\beta)$\")\n",
    "plt.xlabel(r\"$\\beta$\")\n",
    "plt.ylabel(r\"$R(\\beta)$\")\n",
    "plt.grid(ls=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(beta_values) == 400, \"Should have 400 points for smooth plotting\"\n",
    "assert beta_values[0] == -6 and beta_values[-1] == 6, \"Beta range should be [-6, 6]\"\n",
    "assert callable(objective_function), \"objective_function should be callable\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "test_beta = np.array([0, np.pi / 2, -np.pi / 2])\n",
    "expected = np.sin(test_beta) + test_beta / 10\n",
    "assert np.allclose(\n",
    "    objective_function(test_beta), expected\n",
    "), \"objective_function should compute sin(beta) + beta/10\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "(b) What is the derivative of this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The derivative of $R(\\beta) = \\sin(\\beta) + \\frac{\\beta}{10}$ is:\n",
    "\n",
    "$$R'(\\beta) = \\cos(\\beta) + \\frac{1}{10}$$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "(c) Given $\\beta^0 = 2.3$, run gradient descent for 50 iterations to find a local minimum of $R(\\beta)$ using a learning rate of $\\rho = 0.1$. Show each of $\\beta^0, \\beta^1, \\ldots$ in your plot, as well as the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def objective_function_torch(beta):\n",
    "    \"\"\"PyTorch version of R(beta) for autograd.\"\"\"\n",
    "    return torch.sin(beta) + beta / 10\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "beta = torch.tensor(2.3, requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "num_iterations = 50\n",
    "\n",
    "beta_history = [beta.item()]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    loss = objective_function_torch(beta)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        beta -= learning_rate * beta.grad\n",
    "        beta_history.append(beta.item())\n",
    "\n",
    "    beta.grad.zero_()\n",
    "\n",
    "final_beta = beta.item()\n",
    "\n",
    "# Plot\n",
    "beta_grid = np.linspace(-6, 6, 400)\n",
    "r_values = np.sin(beta_grid) + beta_grid / 10\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(beta_grid, r_values, label=r\"$R(\\beta) = \\sin(\\beta) + \\frac{\\beta}{10}$\", color=\"blue\")\n",
    "plt.scatter(\n",
    "    beta_history,\n",
    "    np.sin(np.array(beta_history)) + np.array(beta_history) / 10,\n",
    "    color=\"red\",\n",
    "    alpha=0.3,\n",
    "    label=\"Gradient Descent Steps\",\n",
    ")\n",
    "plt.scatter(\n",
    "    final_beta,\n",
    "    np.sin(final_beta) + final_beta / 10,\n",
    "    color=\"green\",\n",
    "    s=100,\n",
    "    label=f\"Local Minimum: {final_beta:.4f}\",\n",
    ")\n",
    "plt.title(r\"Gradient Descent on $R(\\beta)$ starting from $\\beta^0 = 2.3$\")\n",
    "plt.xlabel(r\"$\\beta$\")\n",
    "plt.ylabel(r\"$R(\\beta)$\")\n",
    "plt.legend()\n",
    "plt.grid(ls=\"--\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Local minimum found at beta = {final_beta:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(beta_history) > 1, \"Should have multiple gradient descent steps\"\n",
    "assert abs(beta_history[0] - 2.3) < 0.001, \"Should start from beta^0 = 2.3\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# The local minimum near beta=2.3 is at approximately 4.577 (where cos(beta) = -0.1)\n",
    "assert 4.0 < final_beta < 5.0, f\"Final beta should converge near 4.577, got {final_beta}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Starting from $\\beta^0 = 2.3$, gradient descent converges to a local minimum at approximately $\\beta \\approx 4.577$.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "(d) Repeat with $\\beta^0 = 1.4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "beta = torch.tensor(1.4, requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "num_iterations = 50\n",
    "\n",
    "beta_history = [beta.item()]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    loss = objective_function_torch(beta)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        beta -= learning_rate * beta.grad\n",
    "        beta_history.append(beta.item())\n",
    "\n",
    "    beta.grad.zero_()\n",
    "\n",
    "final_beta = beta.item()\n",
    "\n",
    "# Plot\n",
    "beta_grid = np.linspace(-6, 6, 400)\n",
    "r_values = np.sin(beta_grid) + beta_grid / 10\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(beta_grid, r_values, label=r\"$R(\\beta) = \\sin(\\beta) + \\frac{\\beta}{10}$\", color=\"blue\")\n",
    "plt.scatter(\n",
    "    beta_history,\n",
    "    np.sin(np.array(beta_history)) + np.array(beta_history) / 10,\n",
    "    color=\"red\",\n",
    "    alpha=0.3,\n",
    "    label=\"Gradient Descent Steps\",\n",
    ")\n",
    "plt.scatter(\n",
    "    final_beta,\n",
    "    np.sin(final_beta) + final_beta / 10,\n",
    "    color=\"green\",\n",
    "    s=100,\n",
    "    label=f\"Local Minimum: {final_beta:.4f}\",\n",
    ")\n",
    "plt.title(r\"Gradient Descent on $R(\\beta)$ starting from $\\beta^0 = 1.4$\")\n",
    "plt.xlabel(r\"$\\beta$\")\n",
    "plt.ylabel(r\"$R(\\beta)$\")\n",
    "plt.legend()\n",
    "plt.grid(ls=\"--\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Local minimum found at beta = {final_beta:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(beta_history) > 1, \"Should have multiple gradient descent steps\"\n",
    "assert abs(beta_history[0] - 1.4) < 0.001, \"Should start from beta^0 = 1.4\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# The local minimum near beta=1.4 is at approximately -1.574 (where cos(beta) = -0.1)\n",
    "assert -2.0 < final_beta < -1.0, f\"Final beta should converge near -1.574, got {final_beta}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Starting from $\\beta^0 = 1.4$, gradient descent converges to a different local minimum at approximately $\\beta \\approx -1.574$.\n",
    "\n",
    "This demonstrates that gradient descent can converge to different local minima depending on the starting point, which is a key challenge in training neural networks with non-convex loss functions.\n",
    "> END SOLUTION\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
