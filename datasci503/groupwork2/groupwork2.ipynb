{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WvmSy31rfI_"
   },
   "source": [
    "# DATASCI 503, Group Work 2: Bias, Variance, and Irreducible Error\n",
    "\n",
    "This lab-initiated groupwork assignment is designed to provide a foundational understanding of bias, variance, and irreducible error, which are key to many machine learning endeavors. Upon completing this assignment, you'll have a practical understanding essential for diagnosing model performance. This knowledge is foundational, forming the basis for more advanced learning in the field of machine learning.\n",
    "\n",
    "**Instructions:** For the first 20 minutes of lab section, the GSI will recap the concepts of bias, variance, and irreducible. During the following 60 minutes of lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the group work tasks below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!** Upon completion, one member of the team should submit their team's work through Canvas as html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGCc17E53jMa"
   },
   "source": [
    "### Helpful Resources for this Subject\n",
    "\n",
    "- [B-V Tradeoff Full Explanation and Video Animation](https://youtu.be/FcXQKsZKRUs?t=46)\n",
    "- [Cool animation-explanation of bias-variance tradeoff](https://mlu-explain.github.io/bias-variance/)\n",
    "- [(Advanced) Derivation of bias-variance decomposition](https://web.archive.org/web/20140821063842/http://ttic.uchicago.edu/~gregory/courses/wis-ml2012/lectures/biasVarDecom.pdf) and [another equivalent derivation with video lecture](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgjG-k_IsfZB"
   },
   "source": [
    "## Recap of Bias, Variance, and Irreducible Error\n",
    "\n",
    "The bias-variance decomposition of the **test** mean squared error is [(ISLR page 34 Eq. (2.7))](https://www.stat.berkeley.edu/users/rabbee/s154/ISLR_First_Printing.pdf):\n",
    "\n",
    "$$\n",
    "\\mathrm{E[(y_0 - \\hat{f}(x_0))^2]} = [\\mathrm{Bias(\\hat{f}(x_0))}]^2 + \\mathrm{Var(\\hat{f}(x_0))} + \\mathrm{Var}(\\epsilon)\n",
    "$$\n",
    "\n",
    "We will explore the terms in this decomposition one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waXYwD55FYS-"
   },
   "source": [
    "### Bias\n",
    "\n",
    "Refers to the error that originates from the assumptions of your model that are often too simple to capture all the underlying patterns in the data. The model's inability to capture more complex interactions in data leads to systematic error in prediction. Mathematically, for $x_0$ given\n",
    "\n",
    "$$\n",
    "\\mathrm{Bias(\\hat{f}(x_0))^2} = (E[\\hat{f}(x_0)] - f(x_0))^2.\n",
    "$$\n",
    "\n",
    "That is, the **bias measures the difference between the mean estimate and the truth**. Let's explore this with an example.\n",
    "\n",
    "Let's think about the true model as taking the following shape\n",
    "$$\n",
    "X \\sim U(-5,5),\\\\\n",
    "Y|X \\sim \\mathcal{N}(X^2, 1).\n",
    "$$\n",
    "In practice, we do not know the true function, and we are left to guess it. For our first example, we will attempt to model this relationship using a linear regression model, which immediately assumes a sprecific -wrong!- relationship between $X$ and $Y$. For illustration purposes, we will pay special attention to the element $x_0=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10840,
     "status": "ok",
     "timestamp": 1737754121930,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "JZJwiIWVEfqF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1511,
     "status": "ok",
     "timestamp": 1737754123440,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "Evb9COAg5QEq",
    "outputId": "984248ad-4241-4eeb-c129-576ad00cddef"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "# np.random.seed(0)\n",
    "\n",
    "# Generate 100 samples of X uniformly from [0, 1]\n",
    "X = np.random.uniform(-5, 5, 100)  # Sample from Uniform distribution\n",
    "Y = X**2 + np.random.normal(0, 1, 100)  # Y is a parabola of X with added noise\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Reshape X for sklearn\n",
    "X_reshaped = data[\"X\"].values.reshape(-1, 1)\n",
    "\n",
    "# Fit a linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X_reshaped, data[\"Y\"])\n",
    "\n",
    "# Predictions\n",
    "data[\"Y_pred\"] = model.predict(X_reshaped)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(data[\"X\"], data[\"Y\"], color=\"blue\", label=\"Data\")\n",
    "plt.plot(data[\"X\"], data[\"Y_pred\"], color=\"red\", label=\"Linear Fit\")\n",
    "plt.axvline(x=0, color=\"green\", linestyle=\"--\", label=\"X=0\")\n",
    "# Adding a grid\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Linear Regression on Non-Linear Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAMSOCs4tKF2"
   },
   "source": [
    "## Task 1: Demonstrating Linear Regression Bias on Quadratic Data\n",
    "\n",
    "**Scenario**: Suppose you showed your boss the bias and variance of the linear regression on your simulated quadratic data. Your boss is terrible and used ChatGPT to summarize your Jupyter notebook. He responds to your work saying \"Linear regression is still probably fine. The bias is high only for the single point tested.\" **Throughout this lab**, we will show that this is false, because a line intersects a quadratic regression function at most twice. In all other regions, there will be a reasonably high distance between the linear regression and true regression curves. But, we need to fully demonstrate linear regression's abysmal performance on this data. Note that Task 1 questions will be presented in chunks as you go through the lab content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyHSmPFQwtrW"
   },
   "source": [
    "### Problem 1.1: Implement the Signal Function\n",
    "\n",
    "Implement the true, noiseless, signal function we used in the lab demo above. The data generating process introduces noise, but in synthetic experiments, we frequently have access to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737754123441,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "Ne9FCYAlvsKo"
   },
   "outputs": [],
   "source": [
    "def signal(x):\n",
    "    # BEGIN SOLUTION\n",
    "    return x**2\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737754123441,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "jB9eFSpYxW8z"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert signal(-2) == 4, \"signal(-2) should be 4\"\n",
    "assert signal(0) == 0, \"signal(0) should be 0\"\n",
    "assert signal(2) == 4, \"signal(2) should be 4\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert signal(3) == 9, \"signal(3) should be 9\"\n",
    "assert signal(-5) == 25, \"signal(-5) should be 25\"\n",
    "assert signal(0.5) == 0.25, \"signal(0.5) should be 0.25\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4vcSR3qyGna"
   },
   "source": [
    "### Problem 1.2: Code Design Principles\n",
    "\n",
    "That was too simple. But, this is actually decent code design. In your own words, why do you think we had you create a separate Python function even when the operation was simple? **(You could get asked this in an interview.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz8t2G59zvWi"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Creating a separate function for even simple operations promotes reusability, modularity, and maintainability. If the signal function needs to change in the future, we only need to modify it in one place. It also makes the code more readable and self-documenting, as the function name describes its purpose. Additionally, it enables easier testing and debugging.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iAIUw8JyuMl"
   },
   "source": [
    "### Problem 1.3: Implement the Data Generating Process\n",
    "\n",
    "We also need to make the data generating process its own function. Write a function that samples the noisy data **that utilizes your signal function from Problem 1.1**. You may assume that the noise follows a standard normal distribution $\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737754123441,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "7_siwimTytTZ"
   },
   "outputs": [],
   "source": [
    "def dgp(x):\n",
    "    # BEGIN SOLUTION\n",
    "    return signal(x) + np.random.normal(0, 1)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Note: dgp involves randomness, so we test the structure and expected behavior\n",
    "np.random.seed(42)\n",
    "test_result = dgp(0)\n",
    "assert isinstance(test_result, int | float | np.floating), \"dgp should return a numeric value\"\n",
    "# The signal at x=0 is 0, so dgp(0) should be close to 0 on average\n",
    "np.random.seed(42)\n",
    "samples = [dgp(0) for _ in range(1000)]\n",
    "assert abs(np.mean(samples)) < 0.2, \"Mean of dgp(0) samples should be close to 0\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "np.random.seed(123)\n",
    "test_val = dgp(2)\n",
    "assert abs(test_val - 4) < 5, \"dgp(2) should be signal(2) + noise, so close to 4\"\n",
    "np.random.seed(456)\n",
    "samples_at_3 = [dgp(3) for _ in range(500)]\n",
    "assert abs(np.mean(samples_at_3) - 9) < 0.5, \"Mean of dgp(3) samples should be close to 9\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zerdYjcoNtzH"
   },
   "source": [
    "Consequently, the discrepancy between the true relationship and our assumption will be a source of bias; the linear model's inability to capture the curvature of the quadratic relationship will result, practically, in systematic errors in prediction.\n",
    "\n",
    "Let's repeat the above experiment multiple times. You can fiddle with K if you wish. What do you observe?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1737754125451,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "xyTusjeKQPGu",
    "outputId": "5d95cb66-b0ee-49c4-b746-839087e091be"
   },
   "outputs": [],
   "source": [
    "# Repeating the experiment 10 times and drawing predictions each time\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Colors for the plots\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 10))\n",
    "\n",
    "K = 10\n",
    "# Repeat the experiment K times\n",
    "for i in range(K):\n",
    "    # Fit a linear model\n",
    "    X = np.random.uniform(-5, 5, 100)  # Sample from Uniform distribution\n",
    "    Y = X**2 + np.random.normal(0, 1, 100)  # Y is a parabola of X with added noise\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "    # Reshape X for sklearn\n",
    "    X_reshaped = data[\"X\"].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_reshaped, data[\"Y\"])\n",
    "\n",
    "    # Predictions\n",
    "    data[f\"Y_pred_{i}\"] = model.predict(X_reshaped)\n",
    "    random_color = np.random.rand(\n",
    "        3,\n",
    "    )\n",
    "\n",
    "    # Plot data and predictions\n",
    "    plt.scatter(data[\"X\"], data[\"Y\"], color=random_color, alpha=0.5, label=f\"Data {i + 1}\")\n",
    "    plt.plot(data[\"X\"], data[f\"Y_pred_{i}\"], color=random_color, label=f\"Prediction {i + 1}\")\n",
    "\n",
    "# Additional plot settings\n",
    "# plt.axvline(x=0, color='green', linestyle='-', label='X=0')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Linear Regression Predictions on Non-Linear Data (10 Experiments)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1504,
     "status": "ok",
     "timestamp": 1737754126948,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "qPG1aJDXT5Me",
    "outputId": "b493a620-20da-4ea0-966d-c8d58b334ac2"
   },
   "outputs": [],
   "source": [
    "n_datasets = 200\n",
    "predictions_x_0 = np.zeros(n_datasets)\n",
    "x_0 = 0\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    # Fit a linear model\n",
    "    X = np.random.uniform(-5, 5, 100)  # Sample from Uniform distribution\n",
    "    Y = X**2 + np.random.normal(0, 1, 100)  # Y is a parabola of X with added noise\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "    # Reshape X for sklearn\n",
    "    X_reshaped = data[\"X\"].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_reshaped, data[\"Y\"])\n",
    "\n",
    "    # Predictions\n",
    "    data[f\"Y_pred_{i}\"] = model.predict(X_reshaped)\n",
    "    # Predictions\n",
    "    y_pred = model.predict([[x_0]]).item()  # Predicting Y for x_0\n",
    "    predictions_x_0[i] = y_pred\n",
    "\n",
    "average_prediction = np.mean(predictions_x_0)\n",
    "print(f\"The average prediction at x_0=0 is: {average_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqBUKBE3WYXC"
   },
   "source": [
    "### Variance\n",
    "\n",
    "Each time we run our simulation, that is, every time we sample a new dataset $\\mathcal{D}$, our predictions changed, how can we quantify this variability?. In other words, how much does a model's predictions vary for a given data point when trained on a different data sample.\n",
    "\n",
    "In the context of our experiment, we repeatedly trained a linear regression model on various datasets and then predicted the value of $Y$ at $x_0 = 0$, variance can be observed in the differing predictions for each dataset. These variations arise because each model, though structurally the same (linear), is trained on a different dataset. Each time, *the model adjusts itself to best fit the specific nuances of its training dataset, leading to different predictions for the same X value across models*. This is indicative of **variance**: the change in model predictions as a result of the different data it's trained on, highlighting the model's sensitivity to the specific data it encounters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1737754127777,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "ijjFI3MlV8a4",
    "outputId": "019deef5-90a2-4704-af5b-25dac2915066"
   },
   "outputs": [],
   "source": [
    "# Plotting the histogram of predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(predictions_x_0, bins=20, color=\"skyblue\", edgecolor=\"black\", density=True)\n",
    "plt.xlabel(\"Predicted Value at X=0\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Predictions at X=0 (200 Datasets) - Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSoQnzecay0c"
   },
   "source": [
    "The spread of the histogram along the x-axis indicates the variance in the model's predictions, and it reflects how sensitive the linear model is to the specifics of the data. The figure clearly illustrates that our prediction at $x_0=0$ changes significantly depending on the dataset used for training.\n",
    "\n",
    "**A high variance can be problematic**: it implies that the model's predictions are not consistent across different datasets, so you can not trust them completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INdzu6tYcbYt"
   },
   "source": [
    "### Bias-variance tradeoff\n",
    "\n",
    "Having reviewed bias and variance, it stands to reason that a **good model** should incur in *small systematic error* and produce *consistent predictions* across datasets.\n",
    "\n",
    " In practice, decreasing one often increases the other. A model that is flexible (low bias) tends to have high variance because it starts to model the random noise in the training data as if it were meaningful, thus not generalizing well to new data. Conversely, a simpler model (high bias) might not capture the complexities of the training data, leading to systematic errors regardless of how much data you feed it. The **tradeoff** is thus finding the right balance between simplicity (to avoid overfitting and high variance) and complexity (to avoid underfitting and high bias). This balance is key to building robust models that perform well not just on the training data but also on unseen data, which is the ultimate goal of a predictive model.\n",
    "\n",
    " In our next example, we will be fitting data with models of increasing complexity. In this scenario, the complexity parameter corresponds to the degree of the polynomial we use to fit our data; the higher the degree, the more patterns (or noise!) the model can express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 13951,
     "status": "ok",
     "timestamp": 1737754141724,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "XcmCMJPpgC9p",
    "outputId": "fbc22d2c-60ea-4a2c-e7f0-2addeea20b16"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "n_datasets = 200\n",
    "x_0 = 0\n",
    "max_degree = 10\n",
    "\n",
    "# Arrays to store bias and variance\n",
    "biases = []\n",
    "variances = []\n",
    "\n",
    "# True value at X=0\n",
    "true_value = x_0**2\n",
    "\n",
    "# Iterate over different degrees of polynomial features\n",
    "for degree in range(1, max_degree + 1):\n",
    "    predictions_x_0 = np.zeros(n_datasets)\n",
    "\n",
    "    for i in range(n_datasets):\n",
    "        # Generate and fit the model\n",
    "        X = np.random.uniform(-5, 5, 100)\n",
    "        Y = np.sin(X) + np.random.normal(0, 1, 100)\n",
    "\n",
    "        poly = PolynomialFeatures(degree)\n",
    "        X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_poly, Y)\n",
    "\n",
    "        # Predict for x_0\n",
    "        x_0_poly = poly.transform([[x_0]])\n",
    "        predictions_x_0[i] = model.predict(x_0_poly)[0]\n",
    "\n",
    "    # Calculate bias and variance\n",
    "    average_prediction = np.mean(predictions_x_0)\n",
    "    bias = np.abs(average_prediction - true_value)\n",
    "    variance = np.var(predictions_x_0)\n",
    "\n",
    "    biases.append(bias)\n",
    "    variances.append(variance)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_degree + 1), biases, label=\"Bias\", marker=\"o\")\n",
    "plt.plot(range(1, max_degree + 1), variances, label=\"Variance\", marker=\"x\")\n",
    "plt.xlabel(\"Degree of Polynomial\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Bias-Variance Tradeoff\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDzwmDVAxjQt"
   },
   "source": [
    "### Problem 1.4: Train Multiple Models\n",
    "\n",
    "Now, we want to calculate the bias, variance, and irreducible loss components for many values of x for our demonstration. To calculate these values, we need to repeat the training procedure many times over. With the help of some lab code, run 1000 iterations of training, each with 100 training samples. Save each model to a list called `model_list`. You must use the `dgp` function you developed in Problem 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1518,
     "status": "ok",
     "timestamp": 1737754143239,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "JdTmpQDLximD"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "NUM_SAMPLES = 100\n",
    "NUM_DATASETS = 1000\n",
    "\n",
    "model_list = []\n",
    "\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Generate and fit the model\n",
    "    X = np.random.uniform(-5, 5, NUM_SAMPLES)\n",
    "    Y = dgp(X)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X.reshape(-1, 1), Y)\n",
    "\n",
    "    model_list.append(model)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(model_list) == 1000, \"model_list should contain 1000 models\"\n",
    "assert hasattr(\n",
    "    model_list[0], \"predict\"\n",
    "), \"Each item in model_list should be a fitted model with predict method\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(model_list) == NUM_DATASETS, \"model_list length should match NUM_DATASETS\"\n",
    "# Check that models can make predictions\n",
    "test_pred = model_list[0].predict([[0]])\n",
    "assert test_pred is not None, \"Models should be able to make predictions\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AspFj4K_cyak"
   },
   "source": [
    "### Irreducible error\n",
    "\n",
    "What if we model the relationship perfectly, in a way such that it will produce the same response, will we have squashed all error and have a perfect model?. The answer to this question is, unfortunately, negative; irreducible error will provide a lower bound for test prediction accuracy. This error is inherent to the data, and no matter how well we model the relationship between $X$ and $Y$, it is intrinsic to it and thus irreducible. In our working example, this error corresponds to the variability in $Y|X$ given by the standard deviation in $Y|X \\sim \\mathcal{N}(X^2, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1074,
     "status": "ok",
     "timestamp": 1737754144311,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "nOZOptOt64aj",
    "outputId": "bc49a5ee-a820-4374-a01c-d6d72de1118a"
   },
   "outputs": [],
   "source": [
    "# Generate 100 samples of X uniformly from [0, 1]\n",
    "X = np.random.uniform(-5, 5, 100)  # Sample from Uniform distribution\n",
    "Y = X**2 + np.random.normal(0, 1, 100)  # Y is a parabola of X with added noise\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Reshape X for sklearn\n",
    "X_reshaped = data[\"X\"].values.reshape(-1, 1)\n",
    "\n",
    "# Fit a linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X_reshaped, data[\"Y\"])\n",
    "\n",
    "# Predictions\n",
    "data[\"Y_pred\"] = model.predict(X_reshaped)\n",
    "\n",
    "# Scatter plot of the simulated dataset\n",
    "plt.scatter(data[\"X\"], data[\"Y\"], color=\"blue\", label=\"Simulated Data\")\n",
    "\n",
    "# Plot the true regression function\n",
    "X = np.linspace(-5, 5, 100)\n",
    "true_Y = np.square(X)\n",
    "plt.plot(X, true_Y, color=\"green\", label=\"True Regression Function\")\n",
    "\n",
    "\n",
    "def f_filled(x):\n",
    "    return np.square(x) + 1\n",
    "\n",
    "\n",
    "def g_filled(x):\n",
    "    return np.square(x) - 1\n",
    "\n",
    "\n",
    "# Generate values for the filled area\n",
    "X_filled = np.linspace(-5, 5, 500)\n",
    "Y_filled_lower = f_filled(X_filled)\n",
    "Y_filled_upper = g_filled(X_filled)\n",
    "plt.fill_between(\n",
    "    X_filled,\n",
    "    Y_filled_lower,\n",
    "    Y_filled_upper,\n",
    "    color=\"gray\",\n",
    "    alpha=0.5,\n",
    "    label=\"Area between +- one standard deviation from the true function\",\n",
    ")\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predictor Feature X\")\n",
    "plt.ylabel(\"Response Y\")\n",
    "plt.title(\"Regression Analysis\")\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZCq3Ir-o499"
   },
   "source": [
    "Even if our algorithm finds the true function and manages to also consistently predict it, there is inherent variability in our data that is not modeled by the relationship between $X$ and $Y$!\n",
    "\n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUZlyIg-4eTa"
   },
   "source": [
    "### Problem 1.5: Implement Bias, Variance, and MSE Functions\n",
    "\n",
    "Our end goal is to animate the bias, variance, and irreducible loss contributions across the inputs. To do this, we need to implement the following functions:\n",
    "\n",
    "- `bias(x, signal, model_list)` which calculates the bias at x.\n",
    "- `variance(x, model_list)` which calculates the variance at x.\n",
    "- `mse(x, dgp, model_list)` which calculates the mean squared error at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1737754144311,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "hUPyknuY4d5T"
   },
   "outputs": [],
   "source": [
    "def bias(x, signal, model_list):\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute the mean prediction minus the true signal value\n",
    "    biases = np.zeros(len(model_list))\n",
    "    predictions = np.zeros(len(model_list))\n",
    "\n",
    "    for i, model in enumerate(model_list):\n",
    "        predictions[i] = model.predict([[x]])[0]\n",
    "        biases[i] = (model.predict([[x]]) - signal(x)).item()\n",
    "\n",
    "    return biases.mean()\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1737754144311,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "SOWLT60CyFIW"
   },
   "outputs": [],
   "source": [
    "def variance(x, model_list):\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute the variance of predictions across all models\n",
    "    predictions = np.zeros(len(model_list))\n",
    "    for i, model in enumerate(model_list):\n",
    "        predictions[i] = model.predict([[x]])[0]\n",
    "\n",
    "    return predictions.var()\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1737754144312,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "mi52fJhDyff4"
   },
   "outputs": [],
   "source": [
    "def mse(x, dgp, model_list):\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute mean squared error by averaging squared differences\n",
    "    mse_list = []\n",
    "    for model in model_list:\n",
    "        mse_list.append((model.predict([[x]]) - dgp(x)).item() ** 2)\n",
    "\n",
    "    return np.array(mse_list).mean()\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1737754144950,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "86pSJVnT4Jao"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test that functions return numeric values (exact values depend on random model_list)\n",
    "test_bias = bias(0, signal, model_list)\n",
    "assert isinstance(test_bias, int | float | np.floating), \"bias should return a numeric value\"\n",
    "test_var = variance(0, model_list)\n",
    "assert isinstance(test_var, int | float | np.floating), \"variance should return a numeric value\"\n",
    "assert test_var >= 0, \"variance should be non-negative\"\n",
    "# MSE at x=-5 should be approximately 282 based on the model specification\n",
    "assert np.abs(mse(-5, dgp, model_list) - 282) < 20, \"MSE at x=-5 should be approximately 282\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Additional edge case tests\n",
    "test_mse = mse(0, dgp, model_list)\n",
    "assert test_mse >= 0, \"MSE should be non-negative\"\n",
    "# Bias squared + variance should be less than or equal to MSE (approximately)\n",
    "test_bias_sq = bias(0, signal, model_list) ** 2\n",
    "test_variance = variance(0, model_list)\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it9n89C0yjxI"
   },
   "source": [
    "### Problem 1.6: Create the Animation\n",
    "\n",
    "With the help of some pre-generated code, we will create an animation that overlays on the plot provided in Problem 1.3. You will need to animate the following:\n",
    "\n",
    "1. x moving from -5 to 5.\n",
    "2. The bias, variance, and irreducible loss calculations changing as x moves, **rounded to 2 decimal places**.\n",
    "\n",
    "To prove you completed the task, edit the markdown cell underneath the animation with an example decomposition. We have highlighted where in the code you need to insert your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 70467,
     "status": "ok",
     "timestamp": 1737754215415,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "ObqgtcgwTCVJ"
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 100\n",
    "x = np.linspace(-5, 5, NUM_STEPS)  # ...\n",
    "signal_array = np.array([signal(xx) for xx in x])\n",
    "bias_array = np.array([bias(xx, signal, model_list) for xx in x])\n",
    "variance_array = np.array([variance(xx, model_list) for xx in x])\n",
    "mse_array = np.array([mse(xx, dgp, model_list) for xx in x])\n",
    "y_predictions_array = signal_array + bias_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "17Kinu7vg7fufsHjqMKVz1yD2SezLa6Ma"
    },
    "executionInfo": {
     "elapsed": 48391,
     "status": "ok",
     "timestamp": 1737754263804,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "NWxzGw2lmouI",
    "outputId": "82e077c0-9a85-4b04-ad18-0595ad38af88"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "rc(\"animation\", html=\"jshtml\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "(signals,) = ax.plot(x[0], signal(x[0]), color=\"purple\")\n",
    "avg_predictions = ax.scatter(x[0], signal(x[0]) + bias(x[0], signal, model_list))\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-10, 30)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "# Initialize the vertical line with a placeholder\n",
    "(bias_line,) = ax.plot(\n",
    "    [], [], color=\"r\", linestyle=\"-\", label=\"Bias Line\"\n",
    ")  # Changed to a plot for dynamic updates\n",
    "(bias_line_marker,) = ax.plot(\n",
    "    [], [], marker=\"_\", color=\"r\", linestyle=\"None\", markersize=10\n",
    ")  # Add flat tips at the ends of the line\n",
    "\n",
    "(variance_line,) = ax.plot([], [], color=\"y\", linestyle=\"--\", label=\"2SD Line\")\n",
    "(variance_line_marker,) = ax.plot(\n",
    "    [], [], marker=\"_\", color=\"y\", linestyle=\"None\", markersize=10\n",
    ")  # Add flat tips at the ends of the line\n",
    "\n",
    "# Create the status text box once outside the update function to avoid memory leak\n",
    "status_text = ax.text(-4.5, -8, \"\", fontsize=12, bbox=dict(facecolor=\"gray\", alpha=1.0))\n",
    "\n",
    "\n",
    "# Update function for the animation\n",
    "def update(frame):\n",
    "    if frame == 0:\n",
    "        return None\n",
    "    ### Add a new dot to the model predictions.\n",
    "    new_x = x[:frame]\n",
    "    new_y_avg_pred = y_predictions_array[:frame]\n",
    "    # update the scatter plot\n",
    "    data = np.stack([new_x, new_y_avg_pred]).T\n",
    "    avg_predictions.set_offsets(data)\n",
    "    # update the line plot\n",
    "    signals.set_xdata(x[:frame])\n",
    "    signals.set_ydata(signal(x[:frame]))\n",
    "\n",
    "    ### Showcase the bias with a vertical line.\n",
    "    # Move the vertical line\n",
    "    bias_line.set_data(\n",
    "        [x[frame], x[frame]], [signal_array[frame], y_predictions_array[frame]]\n",
    "    )  # Set the line to connect signal and prediction\n",
    "    bias_line_marker.set_data(\n",
    "        [x[frame], x[frame]], [signal_array[frame], y_predictions_array[frame]]\n",
    "    )  # Add flat tips at the ends of the line\n",
    "\n",
    "    current_sd = variance_array[frame] ** 0.5\n",
    "\n",
    "    OFFSET = 0.05  # so that the bias and variance lines don't overlay each other\n",
    "    variance_line.set_data(\n",
    "        [x[frame] - OFFSET, x[frame] - OFFSET],\n",
    "        [y_predictions_array[frame] - 2 * current_sd, y_predictions_array[frame] + 2 * current_sd],\n",
    "    )  # Set the line to connect signal and prediction\n",
    "    variance_line_marker.set_data(\n",
    "        [x[frame] - OFFSET, x[frame] - OFFSET],\n",
    "        [y_predictions_array[frame] - 2 * current_sd, y_predictions_array[frame] + 2 * current_sd],\n",
    "    )  # Add flat tips at the ends of the line\n",
    "\n",
    "    # Update the text box with the current values\n",
    "    current_x = x[frame]\n",
    "    current_bias = bias_array[frame]\n",
    "    current_variance = variance_array[frame]\n",
    "    current_mse = mse_array[frame]\n",
    "\n",
    "    # Update the text box to show metric ROUNDED to 2 DECIMAL PLACES\n",
    "    status_text.set_text(\n",
    "        f\"\"\"x: {current_x:.2f}\\n\n",
    "        Bias^2: {current_bias**2:.2f}\\n\n",
    "        Variance: {current_variance:.2f}\\n\n",
    "        MSE: {current_mse:.2f}\\n\n",
    "        Noise: {current_mse - current_bias**2 - current_variance:.2f}\"\"\"\n",
    "    )\n",
    "    return signals, avg_predictions, bias_line\n",
    "\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=len(x), interval=500, blit=False)\n",
    "\n",
    "# View the animation\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXggGPN2hD8g"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "MSE = Bias^2 + Variance + Noise\n",
    "\n",
    "Example at x = 0:\n",
    "MSE (approximately 70) = Bias^2 (approximately 68) + Variance (approximately 1) + Noise (approximately 1)\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx2tuVVas8m7"
   },
   "source": [
    "## Task 2: KNN Regression on NHANES Data\n",
    "\n",
    "The end goal of this Task is to implement KNN regression on data from the [National Health and Nutrition Examination Survey](https://www.cdc.gov/nchs/nhanes/about/index.html) (NHANES). We will select a datapoint and compute bias, variance, and MSE at this value for several neighbor sizes K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayax5oOgtO7k"
   },
   "source": [
    "### Problem 2.1: Import the NHANES Datasets\n",
    "\n",
    "Import the datasets using `pandas`. The Canvas filepath is `Files > datasets > NHANES`. All three files should be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "error",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "CKhwafgFtOZ_",
    "outputId": "79f632c6-c312-45ba-98c6-415bfe160db9"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Import datasets from SAS XPT format\n",
    "hdl = pd.read_sas(\"data/HDL_L.xpt\")\n",
    "bmx = pd.read_sas(\"data/BMX_L.xpt\")\n",
    "demo = pd.read_sas(\"data/DEMO_L.xpt\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"hdl\" in dir(), \"hdl DataFrame should be defined\"\n",
    "assert \"bmx\" in dir(), \"bmx DataFrame should be defined\"\n",
    "assert \"demo\" in dir(), \"demo DataFrame should be defined\"\n",
    "assert hasattr(hdl, \"shape\"), \"hdl should be a DataFrame\"\n",
    "assert hasattr(bmx, \"shape\"), \"bmx should be a DataFrame\"\n",
    "assert hasattr(demo, \"shape\"), \"demo should be a DataFrame\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"SEQN\" in hdl.columns, \"hdl should have SEQN column\"\n",
    "assert \"SEQN\" in bmx.columns, \"bmx should have SEQN column\"\n",
    "assert \"SEQN\" in demo.columns, \"demo should have SEQN column\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u-4-071yL1w"
   },
   "source": [
    "### Problem 2.2: Join the Datasets\n",
    "\n",
    "Join the 3 datasets on their common observation identifier. Documentation is available for the 3 datasets:\n",
    "\n",
    "1. [BMX_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/BMX_L.htm)\n",
    "2. [HDL_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/HDL_L.htm)\n",
    "3. [DEMO_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.htm)\n",
    "\n",
    "How many observations did you retain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "KObVTT3Dxpn0"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Inner join on SEQN (the common identifier)\n",
    "df = pd.merge(hdl, bmx, on=\"SEQN\", how=\"inner\")\n",
    "df = pd.merge(df, demo, on=\"SEQN\", how=\"inner\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "nTJl68xU4TY0"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert df.shape[0] == 8068, \"Should have 8068 observations after join\"\n",
    "assert df.shape[1] == 51, \"Should have 51 columns after join\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"SEQN\" in df.columns, \"SEQN should be in the joined DataFrame\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYKHOe5RzKSC"
   },
   "source": [
    "### Problem 2.3: Select and Rename Variables\n",
    "\n",
    "We are interested in HDL concentration in mg/dL, Gender, and Weight. Using the documentation, remove all observations that do not include the variable of interest. Rename them to `'HDL'`, `'Gender'`, and `'Weight'`. Finally, only keep the identifier you joined on and these 3 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "HOQ71Di4zJWn"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Remove rows where LBDHDD, BMXWT, RIAGENDR is missing\n",
    "df = df.dropna(subset=[\"LBDHDD\", \"BMXWT\", \"RIAGENDR\"])\n",
    "# Rename LBDHDD, BMXWT, RIAGENDR to HDL, Weight, Gender\n",
    "df = df.rename(columns={\"LBDHDD\": \"HDL\", \"BMXWT\": \"Weight\", \"RIAGENDR\": \"Gender\"})\n",
    "# Only keep SEQN and the 3 variables\n",
    "df = df[[\"SEQN\", \"HDL\", \"Gender\", \"Weight\"]]\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "WXeZUU8K4e_g"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert df.shape[0] == 6822, \"Should have 6822 observations after filtering\"\n",
    "assert df.shape[1] == 4, \"Should have 4 columns\"\n",
    "assert \"HDL\" in df.columns, \"HDL column should exist\"\n",
    "assert \"Gender\" in df.columns, \"Gender column should exist\"\n",
    "assert \"Weight\" in df.columns, \"Weight column should exist\"\n",
    "assert \"SEQN\" in df.columns, \"SEQN column should exist\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert df[\"HDL\"].notna().all(), \"HDL should have no missing values\"\n",
    "assert df[\"Gender\"].notna().all(), \"Gender should have no missing values\"\n",
    "assert df[\"Weight\"].notna().all(), \"Weight should have no missing values\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXGazxA23_yq"
   },
   "source": [
    "### Problem 2.4: Sample a Test Observation\n",
    "\n",
    "Now use `pandas` with `random_state=503` to sample a single observation from the dataset. This will be our fixed observation to compute bias, variance, and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "vKEnaJzp4C03"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "random_sample = df.sample(1, random_state=503)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "Lhrg4jp85CEr"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "y_0 = random_sample[\"HDL\"]\n",
    "x_0 = random_sample[[\"Gender\", \"Weight\"]]\n",
    "\n",
    "assert x_0.shape == (1, 2), \"x_0 should have shape (1, 2)\"\n",
    "assert y_0.shape == (1,), \"y_0 should have shape (1,)\"\n",
    "assert abs(y_0.item() - 42.0) < 0.1, \"y_0 should be approximately 42.0\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert random_sample.shape == (1, 4), \"random_sample should have shape (1, 4)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opd7WAtl4taD"
   },
   "source": [
    "### Problem 2.5: Implement KNN Regression Function\n",
    "\n",
    "Write a function that subsamples the data at random, then performs KNN regression and returns the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "zguEN-Qb4mL7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "def knn_regression(x, y, k: int = 10, subsample_size: float = 0.5):\n",
    "    # BEGIN SOLUTION\n",
    "    # Subsample the data x, y\n",
    "    x_train, _, y_train, _ = train_test_split(x, y, test_size=1 - subsample_size)\n",
    "    # Fit the model\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "6rvkaLyJ0Kli"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "np.random.seed(503)\n",
    "x = df[[\"Gender\", \"Weight\"]]\n",
    "y = df[\"HDL\"]\n",
    "test_model = knn_regression(x, y)\n",
    "\n",
    "# Assert prediction is close to 44.8\n",
    "assert abs(test_model.predict(x_0)[0] - 44.8) < 0.1, \"Prediction should be approximately 44.8\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(test_model, \"predict\"), \"Model should have predict method\"\n",
    "assert hasattr(test_model, \"n_neighbors\"), \"Model should have n_neighbors attribute\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMQaX6B-07Xb"
   },
   "source": [
    "### Setup: Modified Bias-Variance Functions\n",
    "\n",
    "Since with real world data we do not know the true signal function, we have to modify the bias, variance, and MSE functions to take in a test point directly. As they are rewritten, they will compute their respective quantities at a test point `(x0, y0)`.\n",
    "\n",
    "**DO NOT MODIFY THE FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1737754263805,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "TzsZPYX30TNG"
   },
   "outputs": [],
   "source": [
    "def bias(x0, y0, model_list: list):\n",
    "    biases = np.zeros(len(model_list))\n",
    "    for i, model in enumerate(model_list):\n",
    "        biases[i] = (model.predict(x0) - y0).item()\n",
    "    return biases.mean()\n",
    "\n",
    "\n",
    "def variance(x0, model_list: list):\n",
    "    predictions = np.zeros(len(model_list))\n",
    "    for i, model in enumerate(model_list):\n",
    "        predictions[i] = model.predict(x0)[0]\n",
    "    return predictions.var()\n",
    "\n",
    "\n",
    "def mse(x0, y0, model_list: list):\n",
    "    mses = np.zeros(len(model_list))\n",
    "    for i, model in enumerate(model_list):\n",
    "        mses[i] = (model.predict(x0) - y0).item() ** 2\n",
    "    return mses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_IrpjF22OBl"
   },
   "source": [
    "### Problem 2.6: Implement Bias-Variance Tradeoff Analysis\n",
    "\n",
    "Using the functions from the setup, fill out the skeleton code for the `bias_variance_tradeoff` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1737754263806,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "l7yeDgNw2R6h"
   },
   "outputs": [],
   "source": [
    "def bias_variance_tradeoff(x, y, x_0, y_0, k_values, n_subsamples=50, subsample_size=0.5):\n",
    "    \"\"\"\n",
    "    Computes the bias-variance tradeoff for K-Nearest Neighbors (KNN) regression at a given point x_0.\n",
    "\n",
    "    Parameters:\n",
    "        x (array-like): Feature matrix of shape (n_samples, n_features).\n",
    "        y (array-like): Target values of shape (n_samples,).\n",
    "        x_0 (array-like): A specific input point where bias-variance tradeoff is evaluated, shape (n_features,).\n",
    "        y_0 (float): The true target value at x_0.\n",
    "        k_values (list): A list of integers specifying the different values of k (number of neighbors) to evaluate.\n",
    "        n_subsamples (int, optional): Number of subsampled datasets to train models on. Default is 50.\n",
    "        subsample_size (float, optional): Fraction of the original dataset to be used for each subsample (0 to 1). Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (len(k_values), 3) matrix where:\n",
    "            - Column 1: Estimated bias squared (E[f(x_0)] - y_0)^2.\n",
    "            - Column 2: Estimated variance of f(x_0).\n",
    "            - Column 3: Mean squared error (bias squared + variance + irreducible error).\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    results = np.zeros((len(k_values), 3))\n",
    "\n",
    "    # For each value of k\n",
    "    for i, k in enumerate(k_values):\n",
    "        # Initialize model list\n",
    "        models = []\n",
    "\n",
    "        # Generate subsamples and fit models\n",
    "        for _ in range(n_subsamples):\n",
    "            # Train KNN model on a subsample of the data\n",
    "            model = knn_regression(x, y, k=k, subsample_size=subsample_size)\n",
    "            models.append(model)\n",
    "\n",
    "        # Compute bias, variance, and mean squared error at (x_0, y_0)\n",
    "        b = bias(x_0, y_0, models)\n",
    "        var = variance(x_0, models)\n",
    "        m = mse(x_0, y_0, models)\n",
    "\n",
    "        # Store results in the matrix\n",
    "        results[i] = [b, var, m]\n",
    "\n",
    "    return results\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test the function structure and output shape\n",
    "np.random.seed(503)\n",
    "test_x = df[[\"Gender\", \"Weight\"]] if \"df\" in dir() else np.random.randn(100, 2)\n",
    "test_y = df[\"HDL\"] if \"df\" in dir() else np.random.randn(100)\n",
    "test_x0 = test_x.iloc[:1] if hasattr(test_x, \"iloc\") else test_x[:1]\n",
    "test_y0 = test_y.iloc[0] if hasattr(test_y, \"iloc\") else test_y[0]\n",
    "test_k_values = [3, 5]\n",
    "\n",
    "test_results = bias_variance_tradeoff(\n",
    "    test_x, test_y, test_x0, test_y0, test_k_values, n_subsamples=5\n",
    ")\n",
    "assert test_results.shape == (2, 3), f\"Expected shape (2, 3), got {test_results.shape}\"\n",
    "assert test_results[:, 2].min() >= 0, \"MSE should be non-negative\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Additional tests for correctness\n",
    "assert callable(bias_variance_tradeoff), \"bias_variance_tradeoff should be callable\"\n",
    "assert test_results[:, 1].min() >= 0, \"Variance should be non-negative\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1737754263806,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "mYIzQ-g97uq0"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "k_values = [2, 5, 10, 15, 20, 30]\n",
    "results = bias_variance_tradeoff(x, y, x_0, y_0, k_values, n_subsamples=50)\n",
    "\n",
    "# plot smoothed results by using a spline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as interpolate\n",
    "\n",
    "# smooth all columns\n",
    "n_points = 100\n",
    "x_range = np.linspace(min(k_values), max(k_values), n_points)\n",
    "smooth_results = np.zeros((n_points, results.shape[1]))\n",
    "for i in range(results.shape[1]):\n",
    "    # interpolate with spline\n",
    "    smooth_results[:, i] = interpolate.UnivariateSpline(k_values, results[:, i], k=3, s=0)(x_range)\n",
    "\n",
    "# plot smoothed results\n",
    "plt.plot(x_range, smooth_results[:, 0] ** 2, linestyle=\"--\")\n",
    "plt.plot(x_range, smooth_results[:, 1], linestyle=\"--\")\n",
    "plt.plot(x_range, smooth_results[:, 2])\n",
    "# plot original values as points\n",
    "plt.scatter(k_values, results[:, 0] ** 2, label=\"Bias\", marker=\"s\")\n",
    "plt.scatter(k_values, results[:, 1], label=\"Variance\", marker=\"x\")\n",
    "plt.scatter(k_values, results[:, 2], label=\"MSE\", marker=\"o\")\n",
    "plt.xlabel(\"K Neighbors\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.title(\"Bias, Variance, and MSE vs. K Neighbors\")\n",
    "# add dashed grid\n",
    "plt.grid(linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MMQaX6B-07Xb"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
