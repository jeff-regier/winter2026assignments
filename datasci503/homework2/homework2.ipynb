{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 503, Homework 2: K-Nearest Neighbors and Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This assignment covers **K-Nearest Neighbors (KNN)**, a non-parametric method for classification and regression, and the **bias-variance tradeoff**, which describes how model complexity affects prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "Consider the following dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{6}$ where each $x^{(i)} \\in \\mathbb{R}^3$ and $y^{(i)} \\in \\{\\text{Red}, \\text{Green}\\}$:\n",
    "\n",
    "| $i$ | $x^{(i)}$ | $y^{(i)}$ |\n",
    "|-----|-----------|----------|\n",
    "| 1 | $(0, 3, 0)$ | Red |\n",
    "| 2 | $(2, 0, 0)$ | Red |\n",
    "| 3 | $(0, 1, 3)$ | Red |\n",
    "| 4 | $(0, 1, 2)$ | Green |\n",
    "| 5 | $(-1, 0, 1)$ | Green |\n",
    "| 6 | $(1, 1, 2)$ | Green |\n",
    "\n",
    "We want to classify a test point $x^{(te)} = (0, 0, 0)$ using K-nearest neighbors with squared Euclidean distance $d(a, b) = \\sum_{j=1}^{3}(a_j - b_j)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1a:** Compute Distances\n",
    "\n",
    "Compute the squared Euclidean distance from the test point $x^{(te)} = (0, 0, 0)$ to each of the six training points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "$d(x^{(te)}, x^{(1)}) = (0-0)^2 + (0-3)^2 + (0-0)^2 = 9$\n",
    "\n",
    "$d(x^{(te)}, x^{(2)}) = (0-2)^2 + (0-0)^2 + (0-0)^2 = 4$\n",
    "\n",
    "$d(x^{(te)}, x^{(3)}) = (0-0)^2 + (0-1)^2 + (0-3)^2 = 10$\n",
    "\n",
    "$d(x^{(te)}, x^{(4)}) = (0-0)^2 + (0-1)^2 + (0-2)^2 = 5$\n",
    "\n",
    "$d(x^{(te)}, x^{(5)}) = (0-(-1))^2 + (0-0)^2 + (0-1)^2 = 2$\n",
    "\n",
    "$d(x^{(te)}, x^{(6)}) = (0-1)^2 + (0-1)^2 + (0-2)^2 = 6$\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1b:** K=3 Classification\n",
    "\n",
    "Using your computed distances, what is the predicted class $\\hat{y}^{(3)}(x^{(te)}; \\mathcal{D})$ when $K = 3$? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "For $K = 3$, $\\hat{y}^{(3)}(x^{(te)}; \\mathcal{D}) = \\mathrm{Green}$. This is because the closest 3 points are $x^{(5)}$ (which is Green), $x^{(2)}$ (which is Red), and $x^{(4)}$ (which is Green); the majority class is Green.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1c:** K=1 Classification\n",
    "\n",
    "What is the predicted class $\\hat{y}^{(1)}(x^{(te)}; \\mathcal{D})$ when $K = 1$? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "For $K = 1$, $\\hat{y}^{(1)}(x^{(te)}; \\mathcal{D}) = \\mathrm{Green}$ as well, because the closest point is $x^{(5)}$ (Green), and the majority class among this single neighbor is Green.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1d:** Comparing K Values\n",
    "\n",
    "True or False: In a typical data-generating process where outliers and noise are present, $K = 3$ tends to give more consistent predictions than $K = 1$. Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "True.\n",
    "\n",
    "In a typical data-generating process where outliers and noise are present, $K=3$ tends to give more consistent predictions. This is because it considers more neighbors, which helps smooth out the effect of noisy or outlier data points.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2a:** Training Error and Flexibility\n",
    "\n",
    "True or False: The predictive error on training data generally decreases as the model becomes more flexible. Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "True.\n",
    "\n",
    "The predictive error on the training data generally decreases as the model becomes more flexible. A more flexible model can fit the training data more closely, reducing training error.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2b:** Test Error and Flexibility\n",
    "\n",
    "Describe how the predictive error on test data typically changes as model flexibility increases. What phenomenon explains this behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "In typical settings, the predictive error on the test data follows a U-shaped curve as the model becomes more flexible. As flexibility increases from a very low level, the predictive error on test data tends to decrease initially; after a certain level, the predictive error tends to increase due to the issue of overfitting.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition: A Simulation Study\n",
    "\n",
    "Consider the data-generating process:\n",
    "- $X \\sim \\mathrm{Uniform}[0, 1]$\n",
    "- $Y | X = x \\sim \\mathrm{Uniform}[x + \\cos(2\\pi x) - 0.1, x + \\cos(2\\pi x) + 0.1]$\n",
    "\n",
    "We will investigate the bias-variance tradeoff by fitting an ordinary least squares (OLS) linear regression model to data generated from this process.\n",
    "\n",
    "**Resources:**\n",
    "- [sklearn LinearRegression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- [ISL Chapter 2.2: Bias-Variance Tradeoff](https://www.statlearning.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3a:** Conditional Expectation\n",
    "\n",
    "Find $f(x) = \\mathbb{E}[Y|X=x]$ and compute $f(0.5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "$f(x) = \\mathbb{E}[Y|X=x] = x + \\cos(2\\pi x)$\n",
    "\n",
    "$f(0.5) = 0.5 + \\cos(2 \\pi \\cdot 0.5) = 0.5 + (-1) = -0.5$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3b:** Conditional Variance\n",
    "\n",
    "Compute $\\mathrm{Var}(Y | X = 0.5)$. Recall that for a uniform distribution on $[a, b]$, the variance is $(b-a)^2/12$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "When $X = 0.5$, $[Y |X = 0.5] \\sim \\mathrm{Uniform}[0.5 + \\cos(2\\pi \\cdot 0.5) - 0.1, 0.5 + \\cos(2\\pi \\cdot 0.5) + 0.1]$.\n",
    "\n",
    "Therefore $[Y |X = 0.5] \\sim \\mathrm{Uniform}[-0.6, -0.4]$\n",
    "\n",
    "$\\mathrm{Var}(Y |X = 0.5) = \\frac{(-0.4 - (-0.6))^2}{12} = \\frac{0.2^2}{12} = \\frac{0.04}{12} = \\frac{1}{300} \\approx 0.00333$\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3c:** Data Generation and OLS Fit\n",
    "\n",
    "Generate 100 samples from the data-generating process. Store the features in a variable `features` and the targets in a variable `targets`. Then fit an OLS linear regression model and store it in a variable `ols_model`. Finally, create a plot showing:\n",
    "1. The data points as a scatter plot\n",
    "2. The true regression function $f(x)$\n",
    "3. The estimated OLS fit $\\hat{f}(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Define the true regression function\n",
    "def true_function(x):\n",
    "    return x + np.cos(2 * np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Generate X and Y from the data-generating process\n",
    "features = np.random.uniform(0, 1, 100)\n",
    "targets = np.random.uniform(\n",
    "    features + np.cos(2 * np.pi * features) - 0.1,\n",
    "    features + np.cos(2 * np.pi * features) + 0.1,\n",
    "    100,\n",
    ")\n",
    "\n",
    "# Fit ordinary least squares model\n",
    "features_reshaped = features.reshape(-1, 1)\n",
    "ols_model = LinearRegression().fit(features_reshaped, targets)\n",
    "\n",
    "# Create the plot\n",
    "x_range = np.linspace(0, 1, 100)\n",
    "ols_predictions = ols_model.predict(x_range.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(features, targets, color=\"blue\", label=\"Data points\")\n",
    "plt.plot(x_range, true_function(x_range), color=\"green\", label=\"True function f\")\n",
    "plt.plot(x_range, ols_predictions, color=\"red\", label=\"Estimated function f_hat\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Data points vs True function vs Estimated function\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(features) == 100, \"features should have 100 samples\"\n",
    "assert len(targets) == 100, \"targets should have 100 samples\"\n",
    "assert hasattr(ols_model, \"coef_\"), \"ols_model should be a fitted LinearRegression model\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert features.min() >= 0 and features.max() <= 1, \"features should be in [0, 1]\"\n",
    "assert hasattr(ols_model, \"intercept_\"), \"ols_model should have intercept_\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3d:** Sampling Distribution of Predictions\n",
    "\n",
    "To understand the variance of the OLS estimator, repeat the following 500 times:\n",
    "1. Generate a new dataset of 100 samples from the data-generating process\n",
    "2. Fit an OLS model\n",
    "3. Store the prediction $\\hat{f}(0.5; \\mathcal{D}_i)$\n",
    "\n",
    "Store all 500 predictions in a list called `predictions` and plot a histogram of these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "predictions = []\n",
    "test_point = np.array([[0.5]])\n",
    "\n",
    "for _ in range(500):\n",
    "    # Generate the dataset from the data-generating process\n",
    "    sample_features = np.random.uniform(0, 1, 100)\n",
    "    sample_targets = np.random.uniform(\n",
    "        sample_features + np.cos(2 * np.pi * sample_features) - 0.1,\n",
    "        sample_features + np.cos(2 * np.pi * sample_features) + 0.1,\n",
    "        100,\n",
    "    )\n",
    "    sample_features_reshaped = sample_features.reshape(-1, 1)\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    model = LinearRegression().fit(sample_features_reshaped, sample_targets)\n",
    "\n",
    "    # Store the prediction at x = 0.5\n",
    "    predictions.append(model.predict(test_point)[0])\n",
    "\n",
    "# Plot a histogram of predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(predictions, bins=30, edgecolor=\"black\")\n",
    "plt.xlabel(\"Predicted f_hat(0.5, D_i)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Predictions for f_hat(0.5, D_i)\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(predictions) == 500, \"predictions should have 500 values\"\n",
    "assert all(\n",
    "    -1 < p < 2 for p in predictions\n",
    "), \"Predictions should be in a reasonable range for this regression problem\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert 0.4 < np.mean(predictions) < 0.6, \"Mean of predictions should be around 0.5\"\n",
    "assert np.var(predictions) < 0.02, \"Variance of predictions should be small\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3e:** Estimator Bias\n",
    "\n",
    "Compute the mean of your 500 predictions at $x = 0.5$ and store it in a variable called `mean_prediction`. Compare this to the true value $f(0.5) = -0.5$ from Problem 3a. What does this suggest about the bias of the OLS estimator for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "mean_prediction = np.mean(predictions)\n",
    "print(f\"Mean prediction at x=0.5: {mean_prediction:.4f}\")\n",
    "print(\"True value f(0.5): -0.5\")\n",
    "print(f\"Difference: {mean_prediction - (-0.5):.4f}\")\n",
    "print()\n",
    "print(\"The mean prediction (~0.5) differs greatly from the true value (-0.5).\")\n",
    "print(\"This large difference suggests high bias in the OLS estimator for this nonlinear problem.\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert -1 < mean_prediction < 2, \"mean_prediction should be in a reasonable range\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    abs(mean_prediction - np.mean(predictions)) < 1e-10\n",
    "), \"mean_prediction should equal np.mean(predictions)\"\n",
    "assert 0.4 < mean_prediction < 0.6, \"Mean prediction should be around 0.5\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3f:** Estimator Variance\n",
    "\n",
    "Compute the variance of your 500 predictions at $x = 0.5$ and store it in a variable called `estimator_variance`. What does this value suggest about the consistency of the OLS estimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "estimator_variance = np.var(predictions)\n",
    "print(f\"Variance of predictions: {estimator_variance:.6f}\")\n",
    "print()\n",
    "print(\"The variance is quite small (~0.005), suggesting the OLS estimator is consistent\")\n",
    "print(\"across different samples. However, this consistency does not imply accuracyâ€”\")\n",
    "print(\"as shown in 3e, the estimator may be consistently biased.\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert estimator_variance > 0, \"estimator_variance should be positive\"\n",
    "assert estimator_variance < 0.02, f\"estimator_variance should be small, got {estimator_variance}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    abs(estimator_variance - np.var(predictions)) < 1e-10\n",
    "), \"estimator_variance should equal np.var(predictions)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3g:** MSPE Decomposition\n",
    "\n",
    "Using the bias-variance decomposition:\n",
    "\n",
    "$$\\mathrm{MSPE} = (\\text{irreducible error}) + (\\text{estimator variance}) + (\\text{estimator bias})^2$$\n",
    "\n",
    "Compute the estimated MSPE at $x = 0.5$ and store it in a variable called `mspe`. Use:\n",
    "- Irreducible error = $\\mathrm{Var}(Y | X = 0.5)$ from Problem 3b (store in `irreducible_error`)\n",
    "- Estimator variance from Problem 3f (store in `estimator_variance`)\n",
    "- Estimator bias = mean prediction - $f(0.5)$ from Problem 3e (store in `estimator_bias`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Irreducible error (variance of uniform on interval of width 0.2)\n",
    "irreducible_error = (0.1 - (-0.1)) ** 2 / 12\n",
    "\n",
    "# Estimator variance from predictions\n",
    "estimator_variance = np.var(predictions)\n",
    "\n",
    "# Estimator bias: difference between mean prediction and true f(0.5)\n",
    "estimator_bias = np.mean(predictions) - (-0.5)\n",
    "\n",
    "# MSPE decomposition\n",
    "mspe = irreducible_error + estimator_variance + estimator_bias**2\n",
    "mspe\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert abs(irreducible_error - 0.00333) < 0.001, \"irreducible_error should be approximately 0.00333\"\n",
    "assert estimator_variance > 0, \"estimator_variance should be positive\"\n",
    "assert estimator_bias > 0.9, \"estimator_bias should be approximately 1 (high bias)\"\n",
    "assert mspe > 0.9, \"MSPE should be dominated by the squared bias term\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "expected_mspe = irreducible_error + estimator_variance + estimator_bias**2\n",
    "assert abs(mspe - expected_mspe) < 1e-10, \"MSPE should equal the sum of components\"\n",
    "assert mspe < 1.1, \"MSPE should be less than 1.1\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
