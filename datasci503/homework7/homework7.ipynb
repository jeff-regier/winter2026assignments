{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 503, Homework 7: Ensemble Methods and Decision Trees\n",
    "\n",
    "This assignment covers classification trees, random forests, and gradient boosting classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Majority Vote vs Average Probability\n",
    "\n",
    "Suppose we produce ten bootstrapped samples from a dataset containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X): 0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.\n",
    "\n",
    "There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability.\n",
    "\n",
    "**Question:** What is the final classification under each of these two approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Majority Vote Approach:**\n",
    "\n",
    "For the majority vote, we count how many estimates predict Red (probability > 0.5). The estimates above 0.5 are: 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75. That gives us 6 out of 10 votes for Red. Since 6/10 > 0.5, the majority decision is **Red**.\n",
    "\n",
    "**Average Probability Approach:**\n",
    "\n",
    "Taking the average of all probabilities:\n",
    "\n",
    "$$\\frac{0.1 + 0.15 + 0.2 + 0.2 + 0.55 + 0.6 + 0.6 + 0.65 + 0.7 + 0.75}{10} = \\frac{4.5}{10} = 0.45$$\n",
    "\n",
    "Since 0.45 < 0.5, the average probability approach predicts **Green**.\n",
    "\n",
    "**Conclusion:** The two methods give different predictions. Majority vote predicts Red, while average probability predicts Green. This discrepancy occurs because the four low probabilities (0.1, 0.15, 0.2, 0.2) are quite extreme and pull down the average, even though the majority of individual classifiers predict Red.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Gradient Boosting with Decision Stumps\n",
    "\n",
    "Consider using gradient boosting to solve a regression problem. Assume that at each iteration, we fit the residuals using a \"decision stump\": a decision tree with exactly two leaf nodes. In this case, the final estimate of the regression function can be expressed in the form:\n",
    "\n",
    "$$\\hat{f}(X) = \\sum_{j=1}^{p} \\hat{f}_j(X_j)$$\n",
    "\n",
    "**Question:** Explain why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "In gradient boosting, $\\hat{f}$ is constructed as a sum of weak learners:\n",
    "\n",
    "$$\\hat{f}(X) = \\lambda \\sum_{m=1}^{M} h_m(X)$$\n",
    "\n",
    "where each $h_m$ is a decision stump and $\\lambda$ is the learning rate.\n",
    "\n",
    "A decision stump can only make one split, so each $h_m$ effectively chooses some feature $k \\in \\{1, \\ldots, p\\}$ and a threshold $c$, then predicts:\n",
    "\n",
    "$$h_m(X) = \\begin{cases}\n",
    "a_1 & \\text{if } X_k < c \\\\\n",
    "a_2 & \\text{if } X_k \\geq c\n",
    "\\end{cases}$$\n",
    "\n",
    "where $a_1$ and $a_2$ are the predicted values (typically the average residuals in each leaf).\n",
    "\n",
    "Crucially, each stump depends on only **one** feature. We can regroup the stumps by which feature they split on. For each feature $j$, let $\\mathcal{S}_j$ be the set of stumps that split on feature $X_j$. Define:\n",
    "\n",
    "$$\\hat{f}_j(X_j) = \\lambda \\sum_{m \\in \\mathcal{S}_j} h_m(X)$$\n",
    "\n",
    "Since each $h_m$ in $\\mathcal{S}_j$ depends only on $X_j$, the function $\\hat{f}_j$ depends only on $X_j$.\n",
    "\n",
    "If some feature $X_j$ is never split on by any stump, we simply set $\\hat{f}_j(X_j) = 0$.\n",
    "\n",
    "This decomposition allows us to write:\n",
    "\n",
    "$$\\hat{f}(X) = \\sum_{j=1}^{p} \\hat{f}_j(X_j)$$\n",
    "\n",
    "This is an **additive model**, where the contribution of each feature is separable. This property makes gradient boosting with stumps particularly interpretable.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Classification Tree Sketches\n",
    "\n",
    "This problem contains hand-drawn sketches illustrating decision tree concepts.\n",
    "\n",
    "![Picture_A.jpg](attachment:Picture_A.jpg)\n",
    "\n",
    "![Picture_B.jpg](attachment:Picture_B.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The sketches above illustrate the partitioning of feature space by decision trees. These visualizations demonstrate how recursive binary splitting creates rectangular decision regions in 2D feature space.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crab Species Classification\n",
    "\n",
    "The following problems use the crabs dataset, which contains five size-related measurements of two different species of crabs (blue and orange). There are 50 male and 50 female crabs of each species. We will classify species based on the predictor variables and evaluate errors using the misclassification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Train-Test Split\n",
    "\n",
    "Load the crabs dataset and perform a train-test split with the following specifications:\n",
    "- Random state: 6789\n",
    "- Training set size: 80% of the dataset\n",
    "- Stratify the split according to both species and sex\n",
    "\n",
    "This stratification ensures that approximately the same proportions of species/sex combinations appear in both the training and test datasets.\n",
    "\n",
    "Store the results in `X_train`, `X_test`, `y_train`, and `y_test`. The features should include the five numerical measurements and sex (encoded as 0 for Female, 1 for Male). The target should be the species column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Load the crabs dataset\n",
    "crabs = pd.read_csv(\"data/crabs.csv\", index_col=[0])\n",
    "\n",
    "# Encode sex as numerical: Male=1, Female=0\n",
    "sex_mapping = {\"M\": 1, \"F\": 0}\n",
    "crabs[\"sex\"] = crabs[\"sex\"].map(sex_mapping)\n",
    "\n",
    "# Perform stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    crabs.drop([\"sp\", \"index\"], axis=1),\n",
    "    crabs[[\"sp\"]],\n",
    "    train_size=0.8,\n",
    "    random_state=6789,\n",
    "    stratify=crabs[[\"sex\", \"sp\"]],\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert X_train.shape == (160, 6), f\"X_train should have shape (160, 6), got {X_train.shape}\"\n",
    "assert X_test.shape == (40, 6), f\"X_test should have shape (40, 6), got {X_test.shape}\"\n",
    "assert y_train.shape == (160, 1), f\"y_train should have shape (160, 1), got {y_train.shape}\"\n",
    "assert y_test.shape == (40, 1), f\"y_test should have shape (40, 1), got {y_test.shape}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"sex\" in X_train.columns, \"X_train should include 'sex' column\"\n",
    "assert set(X_train[\"sex\"].unique()) == {0, 1}, \"Sex should be encoded as 0 and 1\"\n",
    "assert set(y_train[\"sp\"].unique()) == {\"B\", \"O\"}, \"Species should be 'B' and 'O'\"\n",
    "# Check stratification: roughly equal proportions in train and test\n",
    "train_ratio = y_train[\"sp\"].value_counts(normalize=True)\n",
    "test_ratio = y_test[\"sp\"].value_counts(normalize=True)\n",
    "assert abs(train_ratio[\"B\"] - test_ratio[\"B\"]) < 0.1, \"Stratification failed\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Decision Tree Classifier\n",
    "\n",
    "Train a classification tree to predict species from the five numerical measurements and sex. \n",
    "\n",
    "**(a)** Use cross-validation (5 folds) to select the optimal `max_leaf_nodes` from the values {2, 3, ..., 10}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Use GridSearchCV to find optimal max_leaf_nodes\n",
    "grid = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid={\"max_leaf_nodes\": [2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "tree_clf = grid.best_estimator_\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert hasattr(tree_clf, \"predict\"), \"tree_clf should be a fitted classifier\"\n",
    "assert tree_clf.max_leaf_nodes is not None, \"tree_clf should have max_leaf_nodes set\"\n",
    "assert 2 <= tree_clf.max_leaf_nodes <= 10, \"max_leaf_nodes should be between 2 and 10\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "train_acc = tree_clf.score(X_train, y_train)\n",
    "test_acc = tree_clf.score(X_test, y_test)\n",
    "assert train_acc > 0.8, \"Training accuracy should be above 80%\"\n",
    "assert test_acc > 0.7, \"Test accuracy should be above 70%\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Plot the tree and comment on which variables are used.\n",
    "\n",
    "**(c)** Compute and report training and test errors.\n",
    "\n",
    "Store the best tree in a variable called `tree_clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree and compute errors\n",
    "train_accuracy = tree_clf.score(X_train, y_train)\n",
    "test_accuracy = tree_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")\n",
    "print(f\"Training Error: {round(1 - train_accuracy, 4)}\")\n",
    "print(f\"Testing Error: {round(1 - test_accuracy, 4)}\")\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_tree(tree_clf, feature_names=X_train.columns, filled=True)\n",
    "plt.title(\"Decision Tree for Crab Species Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Analysis of Variables Used by the Tree:**\n",
    "\n",
    "The tree primarily uses BD (body depth), CW (carapace width), and FL (frontal lobe size). FL appears to be used most frequently for splitting. Notably, 'sex' is not a helpful predictor for species classification. RW (rear width) and CL (carapace length) are not used, likely because they are highly correlated with CW and FL, so the tree obtains similar information from the variables it does use.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Random Forest Classifier\n",
    "\n",
    "Train a random forest with the following specifications:\n",
    "- Use m=5 randomly selected predictors for each split (`max_features=5`)\n",
    "- Use 1000 trees (`n_estimators=1000`)\n",
    "\n",
    "**(a)** Make a variable importance plot.\n",
    "\n",
    "**(b)** Compare the variable importance with your results from the single decision tree.\n",
    "\n",
    "**(c)** Compute training and test errors.\n",
    "\n",
    "Store the random forest classifier in a variable called `rf_clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Train random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=1000, max_features=5, random_state=42)\n",
    "rf_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(f\"Training Accuracy: {rf_clf.score(X_train, y_train)}\")\n",
    "print(f\"Testing Accuracy: {rf_clf.score(X_test, y_test)}\")\n",
    "print(f\"Training Error: {round(1 - rf_clf.score(X_train, y_train), 4)}\")\n",
    "print(f\"Testing Error: {round(1 - rf_clf.score(X_test, y_test), 4)}\")\n",
    "\n",
    "# Create variable importance plot\n",
    "importance_order = np.argsort(rf_clf.feature_importances_)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    X_train.columns[importance_order],\n",
    "    width=rf_clf.feature_importances_[importance_order],\n",
    "    color=[\"#F9EAF9\", \"#E1C2E1\", \"#C8AAC8\", \"#AB90AB\", \"#917A91\", \"#6C596C\"],\n",
    "    edgecolor=\"#413E41\",\n",
    ")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.xlabel(\"Average Decrease in Impurity\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert hasattr(rf_clf, \"predict\"), \"rf_clf should be a fitted classifier\"\n",
    "assert rf_clf.n_estimators == 1000, \"Random forest should have 1000 trees\"\n",
    "assert rf_clf.max_features == 5, \"max_features should be 5\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "rf_train_acc = rf_clf.score(X_train, y_train)\n",
    "rf_test_acc = rf_clf.score(X_test, y_test)\n",
    "assert rf_train_acc > 0.9, \"RF training accuracy should be above 90%\"\n",
    "assert len(rf_clf.feature_importances_) == 6, \"Should have importance for all 6 features\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Comparison with Single Decision Tree:**\n",
    "\n",
    "The results differ from the single decision tree. In the decision tree, FL (frontal lobe size) was used frequently, but in the random forest importance ranking, it may be further down the list. BD (body depth) and CW (carapace width) remain important predictors. CL (carapace length) gains more usage in the random forest. With random subsets of features at each split, CL and RW become more useful when CW is not in the subset, due to the correlation between these measurements. This demonstrates how random forests can reveal the importance of correlated predictors that might be masked in a single tree.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Gradient Boosting Classifier\n",
    "\n",
    "Fit a `HistGradientBoostingClassifier` to the data. Store the histogram gradient boosting classifier (with `max_iter=1000`) in a variable called `hgb_clf`.\n",
    "\n",
    "**(a)** Plot the training and test errors as a function of the number of trees M, for M from 1 to 1000.\n",
    "\n",
    "**Hint:** Fit the classifier once with `max_iter=1000`, then use `staged_predict` to calculate the error at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Fit HistGradientBoostingClassifier\n",
    "hgb_clf = HistGradientBoostingClassifier(max_iter=1000, random_state=42)\n",
    "hgb_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculate errors at each iteration using staged_predict\n",
    "train_scores = [accuracy_score(y_train, output) for output in hgb_clf.staged_predict(X_train)]\n",
    "test_scores = [accuracy_score(y_test, output) for output in hgb_clf.staged_predict(X_test)]\n",
    "\n",
    "train_errors = np.ones(len(train_scores)) - np.array(train_scores)\n",
    "test_errors = np.ones(len(test_scores)) - np.array(test_scores)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert hasattr(hgb_clf, \"predict\"), \"hgb_clf should be a fitted classifier\"\n",
    "assert hgb_clf.max_iter == 1000, \"max_iter should be 1000\"\n",
    "assert len(train_scores) == 1000, \"Should have 1000 training scores\"\n",
    "assert len(test_scores) == 1000, \"Should have 1000 test scores\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert all(0 <= s <= 1 for s in train_scores), \"All train scores should be between 0 and 1\"\n",
    "assert all(0 <= s <= 1 for s in test_scores), \"All test scores should be between 0 and 1\"\n",
    "# Check that early iterations have worse performance than later ones (generally)\n",
    "assert train_scores[0] < train_scores[-1], \"Training should improve over iterations\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Error Curves for Histogram Gradient Boosting Classifier\")\n",
    "plt.plot(train_errors, color=\"#900F30\", label=\"Training Error\")\n",
    "plt.plot(test_errors, color=\"#571C3D\", label=\"Testing Error\")\n",
    "plt.xlabel(\"Number of Iterations (Number of Boosting Trees)\")\n",
    "plt.ylabel(\"Classification Error\")\n",
    "plt.axvline(x=40, color=\"grey\", linestyle=\"--\", label=\"Chosen M=40\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Choose an optimal value of M and justify your choice.\n",
    "\n",
    "**(c)** Report training and test errors for your chosen M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with optimal M and report errors\n",
    "optimal_m = 40  # SOLUTION\n",
    "hgb_optimal = HistGradientBoostingClassifier(max_iter=optimal_m, random_state=42)\n",
    "hgb_optimal.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(f\"Training Accuracy: {hgb_optimal.score(X_train, y_train)}\")\n",
    "print(f\"Testing Accuracy: {hgb_optimal.score(X_test, y_test)}\")\n",
    "print(f\"Training Error: {round(1 - hgb_optimal.score(X_train, y_train), 4)}\")\n",
    "print(f\"Testing Error: {round(1 - hgb_optimal.score(X_test, y_test), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Justification for M=40:**\n",
    "\n",
    "I chose M=40 because, according to the error plot, there is no significant improvement in test error beyond this point. In fact, continuing to add more trees may lead to overfitting (as evidenced by the training error continuing to decrease while test error plateaus or increases). Using M=40 reduces model complexity and improves computational efficiency without sacrificing predictive performance. With this setting, we obtain a test error of approximately 0.075 and a training error of approximately 0.031.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 8:** Method Comparison\n",
    "\n",
    "Comment on which method appears to perform best for this dataset and whether the results (training and test errors) are consistent across methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Comparison of Methods:**\n",
    "\n",
    "| Method | Training Error | Test Error |\n",
    "|--------|----------------|------------|\n",
    "| Decision Tree | ~0.06 | ~0.10 |\n",
    "| Random Forest | ~0.00 | ~0.15 |\n",
    "| Gradient Boosting (M=40) | ~0.03 | ~0.075 |\n",
    "\n",
    "**Best Performer:** Gradient Boosting appears to perform best on this dataset with a test error of approximately 0.075 (92.5% accuracy).\n",
    "\n",
    "**Consistency:** All methods achieve reasonable performance, but there are notable differences:\n",
    "- The single decision tree achieves ~90% test accuracy, which is quite good for an interpretable model.\n",
    "- The random forest achieves perfect training accuracy but slightly lower test accuracy (~85%), suggesting some overfitting.\n",
    "- Gradient boosting achieves the best balance between training and test performance.\n",
    "\n",
    "**Conclusion:** For this dataset, gradient boosting provides the best generalization, though all ensemble methods outperform (or match) the single decision tree. The relatively small dataset size (200 samples) may limit the ability of more complex models to show their full advantage.\n",
    "> END SOLUTION\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
