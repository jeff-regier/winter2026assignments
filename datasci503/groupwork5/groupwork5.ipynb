{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 503, Group Work 5: Cross Validation\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. During lab, feel free to flag down your GSI to ask questions at any point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this assignment, we will explore logistic regression and $k$-fold cross validation. In $k$-fold cross-validation, we partition a dataset into $k$ equally sized non-overlapping subsets $S$. For each subset $S_i$, a model is trained on $S \\setminus S_i$ and evaluated on $S_i$. The cross-validation estimator of the prediction's error is the average of the prediction errors obtained on each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example that will help us understand how CV works. We'll first import the relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, let's create a synthetic binary classification dataset using scikit-learn's `make_classification` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at two ways to do cross-validation. First we'll do it \"by hand.\" Then we'll show how sklearn makes it easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold CV by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we divide the data into 10 parts (also known as folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10  # 10-fold\n",
    "rng = np.random.default_rng(0)  # make a random number generator with fixed seed\n",
    "permutation = rng.permutation(len(X))  # create a shuffling of the indices of our data\n",
    "splits = np.split(permutation, n_splits)  # make folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., these are the *indices* of the datapoints in \"fold 3\"\n",
    "splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and these are the y values for the corresponding samples\n",
    "y[splits[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll assess predictive performance 10 times. Each time we'll hold out a different fold and use the rest of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll look at two metrics of predictive performance,\n",
    "# and store the results in these arrays\n",
    "missclass = np.zeros(len(splits))\n",
    "aurocs = np.zeros(len(splits))  # since this is binary classification, we can assess via auroc\n",
    "\n",
    "# iterate through folds\n",
    "for i in range(len(splits)):\n",
    "    # create test/train split for this held-out fold\n",
    "    folds_list = list(splits)  # copy list\n",
    "    test = folds_list.pop(i)  # pop out the ith fold\n",
    "    training = np.concatenate(folds_list)  # combine all remaining data for training\n",
    "\n",
    "    # fit model\n",
    "    model = LogisticRegression()  # make estimator\n",
    "    model.fit(X[training], y[training])  # fit estimator using the training data\n",
    "\n",
    "    # assess metric for predictive performance using test data\n",
    "    missclass[i] = np.mean(model.predict(X[test]) != y[test])  # get misclassification on test data\n",
    "    aurocs[i] = sklearn.metrics.roc_auc_score(\n",
    "        y[test], model.predict_proba(X[test])[:, 1]\n",
    "    )  # get auroc on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclass  # misclassification rate for each of the 10 test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aurocs  # auroc for each of the 10 test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report conclusions: mean(scores) +/- sqrt(tau^2/K)\n",
    "print(\n",
    "    f\"Mean misclassification rate: {missclass.mean():.4f}, \"\n",
    "    f\"plus minus {np.sqrt(missclass.var() / n_splits):.4f}\"\n",
    ")\n",
    "print(f\"Auroc: {aurocs.mean():.4f}, plus minus {np.sqrt(aurocs.var() / n_splits):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold CV Using sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the $k$-fold cross-validation configuration. For example, using 10 folds:\n",
    "\n",
    "* `n_splits=10` means 10-fold\n",
    "* `shuffle=True` meaning random folds\n",
    "* `random_state=42` means we'll get the same random folds each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at folds generated by this cross-validation splitter\n",
    "for i, (train, test) in enumerate(cv.split(X)):\n",
    "    # this automatically gives you the train indices and test indices\n",
    "    # without having to construct them yourselves by combining folds\n",
    "    print(f\"\\nsplit {i}\")\n",
    "    print(\"   test:   [\", \" \".join(test[:10].astype(\"U\")), \"...]\")  # test indices\n",
    "    print(\"   train:  [\", \" \".join(train[:10].astype(\"U\")), \"...]\")  # train indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, you don't even have to run the for loop yourself!\n",
    "\n",
    "With a single call to `cross_val_score`, we can evaluate the model using cross-validation. Here, we'll use accuracy as the performance metric, but you can choose other metrics like precision, recall, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies across folds\n",
    "scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy: {scores.mean():.4f}, plus minus {np.sqrt(scores.var() / len(scores)):.4f}\")\n",
    "print(\n",
    "    f\"Mean misclassification rate: {1 - scores.mean():.4f}, \"\n",
    "    f\"plus minus {np.sqrt(scores.var() / len(scores)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that conclusions are not exactly the same, because CV used different splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use multiple metrics, use `sklearn.model_selection.cross_validate` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info for each split\n",
    "results = pd.DataFrame(\n",
    "    sklearn.model_selection.cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=(\"accuracy\", \"roc_auc\"),\n",
    "        return_train_score=True,\n",
    "    )\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Auroc: {results.test_roc_auc.mean():.4f}, \"\n",
    "    f\"plus minus {np.sqrt(results.test_roc_auc.var() / len(results)):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Accuracy: {results.test_accuracy.mean():.4f}, \"\n",
    "    f\"plus minus {np.sqrt(results.test_accuracy.var() / len(results)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the results also include train performance (because we set `return_train_score=True`). It can sometimes be interesting to see discrepancy between train and test performance. Usually train performance is a bit better. If train performance is *a lot* better, your estimator may have \"too much flexibility\" (though sometimes you may also be experiencing so-called \"benign overfitting\" in which case your estimator is actually just fine...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Train auroc: {results.train_roc_auc.mean():.4f}, \"\n",
    "    f\"plus minus {np.sqrt(results.train_roc_auc.var() / len(results)):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Train accuracy: {results.train_accuracy.mean():.4f}, \"\n",
    "    f\"plus minus {np.sqrt(results.train_accuracy.var() / len(results)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Another Estimator (Using the Same Splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KNeighborsClassifier(5)\n",
    "scores2 = cross_val_score(\n",
    "    model2, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1\n",
    ")  # note, using same folds, cv\n",
    "\n",
    "print(\n",
    "    f\"Mean Accuracy: {scores2.mean():.4f}, plus minus {np.sqrt(scores2.var() / len(scores2)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 5-NN is worse, by a margin that is well in excess of the spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "It is very helpful to think about the bias-variance tradeoff in cross-validation. In CV, the number of folds to use (the value of $k$) is an important decision. Imagine repeating the learning procedure on multiple datasets. The lower the value for $k$, the higher the bias in the error estimates and the less variance **across datasets**. Conversely, when $k$ is set equal to the training+val sample size, the error estimate is then very low in bias but has the possibility of high variance **across datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? Some intuitions (but not mathematically rigorous proof) here:\n",
    "\n",
    "While there is no overlap between the test sets on which the models are evaluated, there is overlap between the training sets for all $k>2$. The overlap is largest for leave-one-out cross-validation. This means that the learned models are correlated, i.e., dependent, and the variance of the sum of correlated variables increases with the amount of covariance:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\sum_i X_i\\right) = \\sum_i \\sum_j \\text{Cov}(X_i, X_j)\n",
    "$$\n",
    "\n",
    "Therefore, leave-one-out cross-validation has large variance in comparison to CV with smaller $k$. To summarize, larger $k$ means less bias towards overestimating the true expected error (as training folds will be closer to the total dataset) but higher variance and higher running time (as you are getting closer to the limit case: Leave-One-Out CV).\n",
    "\n",
    "For more fun facts and simulation about bias-variance tradeoff and cross validation, please see [this post](https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation/357749#357749)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Group Work Problems\n",
    "\n",
    "But WHY do we even bother with cross-validation? What is the point?\n",
    "\n",
    "In this group work assignment, you'll perform K-fold cross validation on several classification models on the NHANES dataset. Additionally, you will be asked to write answers to explain WHY we do the things we do. Please feel free to ask instructors for guidance if cross-validation is still new to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1: Dataset Setup**\n",
    "\n",
    "Our favorite (and only) dataset we have used in the group work assignments is back! Please take the appropriate time to load in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Load the three NHANES datasets and merge them on SEQN\n",
    "bmx_df = pd.read_sas(\"data/NHANES/BMX_L.xpt\")\n",
    "demo_df = pd.read_sas(\"data/NHANES/DEMO_L.xpt\")\n",
    "hdl_df = pd.read_sas(\"data/NHANES/HDL_L.xpt\")\n",
    "\n",
    "# inner join on SEQN\n",
    "df = pd.merge(hdl_df, bmx_df, on=\"SEQN\", how=\"inner\")\n",
    "df = pd.merge(df, demo_df, on=\"SEQN\", how=\"inner\")\n",
    "df.head()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"df\" in dir(), \"The variable 'df' should be defined\"\n",
    "assert hasattr(df, \"shape\"), \"df should be a DataFrame\"\n",
    "assert df.shape[0] > 5000, \"The merged dataframe should have more than 5000 rows\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"SEQN\" in df.columns, \"SEQN column should be in the merged dataframe\"\n",
    "assert \"LBDHDD\" in df.columns, \"LBDHDD (HDL) should be in the merged dataframe\"\n",
    "assert \"BMXWT\" in df.columns, \"BMXWT (weight) should be in the merged dataframe\"\n",
    "assert \"RIDAGEYR\" in df.columns, \"RIDAGEYR (age) should be in the merged dataframe\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2: Variable Setup and Selection**\n",
    "\n",
    "For this problem, we will again try to predict individuals that have high-density lipoprotein (HDL) cholesterol of greater than 60. An HDL of 60 **mg/dL** or higher is often viewed as protective against heart diseaseâ€”this is typically the level you'd like to aim for, if possible. \n",
    "\n",
    "For this task please do the following:\n",
    "\n",
    "1. Create the binary indicator variable called `HDL>60`. Also, use the following features for predictive purposes: Gender, Age, Weight, Height, BMI, WaistSize, Household Size, and Ethnicity. You may need to refer to the docs to figure out their variable names:\n",
    "   - [HDL_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/HDL_L.htm)\n",
    "   - [DEMO_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.htm)\n",
    "   - [BMX_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/BMX_L.htm)\n",
    "\n",
    "2. Rename the variable names to be English-legible but still in Python variable style (e.g., `BMXWT` becomes `Weight`).\n",
    "\n",
    "3. Drop all missing values.\n",
    "\n",
    "Store the resulting DataFrame in a variable called `my_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Select relevant columns and rename them to English-legible names\n",
    "selected_columns = [\n",
    "    \"LBDHDD\",\n",
    "    \"RIAGENDR\",\n",
    "    \"RIDAGEYR\",\n",
    "    \"BMXWT\",\n",
    "    \"BMXHT\",\n",
    "    \"DMDHHSIZ\",\n",
    "    \"BMXBMI\",\n",
    "    \"BMXWAIST\",\n",
    "    \"RIDRETH1\",\n",
    "]\n",
    "filtered_data = df[selected_columns].copy()\n",
    "my_df = filtered_data.rename(\n",
    "    columns={\n",
    "        \"LBDHDD\": \"HDL\",\n",
    "        \"RIAGENDR\": \"Gender\",\n",
    "        \"RIDAGEYR\": \"Age\",\n",
    "        \"BMXWT\": \"Weight\",\n",
    "        \"BMXHT\": \"Height\",\n",
    "        \"BMXBMI\": \"BMI\",\n",
    "        \"BMXWAIST\": \"WaistSize\",\n",
    "        \"DMDHHSIZ\": \"HouseholdSize\",\n",
    "        \"RIDRETH1\": \"Ethnicity\",\n",
    "    }\n",
    ")\n",
    "my_df = my_df.dropna()\n",
    "my_df[\"HDL\"] = my_df[\"HDL\"] > 60.0\n",
    "my_df = my_df.rename(columns={\"HDL\": \"HDL>60\"})\n",
    "my_df.head()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"my_df\" in dir(), \"The variable 'my_df' should be defined\"\n",
    "assert my_df.shape[0] / df.shape[0] > 0.8, \"At least 80% of rows should remain after dropping NaN\"\n",
    "assert \"HDL>60\" in my_df.columns, \"my_df should have an 'HDL>60' column\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert my_df[\"HDL>60\"].dtype == bool, \"HDL>60 should be a boolean column\"\n",
    "assert \"Gender\" in my_df.columns, \"my_df should have a 'Gender' column\"\n",
    "assert \"Age\" in my_df.columns, \"my_df should have an 'Age' column\"\n",
    "assert \"Weight\" in my_df.columns, \"my_df should have a 'Weight' column\"\n",
    "assert \"Height\" in my_df.columns, \"my_df should have a 'Height' column\"\n",
    "assert \"BMI\" in my_df.columns, \"my_df should have a 'BMI' column\"\n",
    "assert \"WaistSize\" in my_df.columns, \"my_df should have a 'WaistSize' column\"\n",
    "assert \"HouseholdSize\" in my_df.columns, \"my_df should have a 'HouseholdSize' column\"\n",
    "assert \"Ethnicity\" in my_df.columns, \"my_df should have an 'Ethnicity' column\"\n",
    "assert not my_df.isna().any().any(), \"my_df should have no missing values\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3: Training and Testing Split**\n",
    "\n",
    "Please split your data into a train and test set, with 70% of observations in the train set and 30% in the test set.\n",
    "\n",
    "* Use the `train_test_split` function from sklearn.\n",
    "* Use `random_state=42`.\n",
    "* Stratify the sampling to include roughly the same distribution of response values in each set. [Why should we stratify?](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold)\n",
    "\n",
    "Store the results in `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Split data into features (X) and target (y), then into train/test sets\n",
    "X = my_df.drop(columns=[\"HDL>60\"])\n",
    "y = my_df[\"HDL>60\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"X_train\" in dir(), \"X_train should be defined\"\n",
    "assert \"X_test\" in dir(), \"X_test should be defined\"\n",
    "assert \"y_train\" in dir(), \"y_train should be defined\"\n",
    "assert \"y_test\" in dir(), \"y_test should be defined\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check approximate split ratio (70/30)\n",
    "total_samples = len(X_train) + len(X_test)\n",
    "train_ratio = len(X_train) / total_samples\n",
    "assert 0.69 < train_ratio < 0.71, f\"Train ratio should be ~0.7, got {train_ratio:.3f}\"\n",
    "# Check stratification worked (similar proportions)\n",
    "train_pos_rate = y_train.mean()\n",
    "test_pos_rate = y_test.mean()\n",
    "assert abs(train_pos_rate - test_pos_rate) < 0.02, \"Stratification should preserve class balance\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4: Implementing K-Fold CV**\n",
    "\n",
    "Write a function called `KFoldCV` that takes in 4 arguments:\n",
    "\n",
    "1. `X`: The predictors array.\n",
    "2. `y`: The response variable array.\n",
    "3. `model`: An sklearn model object on which we can call `fit` and `predict`.\n",
    "4. `K`: An integer representing the number of folds (default: 10).\n",
    "\n",
    "The function should return an array of classification accuracies for each fold.\n",
    "\n",
    "**Hint:** You can use sklearn's `KFold` and `cross_val_score` to simplify your implementation. Use `random_state=42` and `shuffle=True` in your `KFold` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCV(X, y, model, K=10):  # noqa: N802, N803\n",
    "    # BEGIN SOLUTION\n",
    "    # Use sklearn's KFold and cross_val_score for clean implementation\n",
    "    cv = KFold(n_splits=K, random_state=42, shuffle=True)\n",
    "    return cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_model = LogisticRegression(max_iter=500)\n",
    "test_scores = KFoldCV(X_train, y_train, test_model, K=5)\n",
    "assert len(test_scores) == 5, \"Should return 5 scores for K=5\"\n",
    "assert all(0 <= score <= 1 for score in test_scores), \"Scores should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with K=10\n",
    "test_scores_10 = KFoldCV(X_train, y_train, LogisticRegression(max_iter=500), K=10)\n",
    "assert len(test_scores_10) == 10, \"Should return 10 scores for K=10\"\n",
    "# Mean accuracy should be reasonable (better than random)\n",
    "assert test_scores_10.mean() > 0.6, \"Mean accuracy should be better than random\"\n",
    "# Test with different model\n",
    "knn_scores = KFoldCV(X_train, y_train, KNeighborsClassifier(n_neighbors=5), K=5)\n",
    "assert len(knn_scores) == 5, \"Should work with KNN model too\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5a: Execution**\n",
    "\n",
    "Run your `KFoldCV` function using Logistic Regression with $K=10$.\n",
    "\n",
    "Store the result in a variable called `cv_scores` and make sure the output is visible in the notebook.\n",
    "\n",
    "**Note:** If you get a warning that the model has not reached convergence, add the argument `max_iter=1000` to the `LogisticRegression` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Run 10-fold CV with logistic regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "cv_scores = KFoldCV(X_train, y_train, model)\n",
    "cv_scores\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"cv_scores\" in dir(), \"cv_scores should be defined\"\n",
    "assert len(cv_scores) == 10, \"Should have 10 fold scores\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert cv_scores.mean() > 0.7, \"Mean CV accuracy should be above 0.7\"\n",
    "assert cv_scores.std() < 0.1, \"CV scores should not have too much variance\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5b: Generalization**\n",
    "\n",
    "One motivation for cross-validation is to assess the generalizability of your model on unseen data.\n",
    "\n",
    "Explain in at most two sentences whether you believe your model generalizes well on unseen data. Reference the output of your function to make your case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The model shows reasonably consistent performance across all 10 folds, with accuracies ranging from approximately 71% to 80% and a mean around 76%. This consistency suggests the model generalizes fairly well to unseen data, as there are no individual folds with dramatically lower performance that would indicate overfitting to particular subsets of the training data.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 6a: Working with Regularization**\n",
    "\n",
    "It turns out that if you look at the LogisticRegression module in sklearn, it uses an L2 penalty by default! You can see for yourself on the [documentation page for sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "One problem: we just accepted the default regularization term `C=1.0`. How do we know this was the right choice? Let's find out.\n",
    "\n",
    "Write a function called `KFoldCV_L2` that performs K-Fold validation with different values of the regularization parameter $C$. The values to consider should be $10^{-5}, 10^{-4}, \\ldots, 10^{4}$ (i.e., 10 values total).\n",
    "\n",
    "The function should take as input `X`, `y`, and `K` (default 10) and return a dictionary with C values as keys and mean validation accuracy as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCV_L2(X, y, K=10):  # noqa: N802, N803\n",
    "    \"\"\"\n",
    "    Perform K-fold CV for logistic regression with different L2 regularization strengths.\n",
    "\n",
    "    Parameters:\n",
    "        X: pandas DataFrame of features\n",
    "        y: pandas Series of labels\n",
    "        K: number of folds (default 10)\n",
    "\n",
    "    Returns:\n",
    "        dict: C values as keys, mean validation accuracy as values\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Perform K-fold CV for each regularization strength C\n",
    "    cv = KFold(n_splits=K, random_state=42, shuffle=True)\n",
    "\n",
    "    results = {}\n",
    "    C_values = [10**i for i in range(-5, 5)]\n",
    "    for C in C_values:\n",
    "        validation_acc = []\n",
    "        for train_idx, val_idx in cv.split(X):\n",
    "            model = LogisticRegression(C=C, max_iter=500)\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            y_pred = model.predict(X.iloc[val_idx])\n",
    "            validation_acc.append(accuracy_score(y.iloc[val_idx], y_pred))\n",
    "\n",
    "        results[C] = np.mean(validation_acc)\n",
    "    return results\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "l2_results = KFoldCV_L2(X_train, y_train)\n",
    "assert isinstance(l2_results, dict), \"Should return a dictionary\"\n",
    "assert len(l2_results) == 10, \"Should have results for 10 C values\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert 1e-05 in l2_results, \"Should include C=1e-05\"\n",
    "assert 10000 in l2_results, \"Should include C=10000\"\n",
    "assert all(0 <= v <= 1 for v in l2_results.values()), \"All accuracies should be between 0 and 1\"\n",
    "# Higher C (less regularization) should generally perform at least as well for this data\n",
    "assert l2_results[1] >= l2_results[1e-05] - 0.05, \"Weak regularization should not hurt much\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "l2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 6b: Evaluation**\n",
    "\n",
    "Based on your cross-validation, do you believe that regularization plays an effect on the predictive accuracy? What level of regularization should we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Yes, regularization does affect predictive accuracy, but mainly for very strong regularization (small C values like 1e-05 to 0.01). The accuracy improves as C increases (weaker regularization) up to around C=1, after which it plateaus. We should choose C=1 (or any value from 1 to 10000) since they all achieve similar peak performance, and C=1 is the simplest default choice that provides adequate regularization without sacrificing accuracy.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7a: Train and Test Evaluation**\n",
    "\n",
    "Please now retrain your final model (with the best regularization value you found in the last part) on all training data. Then, evaluate your model's performance on the test set.\n",
    "\n",
    "Store the trained model in `final_model` and the test accuracy in `test_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Train the final model with best C on all training data and evaluate on test set\n",
    "final_model = LogisticRegression(C=1, max_iter=500)\n",
    "final_model.fit(X_train, y_train)\n",
    "test_accuracy = final_model.score(X_test, y_test)\n",
    "test_accuracy\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"final_model\" in dir(), \"final_model should be defined\"\n",
    "assert \"test_accuracy\" in dir(), \"test_accuracy should be defined\"\n",
    "assert 0 <= test_accuracy <= 1, \"test_accuracy should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(final_model, \"predict\"), \"final_model should be a trained model\"\n",
    "assert test_accuracy > 0.7, \"Test accuracy should be above 0.7\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7b: Retraining Justification**\n",
    "\n",
    "Explain in 1-2 sentences **maximum** why we retrain the model on the full training set (all data except the test set) before final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "We retrain on the full training data because more training data generally leads to better model performance. Cross-validation was only used to select the best hyperparameters (regularization strength), and once that decision is made, we want to use all available training data to fit the final model.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7c: Performance Evaluation**\n",
    "\n",
    "Is logistic regression performing well on this data? Explain in 1-2 sentences **maximum**. You may need some supporting code for your argument.\n",
    "\n",
    "**Hint:** Think about what a trivial baseline would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Calculate baseline accuracy (always predicting the majority class)\n",
    "baseline_accuracy = 1 - y_test.mean()\n",
    "print(f\"Baseline accuracy (always predict False): {baseline_accuracy:.4f}\")\n",
    "print(f\"Logistic regression test accuracy: {test_accuracy:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"baseline_accuracy\" in dir(), \"baseline_accuracy should be defined\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert 0.6 < baseline_accuracy < 0.8, \"Baseline accuracy should be between 0.6 and 0.8\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Logistic regression is performing modestly well but not impressively. It achieves around 76% accuracy compared to a trivial baseline of about 73% (always predicting the majority class), so it is only marginally better than simply predicting that nobody has high HDL.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 8: K-Folds on K-Nearest Neighbors**\n",
    "\n",
    "Using your `KFoldCV` function you created in Problem 4, please run CV on 2-NN classification (K-Nearest Neighbors with `n_neighbors=2`).\n",
    "\n",
    "Store the result in a variable called `knn_cv_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Run 10-fold CV with 2-NN classifier\n",
    "knn_cv_scores = KFoldCV(\n",
    "    X_train, y_train, KNeighborsClassifier(n_neighbors=2, weights=\"distance\"), K=10\n",
    ")\n",
    "knn_cv_scores\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"knn_cv_scores\" in dir(), \"knn_cv_scores should be defined\"\n",
    "assert len(knn_cv_scores) == 10, \"Should have 10 fold scores\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert all(0 <= s <= 1 for s in knn_cv_scores), \"All scores should be between 0 and 1\"\n",
    "assert knn_cv_scores.mean() > 0.5, \"Mean accuracy should be better than random\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 9: Finding the Right K for KNN**\n",
    "\n",
    "Similar to what you did for regularized logistic regression, please run a 10-fold CV that assesses model performance for 10 different values of `n_neighbors`: [1, 3, 5, 10, 15, 20, 25, 35, 50, 100]. You should evaluate each fold with each `n_neighbors` value.\n",
    "\n",
    "Write a function called `KFoldCV_NN` that takes as input `X`, `y`, and `K` (default 10) and returns a dictionary. The dictionary should have the number of neighbors considered as keys and the mean validation accuracy as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCV_NN(X, y, K=10):  # noqa: N802, N803\n",
    "    \"\"\"\n",
    "    Perform K-fold CV for KNN with different numbers of neighbors.\n",
    "\n",
    "    Parameters:\n",
    "        X: pandas DataFrame of features\n",
    "        y: pandas Series of labels\n",
    "        K: number of folds (default 10)\n",
    "\n",
    "    Returns:\n",
    "        dict: n_neighbors values as keys, mean validation accuracy as values\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Perform K-fold CV for each number of neighbors\n",
    "    cv = KFold(n_splits=K, random_state=42, shuffle=True)\n",
    "\n",
    "    results = {}\n",
    "    num_neighbors = [1, 3, 5, 10, 15, 20, 25, 35, 50, 100]\n",
    "    for k in num_neighbors:\n",
    "        validation_acc = []\n",
    "        for train_idx, val_idx in cv.split(X):\n",
    "            model = KNeighborsClassifier(n_neighbors=k, weights=\"distance\")\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            y_pred = model.predict(X.iloc[val_idx])\n",
    "            validation_acc.append(accuracy_score(y.iloc[val_idx], y_pred))\n",
    "\n",
    "        results[k] = np.mean(validation_acc)\n",
    "    return results\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "nn_results = KFoldCV_NN(X_train, y_train)\n",
    "assert isinstance(nn_results, dict), \"Should return a dictionary\"\n",
    "assert len(nn_results) == 10, \"Should have results for 10 n_neighbors values\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert 1 in nn_results, \"Should include n_neighbors=1\"\n",
    "assert 100 in nn_results, \"Should include n_neighbors=100\"\n",
    "assert all(0 <= v <= 1 for v in nn_results.values()), \"All accuracies should be between 0 and 1\"\n",
    "# 1-NN typically has lower accuracy due to overfitting\n",
    "assert nn_results[5] >= nn_results[1] - 0.05, \"5-NN should be comparable or better than 1-NN\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "nn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 10: Retraining KNN on Training Data**\n",
    "\n",
    "Retrain your KNN model on the full training set. Make sure you use the best number of neighbors as determined from the previous part.\n",
    "\n",
    "Store the best number of neighbors in `BEST_K` and the trained model in `knn_final_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Find best K and train final KNN model\n",
    "BEST_K = max(nn_results, key=nn_results.get)  # SOLUTION\n",
    "\n",
    "knn_final_model = KNeighborsClassifier(n_neighbors=BEST_K, weights=\"distance\")\n",
    "knn_final_model.fit(X_train, y_train)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"BEST_K\" in dir(), \"BEST_K should be defined\"\n",
    "assert \"knn_final_model\" in dir(), \"knn_final_model should be defined\"\n",
    "assert BEST_K in [1, 3, 5, 10, 15, 20, 25, 35, 50, 100], \"BEST_K should be one of the tested values\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(knn_final_model, \"predict\"), \"knn_final_model should be a trained model\"\n",
    "assert knn_final_model.n_neighbors == BEST_K, \"Model should use BEST_K neighbors\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 11: Test Set Evaluation for KNN**\n",
    "\n",
    "Evaluate your final KNN model's performance on the test set.\n",
    "\n",
    "Store the test accuracy in `knn_test_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Evaluate KNN on test set\n",
    "knn_test_accuracy = knn_final_model.score(X_test, y_test)\n",
    "knn_test_accuracy\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"knn_test_accuracy\" in dir(), \"knn_test_accuracy should be defined\"\n",
    "assert 0 <= knn_test_accuracy <= 1, \"knn_test_accuracy should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert knn_test_accuracy > 0.65, \"KNN test accuracy should be reasonable\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
