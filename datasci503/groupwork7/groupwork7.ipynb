{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9nNGLo5hVGI"
   },
   "source": [
    "# DATASCI 503, Group Work 7: Trees and Tree Ensembles\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. During lab, feel free to flag down your GSI to ask questions at any point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2o3pCxkyhf1"
   },
   "source": [
    "##  Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vERvXRA_J57"
   },
   "source": [
    "A **classification tree** is built through a process known as binary recursive partitioning. The main idea is as follows:\n",
    "* Recursively partition the input space into rectangular boxes\n",
    "* At each step, ask a question about one variable (split the feature space into parts)\n",
    "* Repeat for each branch (recursively partition the feature space into boxes)\n",
    "* Goal: each box should contain data points mostly from the same class\n",
    "* Each box is labelled with its majority class\n",
    "* The end result: a tree of splits, a partitioning of the variable space into boxes and assignment of a class label to each box\n",
    "\n",
    "\n",
    "\n",
    "Scikit-learn implements CART, whose fundamental principles and methodologies behind its decision tree algorithms are largely based on the concepts introduced by Breiman et al. in the book Classification and Regression Trees.\n",
    "\n",
    "Additionally, scikit-learn enhances the basic CART algorithm with several modern features like support for missing values, various criteria for splitting (Gini impurity, entropy for classification, and mean squared error, mean absolute error for regression), and pre-pruning options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2gHtGly1k59"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rAZmPrB8s7l"
   },
   "source": [
    "We look at the spam email classification problem. This dataset is available at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/spambase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH8p9O3s7QP0"
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/94/spambase.zip\"\n",
    "response = requests.get(url, timeout=30)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "z.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGaEEKw68hy9"
   },
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    \"word_freq_make\",\n",
    "    \"word_freq_address\",\n",
    "    \"word_freq_all\",\n",
    "    \"word_freq_3d\",\n",
    "    \"word_freq_our\",\n",
    "    \"word_freq_over\",\n",
    "    \"word_freq_remove\",\n",
    "    \"word_freq_internet\",\n",
    "    \"word_freq_order\",\n",
    "    \"word_freq_mail\",\n",
    "    \"word_freq_receive\",\n",
    "    \"word_freq_will\",\n",
    "    \"word_freq_people\",\n",
    "    \"word_freq_report\",\n",
    "    \"word_freq_addresses\",\n",
    "    \"word_freq_free\",\n",
    "    \"word_freq_business\",\n",
    "    \"word_freq_email\",\n",
    "    \"word_freq_you\",\n",
    "    \"word_freq_credit\",\n",
    "    \"word_freq_your\",\n",
    "    \"word_freq_font\",\n",
    "    \"word_freq_000\",\n",
    "    \"word_freq_money\",\n",
    "    \"word_freq_hp\",\n",
    "    \"word_freq_hpl\",\n",
    "    \"word_freq_george\",\n",
    "    \"word_freq_650\",\n",
    "    \"word_freq_lab\",\n",
    "    \"word_freq_labs\",\n",
    "    \"word_freq_telnet\",\n",
    "    \"word_freq_857\",\n",
    "    \"word_freq_data\",\n",
    "    \"word_freq_415\",\n",
    "    \"word_freq_85\",\n",
    "    \"word_freq_technology\",\n",
    "    \"word_freq_1999\",\n",
    "    \"word_freq_parts\",\n",
    "    \"word_freq_pm\",\n",
    "    \"word_freq_direct\",\n",
    "    \"word_freq_cs\",\n",
    "    \"word_freq_meeting\",\n",
    "    \"word_freq_original\",\n",
    "    \"word_freq_project\",\n",
    "    \"word_freq_re\",\n",
    "    \"word_freq_edu\",\n",
    "    \"word_freq_table\",\n",
    "    \"word_freq_conference\",\n",
    "    \"char_freq_;\",\n",
    "    \"char_freq_(\",\n",
    "    \"char_freq_[\",\n",
    "    \"char_freq_!\",\n",
    "    \"char_freq_$\",\n",
    "    \"char_freq_#\",\n",
    "    \"capital_run_length_average\",\n",
    "    \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\",\n",
    "    \"is_spam\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OAeiUC665jM"
   },
   "outputs": [],
   "source": [
    "spam_data = pd.read_csv(\"data/spambase.data\", names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCKJEEm_80M0"
   },
   "source": [
    "In the dataset, there are 57 continuous variables as input variables and 1 binary outcome variable. The variable information is given as follows:\n",
    "\n",
    "``word_freq_WORD``: percentage of words in the email that match WORD (48 variables taking value in [0,100]);\n",
    "\n",
    "``char_freq_CHAR``: percentage of characters in the email that match CHAR (6 variables taking value in [0,100]);\n",
    "\n",
    "``capital_run_length_average``: average length of uninterrupted sequences of capital letters;\n",
    "\n",
    "``capital_run_length_longest``: length of longest uninterrupted sequence of capital letters;\n",
    "\n",
    "``capital_run_length_total``: total number of capital letters in the email;\n",
    "\n",
    "``spam``: denotes whether the email was considered spam (1) or not (0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiRWYSwR9ZJE"
   },
   "outputs": [],
   "source": [
    "spam_train, spam_test = train_test_split(\n",
    "    spam_data, test_size=0.3, random_state=1, stratify=spam_data[\"is_spam\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jApK0sQm9k0r"
   },
   "source": [
    "Let us take a look at the word frequencies. In a word vector, words appearing too frequently or too rarely can usually be useless for making predictions. We order the first 48 columns accroding to their mean frequency. We will use tree model later to confirm if our intuition is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740762283748,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "XZLCOs2T9oG-",
    "outputId": "59b034b1-dca6-4561-a486-2e6dcb020e57"
   },
   "outputs": [],
   "source": [
    "# Calculate the column mean for the first 48 columns which corresponds to word frequency\n",
    "word_freq_mean = spam_train.mean(axis=0)[0:48]\n",
    "\n",
    "# Change the order by the frequency value\n",
    "word_freq_mean_sort = word_freq_mean.sort_values(ascending=True)\n",
    "word_freq_mean_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_iFiXjy9yur"
   },
   "source": [
    "It’s interesting to see that the word ``george`` has high frequency. This is because George is the donor of the dataset. We may see later that ``hp``, the place George works, is selected by tree model. We should note that using those two words may causes generalization problem when we apply the model to emails of other users. We also find the labels are slightly unbalanced. This reminds us to adjust the output label distribution when we generalize the model to a broader application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1740762283813,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "wKYcjk9896Rc",
    "outputId": "fb4e29c4-e3b7-445b-c0a5-6d0b541bd2f9"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=spam_train[\"is_spam\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG8KS2GF-BMU"
   },
   "source": [
    "You can check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) for `DecisionTreeClassifier`. Here are some arguments we use:\n",
    "\n",
    "- ``criterion``: {“gini”, “entropy”, “log_loss”}, default= \"gini\"\n",
    "\n",
    "The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the Shannon information gain, see Mathematical formulation.\n",
    "\n",
    "- ``class_weight``: dict, list of dict or \"balanced\", default=None\n",
    "\n",
    "Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "- ``ccp_alpha``: non-negative float, default=0.0\n",
    "\n",
    "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
    "\n",
    "We will explain each argument in details later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIqvXSDh-Tlj"
   },
   "outputs": [],
   "source": [
    "X_spam_train = spam_train.drop([\"is_spam\"], axis=1)\n",
    "y_spam_train = spam_train[\"is_spam\"]\n",
    "X_spam_test = spam_test.drop([\"is_spam\"], axis=1)\n",
    "y_spam_test = spam_test[\"is_spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWKTN2Yl-E7z"
   },
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(random_state=0)\n",
    "tree1 = tree1.fit(X_spam_train, y_spam_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740762283876,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "IydZ4quV-HrN",
    "outputId": "bcc199e8-dbe7-41d8-b1d6-e0499dac825f"
   },
   "outputs": [],
   "source": [
    "training_error = 1 - tree1.score(X_spam_train, y_spam_train)\n",
    "test_error = 1 - tree1.score(X_spam_test, y_spam_test)\n",
    "print(\"training error is\", training_error)\n",
    "print(\"test error is\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIqdp9xI-KWc"
   },
   "source": [
    "One of the most attractive properties of trees is that they can be graphically displayed. We use the ``plot_tree()`` function to depict the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 12811,
     "status": "ok",
     "timestamp": 1740762296686,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "2z6tRe0V-JGM",
    "outputId": "952d8404-8911-41e7-9429-e296540b3400"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(tree1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d1U4den_YHt"
   },
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 13032,
     "status": "ok",
     "timestamp": 1740762309829,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "nVpIQxWv_agF",
    "outputId": "f3abe7d6-ff86-4fae-ee32-91a1b00d2887"
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(\n",
    "    tree1,\n",
    "    out_file=None,\n",
    "    feature_names=column_names[:-1],\n",
    "    class_names=[\"not_spam\", \"spam\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"decision_tree.dot\", format=\"png\", cleanup=True)  # Save and render the graph as PNG\n",
    "# graph  # Display the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn9Y29e5-vYG"
   },
   "source": [
    "Since we didn't penalize the size of the tree, we generated an extremely complex model. The length (or depth) of each branch is proportional to the reduction in impurity (or quality of split) of the corresponding split. There is a very long branch in our complex model.\n",
    "\n",
    "As mentioned in the lecture, we prune a tree by finding a sub-tree $T$ that minimizes:\n",
    "$$\n",
    "  C(T)=\\sum_{t=1}^{|T|} N_{t} \\cdot \\text{Impurity} (t)+c_{p} \\cdot|T|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL_cqxJyykZH"
   },
   "source": [
    "## Comparing Different Splitting Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8NZVakL_Nrp"
   },
   "source": [
    "\n",
    "There are multiple options for splitting measure. Previously we have implemented Gini index. Now we try the entropy($-\\sum_{k=1}^{K} p_{k}(m) \\log p_{k}(m)$) splitting measure.\n",
    "\n",
    "We modify the argument ``criterion`` in ``DecisionTreeClassifier``:\n",
    "\n",
    "-``criterion``: {“gini”, “entropy”, “log_loss”}, default= ”gini”\n",
    "\n",
    "Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see [Mathematical formulation](https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1740762309992,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "aUepTDayBhkm",
    "outputId": "f0cc0dcb-258f-4a7b-d2d6-b7e8e4a472e7"
   },
   "outputs": [],
   "source": [
    "tree2 = DecisionTreeClassifier(random_state=0, criterion=\"entropy\")\n",
    "tree2 = tree2.fit(X_spam_train, y_spam_train)\n",
    "\n",
    "training_error = 1 - tree2.score(X_spam_train, y_spam_train)\n",
    "test_error = 1 - tree2.score(X_spam_test, y_spam_test)\n",
    "print(\"training error is\", training_error)\n",
    "print(\"test error is\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNpUP-LIBk-E"
   },
   "source": [
    "Using cross-entropy, we get smaller test error compared with that we obtained using Gini index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnFxbNNvBmQ2"
   },
   "source": [
    "There are two types of misclassification, i.e. to misclassify ``Spam`` as ``Non-Spam`` and to misclassify ``Non-Spam`` as ``Spam``. The number of two types of errors can be unbalanced.\n",
    "\n",
    "Our model tends to make the error of predicting ``Spam`` to be ``Non-Spam``, since in our training set ``Non-Spam`` is the dominant class. This can be a problem when we want to control certain type of errors.\n",
    "\n",
    "We can assign weights to adjust the unbalance to reduce the another type of error. In our case, since training and test set have the same label distribution, the true test error might increase slightly.\n",
    "\n",
    "To assign weights, we modify the argument ``class_weight`` in ``DecisionTreeClassifier``:\n",
    "\n",
    "- ``class_weight``: dict, list of dict or “balanced”, default=None\n",
    "\n",
    "Explanation of this argument: Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740762310004,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "y6vHOxjzBq-T",
    "outputId": "9e5391ca-8138-42ce-a1f8-ae54f79f71d6"
   },
   "outputs": [],
   "source": [
    "y_spam_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1740762310075,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "EmzTLA79Bsjs",
    "outputId": "572475ee-786e-413f-8e6e-22291c9e3a09"
   },
   "outputs": [],
   "source": [
    "proportion_non_spam = y_spam_train.value_counts()[0] / y_spam_train.shape[0]\n",
    "proportion_spam = y_spam_train.value_counts()[1] / y_spam_train.shape[0]\n",
    "proportion_non_spam / proportion_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1740762311033,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "PHMbrn9wBx-L",
    "outputId": "f7dd4976-8f20-4c5a-ee79-84c8e1457129"
   },
   "outputs": [],
   "source": [
    "# Model without balancing labels\n",
    "tree2_no_weight = DecisionTreeClassifier(random_state=0)\n",
    "tree2_no_weight.fit(X_spam_train, y_spam_train)\n",
    "\n",
    "training_error = 1 - tree2_no_weight.score(X_spam_train, y_spam_train)\n",
    "test_error = 1 - tree2_no_weight.score(X_spam_test, y_spam_test)\n",
    "\n",
    "print(\"training error is\", training_error)\n",
    "print(\"test error is\", test_error)\n",
    "\n",
    "y_pred_test = tree2_no_weight.predict(X_spam_test)\n",
    "cm = confusion_matrix(y_spam_test, y_pred_test)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tree2_no_weight.classes_)\n",
    "cm_display.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1740762312114,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "2m4w-RW-BzXb",
    "outputId": "8e249606-b15f-46c9-fc74-930a15bc791f"
   },
   "outputs": [],
   "source": [
    "# Model with balancing labels\n",
    "tree2_weight = DecisionTreeClassifier(\n",
    "    random_state=0, class_weight={0: 1, 1: proportion_non_spam / proportion_spam}\n",
    ")\n",
    "tree2_weight.fit(X_spam_train, y_spam_train)\n",
    "\n",
    "training_error = 1 - tree2_weight.score(X_spam_train, y_spam_train)\n",
    "test_error = 1 - tree2_weight.score(X_spam_test, y_spam_test)\n",
    "print(\"training error is\", training_error)\n",
    "print(\"test error is\", test_error)\n",
    "\n",
    "y_pred_test = tree2_weight.predict(X_spam_test)\n",
    "cm = confusion_matrix(y_spam_test, y_pred_test)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tree2_weight.classes_)\n",
    "cm_display.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l-bDojDDVlq"
   },
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1740762313061,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "oVJQewD8l1hO",
    "outputId": "b8180227-5862-46a8-9e0e-aecd9dc4f4dc"
   },
   "outputs": [],
   "source": [
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(200 * rng.rand(100, 1) - 100, axis=0)\n",
    "y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\n",
    "y[::5, :] += 0.5 - rng.rand(20, 2)\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_3 = DecisionTreeRegressor(max_depth=8)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(-100.0, 100.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "y_3 = regr_3.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "s = 25\n",
    "plt.scatter(y[:, 0], y[:, 1], c=\"navy\", s=s, edgecolor=\"black\", label=\"data\")\n",
    "plt.scatter(\n",
    "    y_1[:, 0],\n",
    "    y_1[:, 1],\n",
    "    c=\"cornflowerblue\",\n",
    "    s=s,\n",
    "    edgecolor=\"black\",\n",
    "    label=\"max_depth=2\",\n",
    ")\n",
    "plt.scatter(y_2[:, 0], y_2[:, 1], c=\"red\", s=s, edgecolor=\"black\", label=\"max_depth=5\")\n",
    "plt.scatter(y_3[:, 0], y_3[:, 1], c=\"orange\", s=s, edgecolor=\"black\", label=\"max_depth=8\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-6, 6])\n",
    "plt.xlabel(\"target 1\")\n",
    "plt.ylabel(\"target 2\")\n",
    "plt.title(\"Multi-output Decision Tree Regression\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0x7BDAay_yN"
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmXP9mF6_CIn"
   },
   "source": [
    "A **random forest** is an ensemble method that combines multiple CART for classification and regression tasks. Each CART model is fitted using a bootstrapped sample, just like bagging, but at each split node, only a subset of $m$ predictors of all $p$ features are chosen as candidates for creating the split, typically $m \\approx \\sqrt{p}$ for classification and $m \\approx p/3$ for regression (Section 15.3 of *Elements of Statistical Learning*). Random Forest selects random subsets of features to *decorrelate* trees.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will also look at the spam email classification problem using the spam dataset.\n",
    "\n",
    "RandomForestClassifier in sklearn will be used to implement random forest. Official document could be found here\n",
    "\n",
    "Note that we could adjust the number of predictors used in random forest by changing the max_features argument.\n",
    "\n",
    "    max_features: {“sqrt”, “log2”, None}, int or float, default=”sqrt”\n",
    "\n",
    "The usgae of other arguments such as criterion, ccp_alpha and class_weight are the same as we introduced in DecisionTreeClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq_G-UthDkXc"
   },
   "source": [
    "We could fit the bagging model with ``RandomForestClassifier``. Instead of writing a for-loop for bootstrap, we can set the number of predictors used in random forest as the number of all predictors in our training set ($m = p$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7078,
     "status": "ok",
     "timestamp": 1740762320138,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "w4XLLuPnDnBD",
    "outputId": "41deac44-4543-4a45-9bee-19da0997feee"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0, max_features=X_spam_train.shape[1])\n",
    "clf.fit(X_spam_train, y_spam_train)\n",
    "accuracy = clf.score(X_spam_test, y_spam_test)\n",
    "test_error_bagging = 1 - accuracy\n",
    "test_error_bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsQJP2MVDqsL"
   },
   "source": [
    "Now we fit random forest, notice that by default, ``max_features``= \"sqrt\", which means we choose number of features used in random forest $m$ based on the square root of the number of all predictors in our training set $\\sqrt{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1338,
     "status": "ok",
     "timestamp": 1740762321477,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "N10-Yo6uDr9T",
    "outputId": "17d1c686-cfcb-419d-90de-81b56419c256"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_spam_train, y_spam_train)\n",
    "accuracy = rf.score(X_spam_test, y_spam_test)\n",
    "test_error_rf = 1 - accuracy\n",
    "test_error_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctBMR6XPDu6U"
   },
   "source": [
    "## Feature importance based on mean decrease in impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j2mvHEd-0W3"
   },
   "source": [
    "Feature importances are provided by the fitted attribute ``feature_importances_`` and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree. Note that Impurity-based feature importances can be misleading for high cardinality features (many unique values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5ac3_wtDzFQ"
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=rf.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1740762322665,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "pDHQm_2AD0o8",
    "outputId": "4a7f2850-98ab-47b1-fc6a-75b0dd7ee2d5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "forest_importances.sort_values(ascending=True).plot.barh()\n",
    "plt.ylabel(\"Mean decrease in impurity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rlft-h_D6EF"
   },
   "source": [
    "## Feature importance based on feature permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFbEV-cd-8BN"
   },
   "source": [
    "Permutation feature importance overcomes limitations of the impurity-based feature importance: they do not have a bias toward high-cardinality features and can be computed on a left-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUKt0eJbD_C7"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtn2_MKlEAFD"
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    rf, X_spam_test, y_spam_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1740762346246,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "lqtg1y4QEBLL",
    "outputId": "05fce12b-7b2e-425a-f102-ea551afef2be"
   },
   "outputs": [],
   "source": [
    "forest_importances = pd.Series(result.importances_mean, index=rf.feature_names_in_)\n",
    "plt.figure(figsize=(10, 12))\n",
    "forest_importances.sort_values(ascending=True).plot.barh()\n",
    "plt.ylabel(\"Mean decrease in accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLYtsw6EEMz"
   },
   "source": [
    "We observe some negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jxu6FlkrVgZx"
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XamM29vB1GxT"
   },
   "source": [
    "A single learned tree may be a bit too restrictive to fully capture patterns in the data. One way to ensure multiple trees get used is with boosting. By using CART as a base learner, gradient boosting trains new iterations of a CART based on the errors of the previous learned models.\n",
    "\n",
    "$$f_m(x)=f_{m-1}(x)+\\left(\\underset{h_m \\epsilon H}{\\operatorname{argmin}}\\left[\\sum_{i=1}^N L\\left(y_i, f_{m-1}\\left(x_i\\right)+h_m\\left(x_i\\right)\\right)\\right]\\right)(x)$$\n",
    "\n",
    "The $m^{\\textrm{th}}$ model is an amalgamation of weak learners. Each $h_m$ is a weak learner trained to minimize the remaining error after $f_{m-1}$ is learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16708,
     "status": "ok",
     "timestamp": 1740762362955,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "euSg_od38T-2",
    "outputId": "b2101e63-22f7-4194-9be0-8e2bbdf1b7e8"
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=1000, random_state=0, max_features=X_spam_train.shape[1]\n",
    ")\n",
    "clf.fit(X_spam_train, y_spam_train)\n",
    "accuracy = clf.score(X_spam_test, y_spam_test)\n",
    "test_error_boosting = 1 - accuracy\n",
    "test_error_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4297,
     "status": "ok",
     "timestamp": 1740762367250,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "fA3o8XFf_ziA",
    "outputId": "d12fd4cd-49a8-4b2f-9d34-6a04b18b5866"
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=1000, random_state=0, max_features=\"sqrt\")\n",
    "clf.fit(X_spam_train, y_spam_train)\n",
    "accuracy = clf.score(X_spam_test, y_spam_test)\n",
    "test_error_boosting = 1 - accuracy\n",
    "test_error_boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Motivation (free response)\n",
    "\n",
    "In at most **two** sentences, explain whether you expect decision tree techniques to outperform our previously covered regression methods **and why.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Decision trees may outperform linear regression methods when the underlying relationships between features and the target are nonlinear or involve complex interactions. However, if the relationship is approximately linear, traditional regression methods may perform equally well or better due to their lower variance.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Variable Setup and Selection\n",
    "\n",
    "For this problem, we will try to predict individuals' high-density lipoprotein (HDL) cholesterol levels. Please do the following:\n",
    "\n",
    "1. Use the following features for predictive purposes from our NHANES dataset: Gender, Age, Weight, Height, BMI, WaistSize, HouseholdSize, and Ethnicity. You may need to refer to the documentation to figure out their variable names:\n",
    "   - [HDL_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/HDL_L.htm)\n",
    "   - [DEMO_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.htm)\n",
    "   - [BMX_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/BMX_L.htm)\n",
    "\n",
    "2. Rename the variable names to be English-readable but still in Python variable style (e.g., `RIAGENDR` becomes `Gender`).\n",
    "\n",
    "3. Drop all rows with missing values.\n",
    "\n",
    "Store your final cleaned DataFrame in a variable named `my_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "bmx_df = pd.read_sas(\"data/NHANES/BMX_L.xpt\")\n",
    "demo_df = pd.read_sas(\"data/NHANES/DEMO_L.xpt\")\n",
    "hdl_df = pd.read_sas(\"data/NHANES/HDL_L.xpt\")\n",
    "\n",
    "# Inner join on SEQN\n",
    "df = pd.merge(hdl_df, bmx_df, on=\"SEQN\", how=\"inner\")\n",
    "df = pd.merge(df, demo_df, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "# Select and rename columns\n",
    "selected_columns = [\n",
    "    \"LBDHDD\",\n",
    "    \"RIAGENDR\",\n",
    "    \"RIDAGEYR\",\n",
    "    \"BMXWT\",\n",
    "    \"BMXHT\",\n",
    "    \"DMDHHSIZ\",\n",
    "    \"BMXBMI\",\n",
    "    \"BMXWAIST\",\n",
    "    \"RIDRETH1\",\n",
    "]\n",
    "filtered_data = df[selected_columns].copy()\n",
    "my_df = filtered_data.rename(\n",
    "    columns={\n",
    "        \"LBDHDD\": \"HDL\",\n",
    "        \"RIAGENDR\": \"Gender\",\n",
    "        \"RIDAGEYR\": \"Age\",\n",
    "        \"BMXWT\": \"Weight\",\n",
    "        \"BMXHT\": \"Height\",\n",
    "        \"BMXBMI\": \"BMI\",\n",
    "        \"BMXWAIST\": \"WaistSize\",\n",
    "        \"DMDHHSIZ\": \"HouseholdSize\",\n",
    "        \"RIDRETH1\": \"Ethnicity\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Drop rows with missing values\n",
    "my_df = my_df.dropna()\n",
    "# END SOLUTION\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"my_df\" in dir(), \"my_df should be defined\"\n",
    "assert len(my_df.columns) == 9, f\"Expected 9 columns, got {len(my_df.columns)}\"\n",
    "assert \"HDL\" in my_df.columns, \"HDL column should be present\"\n",
    "assert \"Gender\" in my_df.columns, \"Gender column should be present\"\n",
    "assert my_df.isna().sum().sum() == 0, \"There should be no missing values\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"Age\" in my_df.columns, \"Age column should be present\"\n",
    "assert \"Weight\" in my_df.columns, \"Weight column should be present\"\n",
    "assert \"Height\" in my_df.columns, \"Height column should be present\"\n",
    "assert \"BMI\" in my_df.columns, \"BMI column should be present\"\n",
    "assert \"WaistSize\" in my_df.columns, \"WaistSize column should be present\"\n",
    "assert \"HouseholdSize\" in my_df.columns, \"HouseholdSize column should be present\"\n",
    "assert \"Ethnicity\" in my_df.columns, \"Ethnicity column should be present\"\n",
    "assert len(my_df) > 0, \"DataFrame should not be empty\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Train, Val, Test Split\n",
    "\n",
    "Split your data into train, validation, and test sets with a 60%/20%/20% breakdown of observations, respectively.\n",
    "\n",
    "**Use `random_state=42` for this and all subsequent problems.**\n",
    "\n",
    "Store your splits in variables named `X_train`, `X_validation`, `X_test`, `y_train`, `y_validation`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "X = my_df.drop(columns=[\"HDL\"])\n",
    "y = my_df[\"HDL\"]\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Second split: 75% of 80% = 60% train, 25% of 80% = 20% validation\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "total_samples = len(my_df)\n",
    "train_ratio = len(X_train) / total_samples\n",
    "val_ratio = len(X_validation) / total_samples\n",
    "test_ratio = len(X_test) / total_samples\n",
    "\n",
    "assert 0.58 < train_ratio < 0.62, f\"Train ratio should be ~60%, got {train_ratio:.2%}\"\n",
    "assert 0.18 < val_ratio < 0.22, f\"Validation ratio should be ~20%, got {val_ratio:.2%}\"\n",
    "assert 0.18 < test_ratio < 0.22, f\"Test ratio should be ~20%, got {test_ratio:.2%}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    len(X_train) + len(X_validation) + len(X_test) == total_samples\n",
    "), \"Splits should cover all data\"\n",
    "assert len(y_train) == len(X_train), \"X_train and y_train should have same length\"\n",
    "assert len(y_validation) == len(\n",
    "    X_validation\n",
    "), \"X_validation and y_validation should have same length\"\n",
    "assert len(y_test) == len(X_test), \"X_test and y_test should have same length\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Regression Models\n",
    "\n",
    "Train an instance of CART (`DecisionTreeRegressor`), Random Forest (`RandomForestRegressor`), and Gradient Boosting (`GradientBoostingRegressor`) to predict HDL levels. Evaluate each model's performance on the train, validation, and test sets using mean squared error (MSE).\n",
    "\n",
    "Store your results in variables with the format `[split]_mse_[model]`. For example:\n",
    "- `train_mse_cart`, `validation_mse_cart`, `test_mse_cart`\n",
    "- `train_mse_rf`, `validation_mse_rf`, `test_mse_rf`\n",
    "- `train_mse_boosting`, `validation_mse_boosting`, `test_mse_boosting`\n",
    "\n",
    "Use `random_state=42` when initializing your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# CART Model\n",
    "nhanes_cart = DecisionTreeRegressor(random_state=42)\n",
    "nhanes_cart.fit(X_train, y_train)\n",
    "\n",
    "train_mse_cart = mean_squared_error(y_train, nhanes_cart.predict(X_train))\n",
    "validation_mse_cart = mean_squared_error(y_validation, nhanes_cart.predict(X_validation))\n",
    "test_mse_cart = mean_squared_error(y_test, nhanes_cart.predict(X_test))\n",
    "\n",
    "# Random Forest Model\n",
    "nhanes_rf = RandomForestRegressor(random_state=42)\n",
    "nhanes_rf.fit(X_train, y_train)\n",
    "\n",
    "train_mse_rf = mean_squared_error(y_train, nhanes_rf.predict(X_train))\n",
    "validation_mse_rf = mean_squared_error(y_validation, nhanes_rf.predict(X_validation))\n",
    "test_mse_rf = mean_squared_error(y_test, nhanes_rf.predict(X_test))\n",
    "\n",
    "# Gradient Boosting Model\n",
    "nhanes_boosting = GradientBoostingRegressor(random_state=42)\n",
    "nhanes_boosting.fit(X_train, y_train)\n",
    "\n",
    "train_mse_boosting = mean_squared_error(y_train, nhanes_boosting.predict(X_train))\n",
    "validation_mse_boosting = mean_squared_error(y_validation, nhanes_boosting.predict(X_validation))\n",
    "test_mse_boosting = mean_squared_error(y_test, nhanes_boosting.predict(X_test))\n",
    "# END SOLUTION\n",
    "\n",
    "print(\n",
    "    f\"CART - Train: {train_mse_cart:.2f}, Val: {validation_mse_cart:.2f}, Test: {test_mse_cart:.2f}\"\n",
    ")\n",
    "print(f\"RF - Train: {train_mse_rf:.2f}, Val: {validation_mse_rf:.2f}, Test: {test_mse_rf:.2f}\")\n",
    "print(\n",
    "    f\"Boosting - Train: {train_mse_boosting:.2f}, \"\n",
    "    f\"Val: {validation_mse_boosting:.2f}, Test: {test_mse_boosting:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert train_mse_cart >= 0, \"MSE should be non-negative\"\n",
    "assert validation_mse_cart >= 0, \"MSE should be non-negative\"\n",
    "assert test_mse_cart >= 0, \"MSE should be non-negative\"\n",
    "assert train_mse_rf >= 0, \"MSE should be non-negative\"\n",
    "assert test_mse_rf >= 0, \"MSE should be non-negative\"\n",
    "assert train_mse_boosting >= 0, \"MSE should be non-negative\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# CART should have near-zero training error (overfits)\n",
    "assert train_mse_cart < 1, \"CART should have very low training MSE\"\n",
    "# Ensemble methods should generally have lower test error than single tree\n",
    "assert (\n",
    "    test_mse_rf < test_mse_cart or test_mse_boosting < test_mse_cart\n",
    "), \"Ensemble methods should generally outperform single tree\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Regression Results Display\n",
    "\n",
    "Create a DataFrame that compares the performance of the decision tree methods. The DataFrame should have:\n",
    "- **Index**: Train Error, Validation Error, Test Error\n",
    "- **Columns**: CART, Random Forest, Boosting\n",
    "- **Values**: The corresponding MSE values\n",
    "\n",
    "Store the DataFrame in a variable named `results_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "results = np.array(\n",
    "    [\n",
    "        [train_mse_cart, train_mse_rf, train_mse_boosting],\n",
    "        [validation_mse_cart, validation_mse_rf, validation_mse_boosting],\n",
    "        [test_mse_cart, test_mse_rf, test_mse_boosting],\n",
    "    ]\n",
    ")\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    index=[\"Train Error\", \"Validation Error\", \"Test Error\"],\n",
    "    columns=[\"CART\", \"Random Forest\", \"Boosting\"],\n",
    ")\n",
    "# END SOLUTION\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(results_df, pd.DataFrame), \"results_df should be a DataFrame\"\n",
    "assert results_df.shape == (3, 3), f\"Expected shape (3, 3), got {results_df.shape}\"\n",
    "assert list(results_df.columns) == [\n",
    "    \"CART\",\n",
    "    \"Random Forest\",\n",
    "    \"Boosting\",\n",
    "], \"Column names are incorrect\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"Train Error\" in results_df.index, \"Index should contain 'Train Error'\"\n",
    "assert \"Validation Error\" in results_df.index, \"Index should contain 'Validation Error'\"\n",
    "assert \"Test Error\" in results_df.index, \"Index should contain 'Test Error'\"\n",
    "assert results_df.loc[\"Train Error\", \"CART\"] == train_mse_cart, \"CART train MSE should match\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Switching to Classification\n",
    "\n",
    "Previously, we treated HDL as a regression problem. However, it can be helpful to view HDL levels in categorical bins. According to the [Cleveland Clinic](https://my.clevelandclinic.org/health/articles/11920-cholesterol-numbers-what-do-they-mean), HDL levels can be categorized as:\n",
    "\n",
    "- **Heart-Healthy (0)**: HDL >= 60 mg/dL\n",
    "- **At-Risk (1)**: HDL 40-59 mg/dL for men, or HDL 50-59 mg/dL for women\n",
    "- **Dangerous (2)**: HDL < 40 mg/dL for men, or HDL < 50 mg/dL for women\n",
    "\n",
    "Create a new column called `Level` in `my_df` with these categorical labels (0, 1, or 2).\n",
    "\n",
    "Then create new train, validation, and test splits with the same 60%/20%/20% breakdown, but this time **stratify on the `Level` values**. Use `random_state=42`.\n",
    "\n",
    "**Note**: Gender is encoded as 1 for male and 2 for female in the NHANES dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def get_hdl_level(gender, hdl):\n",
    "    \"\"\"\n",
    "    Categorize HDL level based on gender and HDL value.\n",
    "    Gender: 1 = male, 2 = female\n",
    "    Returns: 0 = Heart-Healthy, 1 = At-Risk, 2 = Dangerous\n",
    "    \"\"\"\n",
    "    if hdl >= 60:\n",
    "        return 0  # Heart-Healthy\n",
    "    elif gender == 1:  # Male\n",
    "        if hdl >= 40:\n",
    "            return 1  # At-Risk\n",
    "        else:\n",
    "            return 2  # Dangerous\n",
    "    elif hdl >= 50:\n",
    "        return 1  # At-Risk\n",
    "    else:\n",
    "        return 2  # Dangerous\n",
    "\n",
    "\n",
    "my_df[\"Level\"] = my_df.apply(lambda row: get_hdl_level(row[\"Gender\"], row[\"HDL\"]), axis=1)\n",
    "\n",
    "# Create new splits for classification, stratified by Level\n",
    "X = my_df.drop(columns=[\"HDL\", \"Level\"])\n",
    "y = my_df[\"Level\"]\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"Level\" in my_df.columns, \"Level column should be added to my_df\"\n",
    "assert set(my_df[\"Level\"].unique()).issubset(\n",
    "    {0, 1, 2}\n",
    "), \"Level should only contain values 0, 1, or 2\"\n",
    "\n",
    "# Test that Level assignments follow the correct logic based on HDL and Gender\n",
    "# Heart-Healthy (0): HDL >= 60 for all genders\n",
    "heart_healthy = my_df[my_df[\"HDL\"] >= 60]\n",
    "assert (heart_healthy[\"Level\"] == 0).all(), \"All HDL >= 60 should be Heart-Healthy (0)\"\n",
    "\n",
    "# Dangerous (2) for males: HDL < 40\n",
    "dangerous_males = my_df[(my_df[\"Gender\"] == 1) & (my_df[\"HDL\"] < 40)]\n",
    "assert (dangerous_males[\"Level\"] == 2).all(), \"Males with HDL < 40 should be Dangerous (2)\"\n",
    "\n",
    "# Dangerous (2) for females: HDL < 50\n",
    "dangerous_females = my_df[(my_df[\"Gender\"] == 2) & (my_df[\"HDL\"] < 50)]\n",
    "assert (dangerous_females[\"Level\"] == 2).all(), \"Females with HDL < 50 should be Dangerous (2)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check that stratification was applied (class proportions should be similar across splits)\n",
    "train_prop = y_train.value_counts(normalize=True).sort_index()\n",
    "test_prop = y_test.value_counts(normalize=True).sort_index()\n",
    "for level in [0, 1, 2]:\n",
    "    if level in train_prop.index and level in test_prop.index:\n",
    "        assert (\n",
    "            abs(train_prop[level] - test_prop[level]) < 0.05\n",
    "        ), f\"Stratification issue for level {level}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Classification Models\n",
    "\n",
    "Similar to Problem 4, train CART (`DecisionTreeClassifier`), Random Forest (`RandomForestClassifier`), and Gradient Boosting (`GradientBoostingClassifier`) models for the classification task. Evaluate each model's performance using accuracy.\n",
    "\n",
    "Store your results in variables with the format `[split]_acc_[model]`. For example:\n",
    "- `train_acc_cart`, `validation_acc_cart`, `test_acc_cart`\n",
    "- `train_acc_rf`, `validation_acc_rf`, `test_acc_rf`\n",
    "- `train_acc_boosting`, `validation_acc_boosting`, `test_acc_boosting`\n",
    "\n",
    "Use `random_state=42` when initializing your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# CART Model\n",
    "nhanes_cart = DecisionTreeClassifier(random_state=42)\n",
    "nhanes_cart.fit(X_train, y_train)\n",
    "\n",
    "train_acc_cart = accuracy_score(y_train, nhanes_cart.predict(X_train))\n",
    "validation_acc_cart = accuracy_score(y_validation, nhanes_cart.predict(X_validation))\n",
    "test_acc_cart = accuracy_score(y_test, nhanes_cart.predict(X_test))\n",
    "\n",
    "# Random Forest Model\n",
    "nhanes_rf = RandomForestClassifier(random_state=42)\n",
    "nhanes_rf.fit(X_train, y_train)\n",
    "\n",
    "train_acc_rf = accuracy_score(y_train, nhanes_rf.predict(X_train))\n",
    "validation_acc_rf = accuracy_score(y_validation, nhanes_rf.predict(X_validation))\n",
    "test_acc_rf = accuracy_score(y_test, nhanes_rf.predict(X_test))\n",
    "\n",
    "# Gradient Boosting Model\n",
    "nhanes_boosting = GradientBoostingClassifier(random_state=42)\n",
    "nhanes_boosting.fit(X_train, y_train)\n",
    "\n",
    "train_acc_boosting = accuracy_score(y_train, nhanes_boosting.predict(X_train))\n",
    "validation_acc_boosting = accuracy_score(y_validation, nhanes_boosting.predict(X_validation))\n",
    "test_acc_boosting = accuracy_score(y_test, nhanes_boosting.predict(X_test))\n",
    "# END SOLUTION\n",
    "\n",
    "print(\n",
    "    f\"CART - Train: {train_acc_cart:.3f}, Val: {validation_acc_cart:.3f}, Test: {test_acc_cart:.3f}\"\n",
    ")\n",
    "print(f\"RF - Train: {train_acc_rf:.3f}, Val: {validation_acc_rf:.3f}, Test: {test_acc_rf:.3f}\")\n",
    "print(\n",
    "    f\"Boosting - Train: {train_acc_boosting:.3f}, \"\n",
    "    f\"Val: {validation_acc_boosting:.3f}, Test: {test_acc_boosting:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert 0 <= train_acc_cart <= 1, \"Accuracy should be between 0 and 1\"\n",
    "assert 0 <= validation_acc_cart <= 1, \"Accuracy should be between 0 and 1\"\n",
    "assert 0 <= test_acc_cart <= 1, \"Accuracy should be between 0 and 1\"\n",
    "assert 0 <= train_acc_rf <= 1, \"Accuracy should be between 0 and 1\"\n",
    "assert 0 <= test_acc_boosting <= 1, \"Accuracy should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# CART typically overfits, so training accuracy should be very high\n",
    "assert train_acc_cart > 0.9, \"CART should have high training accuracy\"\n",
    "# Ensemble methods should generally perform better on test set\n",
    "assert (\n",
    "    test_acc_rf >= test_acc_cart * 0.9 or test_acc_boosting >= test_acc_cart * 0.9\n",
    "), \"Ensemble methods should be competitive\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 8:** Classification Results Display\n",
    "\n",
    "Create a DataFrame that compares the classification performance of the decision tree methods. The DataFrame should have:\n",
    "- **Index**: Train Accuracy, Validation Accuracy, Test Accuracy\n",
    "- **Columns**: CART, Random Forest, Boosting\n",
    "- **Values**: The corresponding accuracy values\n",
    "\n",
    "Store the DataFrame in a variable named `classification_results_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "results = np.array(\n",
    "    [\n",
    "        [train_acc_cart, train_acc_rf, train_acc_boosting],\n",
    "        [validation_acc_cart, validation_acc_rf, validation_acc_boosting],\n",
    "        [test_acc_cart, test_acc_rf, test_acc_boosting],\n",
    "    ]\n",
    ")\n",
    "classification_results_df = pd.DataFrame(\n",
    "    results,\n",
    "    index=[\"Train Accuracy\", \"Validation Accuracy\", \"Test Accuracy\"],\n",
    "    columns=[\"CART\", \"Random Forest\", \"Boosting\"],\n",
    ")\n",
    "# END SOLUTION\n",
    "classification_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(\n",
    "    classification_results_df, pd.DataFrame\n",
    "), \"classification_results_df should be a DataFrame\"\n",
    "assert classification_results_df.shape == (\n",
    "    3,\n",
    "    3,\n",
    "), f\"Expected shape (3, 3), got {classification_results_df.shape}\"\n",
    "assert list(classification_results_df.columns) == [\n",
    "    \"CART\",\n",
    "    \"Random Forest\",\n",
    "    \"Boosting\",\n",
    "], \"Column names are incorrect\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"Train Accuracy\" in classification_results_df.index, \"Index should contain 'Train Accuracy'\"\n",
    "assert (\n",
    "    \"Validation Accuracy\" in classification_results_df.index\n",
    "), \"Index should contain 'Validation Accuracy'\"\n",
    "assert \"Test Accuracy\" in classification_results_df.index, \"Index should contain 'Test Accuracy'\"\n",
    "assert (\n",
    "    classification_results_df.loc[\"Train Accuracy\", \"CART\"] == train_acc_cart\n",
    "), \"CART train accuracy should match\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 9a:** Error Costs (free response)\n",
    "\n",
    "In 1-2 sentences, identify which misclassification error is the worst one to make. Your answer should follow this format: \"Predicting [THIS CLASS] when the ground truth is [THIS OTHER CLASS] is the worst because [REASON].\"\n",
    "\n",
    "Consider the health implications of each type of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Predicting \"Heart-Healthy\" (0) when the ground truth is \"Dangerous\" (2) is the worst error because it would lead to a false sense of security for patients who actually have critically low HDL levels and need immediate medical intervention to prevent heart disease.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 9b:** Error Analysis\n",
    "\n",
    "Now that you have identified which error is the most costly, analyze the confusion matrices for each model to determine which model best avoids that specific error.\n",
    "\n",
    "The code below generates confusion matrices for each model. After examining them, provide the ranking of models from **lowest to highest** misclassification for your identified worst error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices for analysis\n",
    "cm_cart = confusion_matrix(y_test, nhanes_cart.predict(X_test))\n",
    "cm_cart_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_cart, display_labels=nhanes_cart.classes_\n",
    ")\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, nhanes_rf.predict(X_test))\n",
    "cm_rf_display = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=nhanes_rf.classes_)\n",
    "\n",
    "cm_boosting = confusion_matrix(y_test, nhanes_boosting.predict(X_test))\n",
    "cm_boosting_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_boosting, display_labels=nhanes_boosting.classes_\n",
    ")\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "cm_cart_display.plot(ax=axes[0])\n",
    "axes[0].set_title(\"CART Confusion Matrix\")\n",
    "axes[0].set_xticklabels([\"Heart-Healthy\", \"At-Risk\", \"Dangerous\"])\n",
    "axes[0].set_yticklabels([\"Heart-Healthy\", \"At-Risk\", \"Dangerous\"])\n",
    "\n",
    "cm_rf_display.plot(ax=axes[1])\n",
    "axes[1].set_title(\"Random Forest Confusion Matrix\")\n",
    "axes[1].set_xticklabels([\"Heart-Healthy\", \"At-Risk\", \"Dangerous\"])\n",
    "axes[1].set_yticklabels([\"Heart-Healthy\", \"At-Risk\", \"Dangerous\"])\n",
    "\n",
    "cm_boosting_display.plot(ax=axes[2])\n",
    "axes[2].set_title(\"Boosting Confusion Matrix\")\n",
    "axes[2].set_xticklabels([\"Heart-Healthy\", \"At-Risk\", \"Dangerous\"])\n",
    "axes[2].set_yticklabels([\"Heart-Healthy\", \"At-Risk\", \"Dangerous\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Based on the confusion matrices, the ranking of models from lowest to highest misclassification of predicting \"Heart-Healthy\" when the truth is \"Dangerous\" (i.e., the (2,0) entry) is:\n",
    "\n",
    "1. Random Forest (lowest misclassification)\n",
    "2. Boosting\n",
    "3. CART (highest misclassification)\n",
    "\n",
    "Note: The exact ranking may vary depending on the random state and data split.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 10:** Maximizing Performance\n",
    "\n",
    "Decision trees work well for data with nonlinear relationships, and their performance depends heavily on hyperparameter choices. Your task is to achieve a test classification accuracy of **57% or higher**.\n",
    "\n",
    "You may use any decision tree ensemble method covered in this lab. Hyperparameters you can tune include:\n",
    "- Learning rate\n",
    "- Number of estimators (trees)\n",
    "- Tree depth (`max_depth`)\n",
    "- Minimum samples in each leaf (`min_samples_leaf`)\n",
    "- Maximum features considered per tree (`max_features`)\n",
    "- Minimum impurity decrease (`min_impurity_decrease`)\n",
    "\n",
    "Store your final model in a variable named `final_model`.\n",
    "\n",
    "**Important**: Do not include HDL as a feature, as it would make the problem trivial.\n",
    "\n",
    "**Hint**: See the [GradientBoostingClassifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) for available hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Tuned Gradient Boosting model for improved accuracy\n",
    "final_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "# END SOLUTION\n",
    "\n",
    "final_accuracy = final_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"final_model\" in dir(), \"final_model should be defined\"\n",
    "final_accuracy = final_model.score(X_test, y_test)\n",
    "assert final_accuracy > 0.57, f\"Test accuracy should be > 57%, got {final_accuracy:.2%}\"\n",
    "print(f\"Congratulations! Your model achieved {final_accuracy:.2%} accuracy.\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Ensure HDL is not used as a feature\n",
    "assert \"HDL\" not in X_test.columns, \"HDL should not be used as a feature\"\n",
    "# Check that the model is a valid classifier\n",
    "assert hasattr(final_model, \"predict\"), \"Model should have a predict method\"\n",
    "assert hasattr(final_model, \"score\"), \"Model should have a score method\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
