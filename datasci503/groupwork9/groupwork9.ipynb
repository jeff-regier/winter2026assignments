{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "BGHLGfU8-W4B"
   },
   "source": [
    "# DATASCI 503, Group Work 9: PCA and Clustering\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. During lab, feel free to flag down your GSI to ask questions at any point!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "tWDVdkiT-W4C"
   },
   "source": [
    "## Review of PCA and Clustering\n",
    "\n",
    "First, let's review how to use Python for PCA and clustering. We start with some theory about PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "EsO3drYz-W4D"
   },
   "source": [
    "### PCA\n",
    "\n",
    "\n",
    "- PCA replaces the original $p$ variables with $d<p$ linear combinations of the original variables that are a “good representation” of the data.\n",
    "\n",
    "\n",
    "- Mathematical formulation of PCA: The problem is to find the $k$ th new variable $Z_k$ which is a linear combination of the original variables $X_1, X_2, \\cdots, X_p$ (i.e. $\\left.Z_k=\\sum_{j=1}^p w_{k j} X_j\\right)$ such that it maximizes\n",
    "  $$\n",
    "  w_k^T \\Sigma w_k \\\\\n",
    "  \\text { subject to } w_k^T w_k=1, w_k^T w_{k^{\\prime}}=0, k^{\\prime}<k\n",
    "  $$\n",
    "\n",
    "  The solution to the above problem is given by the eigendecomposition of $\\Sigma$: \\\\\n",
    "  $\\Sigma=W \\Lambda W^T$, where $W$ is a  $p \\times p$ matrix of (column) eigenvectors and $\\Lambda$ is a $p \\times p$ diagonal matrix of eigenvalues.\n",
    "\n",
    "\n",
    "- PCA in practice: Let $X_{n \\times p}$ now be the $n \\times p$ data matrix (centered). Each data point is represented by a row.\n",
    "    - Compute the sample covariance matrix $\\hat{\\Sigma}= \\frac{1}{n-1}X^{\\top} X$\n",
    "    - Vectors $w_k$ 's are the eigenvectors of $\\hat{\\Sigma}$ and are called **PC directions**. The coordinates $w_{k j}$ are called **(factor) loadings**.\n",
    "    - Vectors $z_k=X w_k(k=1, \\ldots, d)$ are called the **principal components** of $X$ and are projections of the data onto the PC directions. Components of $X w_k$ are also called scores.\n",
    "    - $\\operatorname{var}\\left(X w_k\\right)=\\lambda_k$, the eigenvalues of $\\hat{\\Sigma} ; \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\lambda_p \\geq 0$.\n",
    "    \n",
    "    \n",
    "- How to pick the number of PCs?\n",
    "  - For visualization, can only use 2 or 3\n",
    "  - Can choose $d$ to explain certain percent of variation: pick first $d$ so that $\\frac{\\sum_{k=1}^d \\lambda_k}{\\sum_{k=1}^p \\lambda_k} \\geq (1-\\alpha)$\n",
    "    for some pre-specified small alpha (e.g. 0.1)\n",
    "  - Scree plot: plot $\\lambda_k$ or $\\sqrt{\\lambda_k}$ against $k$ and look for an 'elbow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742574154809,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "-og1Ku3w-W4F"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "9f2v6Qly-W4G"
   },
   "source": [
    "Now we use the crab dataset for a PCA practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742574154812,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "ivW6e_LY-W4G"
   },
   "outputs": [],
   "source": [
    "crabs = pd.read_csv(\"crabs.csv\", index_col=0)\n",
    "crabs.index = crabs[\"index\"]\n",
    "crabs.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742574154817,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "5hxEWQjV-W4H",
    "outputId": "b40395cb-6967-41b9-e197-773727d39945"
   },
   "outputs": [],
   "source": [
    "crabs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "OHMNBQGg-W4I"
   },
   "source": [
    "``sp``: species - \"B\" or \"O\" for blue or orange.\n",
    "\n",
    "``sex``: male or female\n",
    "\n",
    "``index``: index 1:50 within each of the four groups.\n",
    "\n",
    "``FL``: frontal lobe size (mm).\n",
    "\n",
    "``RW``: rear width (mm).\n",
    "\n",
    "``CL``: carapace length (mm).\n",
    "\n",
    "``CW``: carapace width (mm).\n",
    "\n",
    "``BD``: body depth (mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742574154819,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "z2Z7w23p-W4J",
    "outputId": "a8abcd27-7401-4836-d524-bd5af5048e00"
   },
   "outputs": [],
   "source": [
    "crabs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1742574154857,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "X1WaRikE-W4J",
    "outputId": "ff4ba023-25de-4b67-fd71-eb1dedd16dd8"
   },
   "outputs": [],
   "source": [
    "crabs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742574154862,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "xqYRYhuM-W4K",
    "outputId": "292ea176-2798-44d9-d717-e6d1c81a83c6"
   },
   "outputs": [],
   "source": [
    "crabs.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "LsuTcyOf-W4K"
   },
   "source": [
    "Now, we use the 4 numerical (continuous) variables to perform PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742574154867,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "CaFqGO_8-W4K",
    "outputId": "c4f68e41-512c-4f79-84b3-6f812e3f8016"
   },
   "outputs": [],
   "source": [
    "X = crabs.iloc[:, 3:]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742574154869,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "nFHnWbam-W4L"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1742574154951,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "amggJRHe-W4L",
    "outputId": "abe4eac8-9ca1-455c-c644-1f090def0d56"
   },
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver=\"full\")\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1742574154991,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "TDEuyCZF-W4L",
    "outputId": "b5f6730b-86c8-4560-8eeb-d252a78c1908"
   },
   "outputs": [],
   "source": [
    "# By default, it is min(n_features, n_samples)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "0XveLrW3-W4M"
   },
   "source": [
    "We can get the amount of variance explained by each of the selected components by using the attribute ``explained_variance_``. Note that it corresponds to the eigenvalue $\\lambda_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742574154995,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "x2YsjaFd-W4M",
    "outputId": "4eda3f69-dfc2-4137-98e8-b39d242926da"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1742574155025,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "scckAcvP-W4M",
    "outputId": "98f3d3ce-dcc3-49c1-9853-49a250f27c5e"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_, \"-o\")\n",
    "plt.xticks(\n",
    "    range(1, pca.n_components_ + 1)\n",
    ")  # This line makes sure that the x-axis only shows the integer values\n",
    "plt.xlabel(\"k-th PC\")\n",
    "plt.ylabel(r\"Variance by each component ($\\lambda_k$) \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "NhQw6bE3-W4M"
   },
   "source": [
    "The percentage of variance explained by each of the selected components is calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1742574155070,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "o1U1uDD--W4N",
    "outputId": "c11a9f39-af54-4983-e099-82449c0d6806"
   },
   "outputs": [],
   "source": [
    "np.round(pca.explained_variance_ratio_, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "BiYHJkZj-W4N"
   },
   "source": [
    "The tranformed data (the scores) can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1742574155071,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "hryUU-s0-W4N",
    "outputId": "e260b317-f013-4e91-e640-906df535dd55"
   },
   "outputs": [],
   "source": [
    "Z = pca.transform(X)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "IbSfcCZY-W4N"
   },
   "source": [
    "Now we try to visualize the first two dimensions based on the PCA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1742574155216,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "XsiA-EBy-W4N",
    "outputId": "c970242a-62d8-4678-ff1c-17a187e74114"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=Z[:, 0], y=Z[:, 1], hue=crabs[\"sex\"])\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1742574155343,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "OuNAsLVN-W4O",
    "outputId": "a27991d0-6fd3-4c95-b2b2-1a278d161758"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=Z[:, 0], y=Z[:, 1], hue=crabs[\"sp\"])\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "p-WWnqmU-W4O"
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "WJWURg_p-W4O"
   },
   "source": [
    "A dendrogram is a diagram representing a tree. The figure factory called create_dendrogram performs hierarchical clustering on data and represents the resulting tree. Values on the tree depth axis correspond to distances between clusters.\n",
    "\n",
    "Dendrogram plots are commonly used in computational biology to show the clustering of genes or samples, sometimes in the margin of heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1742574155361,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "WIjDQnAL-W4O",
    "outputId": "d6ec0f46-483e-470a-9440-dd48d91ea9d1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "dendrogram_data = np.random.rand(15, 12)  # 15 samples, with 12 dimensions each\n",
    "fig = ff.create_dendrogram(dendrogram_data)\n",
    "fig.update_layout(width=800, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "LjWSq21e-W4O"
   },
   "source": [
    "We can also set up a height or threshold manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1742574155404,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "ECUJO4jG-W4O",
    "outputId": "7005af0b-622b-4899-f96a-0d97175d92c4"
   },
   "outputs": [],
   "source": [
    "dendrogram_data = np.random.rand(15, 10)  # 15 samples, with 10 dimensions each\n",
    "fig = ff.create_dendrogram(dendrogram_data, color_threshold=1.5)\n",
    "fig.update_layout(width=800, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "a90L3g5L-W4P"
   },
   "source": [
    "Now we practice another clustering method: K-means clustering. We work on a synthetic baby data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1742574155568,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "tDLNeMHN-W4P",
    "outputId": "a3b8ca62-e8d4-43c5-80b7-701b22ae01ac"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [4, 5, 10, 4, 3, 11, 14, 6, 10, 12]\n",
    "y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "GJcL3jy5-W4P"
   },
   "source": [
    "\"Inertia,\" also known as within-cluster sum of squares (WSS), measures squared distances between points and the centroid of their cluster.  It is equal to one-half the within-cluster variation (i.e., sum of squared distance between all pairs of points in the same cluster, divided by size of cluster, summed over all clusters).\n",
    "\n",
    "Now we utilize the elbow method to visualize the intertia for different values of K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1742574155724,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "PZWzX9ce-W4P",
    "outputId": "dc48ab38-c71c-400f-8069-7aa2272c85d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = list(zip(x, y))\n",
    "inertias = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertias, marker=\"o\")\n",
    "plt.title(\"Elbow method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "hw4Kcu-1-W4Q"
   },
   "source": [
    "The elbow method shows that 2 is a good value for K, so we retrain and visualize the result using K=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1742574155857,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "Lj81IK2v-W4Q",
    "outputId": "b37d4392-b940-424e-cf38-754a468c5b95"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(data)\n",
    "\n",
    "plt.scatter(x, y, c=kmeans.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Clustering Evaluation Metrics: TSS, WSS, and BSS\n",
    "\n",
    "When evaluating clustering quality, we use several metrics based on sum of squares:\n",
    "\n",
    "**Notation:**\n",
    "- Let $x_j\\in\\mathbb{R}^d$, for $j=1,\\ldots,n$, be data points.\n",
    "- Let $C_1,\\ldots,C_K$ be a partition of the indices $1,\\ldots,n$ into disjoint clusters.\n",
    "- For cluster $C_k$ with size $n_k=|C_k|$, define the cluster mean: $\\bar{x}_{(k)} = \\frac{1}{n_k}\\sum_{j\\in C_k} x_j$\n",
    "- Define the grand mean: $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^n x_j$\n",
    "\n",
    "**Metrics:**\n",
    "\n",
    "- **Total Sum of Squares (TSS)**: measures spread of points from the grand mean\n",
    "$$\\mathrm{TSS} = \\sum_{j=1}^n \\|x_j - \\bar{x}\\|^2$$\n",
    "\n",
    "- **Within-Cluster Sum of Squares (WSS)**: measures spread of points around their respective cluster means\n",
    "$$\\mathrm{WSS} = \\sum_{k=1}^K \\sum_{j\\in C_k} \\|x_j - \\bar{x}_{(k)}\\|^2$$\n",
    "\n",
    "- **Between-Cluster Sum of Squares (BSS)**: measures spread of cluster means around the grand mean\n",
    "$$\\mathrm{BSS} = \\sum_{k=1}^K n_k \\|\\bar{x}_{(k)} - \\bar{x}\\|^2$$\n",
    "\n",
    "These satisfy the identity: $\\mathrm{TSS} = \\mathrm{WSS} + \\mathrm{BSS}$\n",
    "\n",
    "**Key insight:** TSS is fixed for a dataset, while WSS and BSS depend on cluster assignment. Good clustering has low WSS (tight clusters) and high BSS (well-separated centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing different clustering scenarios\n",
    "np.random.seed(42)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Top-Left: High BSS, Low WSS (OPTIMAL)\n",
    "ax = axes[0, 0]\n",
    "c1 = np.random.randn(50, 2) * 0.3 + [1, 1]\n",
    "c2 = np.random.randn(50, 2) * 0.3 + [4, 1]\n",
    "c3 = np.random.randn(50, 2) * 0.3 + [2.5, 4]\n",
    "ax.scatter(c1[:, 0], c1[:, 1], c=\"red\", s=30, alpha=0.6)\n",
    "ax.scatter(c2[:, 0], c2[:, 1], c=\"blue\", s=30, alpha=0.6)\n",
    "ax.scatter(c3[:, 0], c3[:, 1], c=\"green\", s=30, alpha=0.6)\n",
    "ax.set_title(\"High BSS, Low WSS\\n(OPTIMAL)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-1, 6)\n",
    "\n",
    "# Top-Right: High BSS, High WSS (SEPARATED BUT LOOSE)\n",
    "ax = axes[0, 1]\n",
    "c1 = np.random.randn(50, 2) * 0.8 + [1, 1]\n",
    "c2 = np.random.randn(50, 2) * 0.8 + [4, 1]\n",
    "c3 = np.random.randn(50, 2) * 0.8 + [2.5, 4]\n",
    "ax.scatter(c1[:, 0], c1[:, 1], c=\"red\", s=30, alpha=0.6)\n",
    "ax.scatter(c2[:, 0], c2[:, 1], c=\"blue\", s=30, alpha=0.6)\n",
    "ax.scatter(c3[:, 0], c3[:, 1], c=\"green\", s=30, alpha=0.6)\n",
    "ax.set_title(\"High BSS, High WSS\\n(SEPARATED BUT LOOSE)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-1, 6)\n",
    "\n",
    "# Bottom-Left: Low BSS, Low WSS (TIGHT BUT OVERLAPPING)\n",
    "ax = axes[1, 0]\n",
    "c1 = np.random.randn(50, 2) * 0.3 + [2, 2]\n",
    "c2 = np.random.randn(50, 2) * 0.3 + [2.5, 2.5]\n",
    "c3 = np.random.randn(50, 2) * 0.3 + [3, 2]\n",
    "ax.scatter(c1[:, 0], c1[:, 1], c=\"red\", s=30, alpha=0.6)\n",
    "ax.scatter(c2[:, 0], c2[:, 1], c=\"blue\", s=30, alpha=0.6)\n",
    "ax.scatter(c3[:, 0], c3[:, 1], c=\"green\", s=30, alpha=0.6)\n",
    "ax.set_title(\"Low BSS, Low WSS\\n(TIGHT BUT OVERLAPPING)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-1, 6)\n",
    "\n",
    "# Bottom-Right: Low BSS, High WSS (POOR)\n",
    "ax = axes[1, 1]\n",
    "c1 = np.random.randn(50, 2) * 0.8 + [2, 2]\n",
    "c2 = np.random.randn(50, 2) * 0.8 + [2.5, 2.5]\n",
    "c3 = np.random.randn(50, 2) * 0.8 + [3, 2]\n",
    "ax.scatter(c1[:, 0], c1[:, 1], c=\"red\", s=30, alpha=0.6)\n",
    "ax.scatter(c2[:, 0], c2[:, 1], c=\"blue\", s=30, alpha=0.6)\n",
    "ax.scatter(c3[:, 0], c3[:, 1], c=\"green\", s=30, alpha=0.6)\n",
    "ax.set_title(\"Low BSS, High WSS\\n(POOR CLUSTERING)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-1, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### How K-Means Works\n",
    "\n",
    "K-Means is one of the most popular clustering algorithms. The goal is to partition $n$ data points into $K$ clusters, where each point belongs to the cluster with the nearest **mean** (center).\n",
    "\n",
    "**The Algorithm:**\n",
    "\n",
    "Given data points $x_1, \\ldots, x_n \\in \\mathbb{R}^d$ and desired number of clusters $K$:\n",
    "\n",
    "1. **Initialize**: Randomly select $K$ points as initial cluster centers $\\mu_1, \\ldots, \\mu_K$\n",
    "\n",
    "2. **Assignment**: Assign each point to the nearest center:\n",
    "$$C_k = \\{i : \\|x_i - \\mu_k\\| \\leq \\|x_i - \\mu_j\\| \\text{ for all } j\\}$$\n",
    "\n",
    "3. **Update**: Recalculate centers as the mean of assigned points:\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x_i$$\n",
    "\n",
    "4. **Repeat** Steps 2-3 until convergence (labels stop changing)\n",
    "\n",
    "**Connection to WSS:** The K-Means algorithm minimizes WSS! Each iteration reduces WSS until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1: Implement Clustering Metrics**\n",
    "\n",
    "Implement three functions to compute the clustering evaluation metrics:\n",
    "- `tss(X)`: Total Sum of Squares\n",
    "- `wss(X, labels)`: Within-Cluster Sum of Squares  \n",
    "- `bss(X, labels)`: Between-Cluster Sum of Squares\n",
    "\n",
    "where `X` is an $n \\times d$ NumPy array and `labels` is a 1-D array of length $n$ with integer cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate demo data for testing\n",
    "np.random.seed(0)\n",
    "cluster_a = np.random.randn(30, 2) + np.array([3, 0])\n",
    "cluster_b = np.random.randn(30, 2) + np.array([-2, 2])\n",
    "cluster_c = np.random.randn(30, 2) + np.array([0, -3])\n",
    "\n",
    "X_demo = np.vstack([cluster_a, cluster_b, cluster_c])\n",
    "labels_demo = np.array([0] * 30 + [1] * 30 + [2] * 30)\n",
    "\n",
    "\n",
    "def tss(data):\n",
    "    \"\"\"\n",
    "    Compute Total Sum of Squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data matrix of shape (n, d).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total sum of squared distances from the grand mean.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    grand_mean = data.mean(axis=0)\n",
    "    return np.sum((data - grand_mean) ** 2)\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def wss(data, labels):\n",
    "    \"\"\"\n",
    "    Compute Within-Cluster Sum of Squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data matrix of shape (n, d).\n",
    "    labels : np.ndarray\n",
    "        Cluster labels of shape (n,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Sum of squared distances from each point to its cluster mean.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    total = 0.0\n",
    "    for k in np.unique(labels):\n",
    "        cluster_points = data[labels == k]\n",
    "        cluster_mean = cluster_points.mean(axis=0)\n",
    "        total += np.sum((cluster_points - cluster_mean) ** 2)\n",
    "    return total\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def bss(data, labels):\n",
    "    \"\"\"\n",
    "    Compute Between-Cluster Sum of Squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data matrix of shape (n, d).\n",
    "    labels : np.ndarray\n",
    "        Cluster labels of shape (n,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Weighted sum of squared distances from cluster means to the grand mean.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    grand_mean = data.mean(axis=0)\n",
    "    total = 0.0\n",
    "    for k in np.unique(labels):\n",
    "        cluster_points = data[labels == k]\n",
    "        cluster_mean = cluster_points.mean(axis=0)\n",
    "        n_k = len(cluster_points)\n",
    "        total += n_k * np.sum((cluster_mean - grand_mean) ** 2)\n",
    "    return total\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "tss_val = tss(X_demo)\n",
    "wss_val = wss(X_demo, labels_demo)\n",
    "bss_val = bss(X_demo, labels_demo)\n",
    "\n",
    "print(f\"TSS: {tss_val:.4f}\")\n",
    "print(f\"WSS: {wss_val:.4f}\")\n",
    "print(f\"BSS: {bss_val:.4f}\")\n",
    "print(f\"WSS + BSS: {wss_val + bss_val:.4f}\")\n",
    "\n",
    "assert abs(wss_val - 179.1138) < 0.01, f\"WSS should be ~179.11, got {wss_val}\"\n",
    "assert abs(bss_val - 789.3281) < 0.01, f\"BSS should be ~789.33, got {bss_val}\"\n",
    "assert abs(wss_val + bss_val - tss_val) < 1e-7, \"TSS should equal WSS + BSS\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert tss_val > 0, \"TSS should be positive\"\n",
    "assert wss_val > 0, \"WSS should be positive\"\n",
    "assert bss_val > 0, \"BSS should be positive\"\n",
    "assert bss_val / tss_val > 0.8, \"BSS/TSS ratio should be high for well-separated clusters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Implement K-Means from Scratch\n",
    "\n",
    "Complete the K-Means implementation below. The initialization is provided. You need to implement:\n",
    "1. The **assignment step**: compute distances and assign each point to the nearest center\n",
    "2. The **update step**: recalculate centers as the mean of assigned points\n",
    "\n",
    "Track `labels_history` and `centers_history` so we can visualize how the algorithm progresses.\n",
    "\n",
    "**Hint:** Use [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) to compute distances efficiently. This function returns a distance matrix where entry `[i, j]` is the distance between `data[i]` and `centers[j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "def kmeans_custom(data, n_clusters, max_iter=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Custom K-Means implementation with history tracking.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data matrix of shape (n, d).\n",
    "    n_clusters : int\n",
    "        Number of clusters.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : np.ndarray\n",
    "        Final cluster assignments of shape (n,).\n",
    "    centers : np.ndarray\n",
    "        Final cluster centers of shape (n_clusters, d).\n",
    "    labels_history : list of np.ndarray\n",
    "        Labels after each iteration (for visualization).\n",
    "    centers_history : list of np.ndarray\n",
    "        Centers after each iteration (for visualization).\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n, d = data.shape\n",
    "\n",
    "    # Step 1: Initialize centers by randomly selecting n_clusters data points\n",
    "    indices = np.random.choice(n, n_clusters, replace=False)\n",
    "    centers = data[indices].copy()\n",
    "\n",
    "    labels_history = []\n",
    "    centers_history = []\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Step 2: Assignment - assign each point to nearest center\n",
    "        # BEGIN SOLUTION\n",
    "        dist_matrix = pairwise_distances(data, centers)\n",
    "        labels = np.argmin(dist_matrix, axis=1)\n",
    "        # END SOLUTION\n",
    "\n",
    "        # Save current state to history\n",
    "        labels_history.append(labels.copy())\n",
    "        centers_history.append(centers.copy())\n",
    "\n",
    "        # Check for convergence (if labels didn't change)\n",
    "        if iteration > 0 and np.array_equal(labels, labels_history[-2]):\n",
    "            break\n",
    "\n",
    "        # Step 3: Update - recalculate centers as mean of assigned points\n",
    "        new_centers = np.zeros((n_clusters, d))\n",
    "        # BEGIN SOLUTION\n",
    "        for k in range(n_clusters):\n",
    "            mask = labels == k\n",
    "            if np.any(mask):\n",
    "                new_centers[k] = data[mask].mean(axis=0)\n",
    "            else:\n",
    "                new_centers[k] = centers[k]  # Keep old center if cluster is empty\n",
    "        # END SOLUTION\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    return labels, centers, labels_history, centers_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Generate test data\n",
    "from sklearn import datasets\n",
    "\n",
    "X_test, y_test = datasets.make_blobs(n_samples=150, centers=3, random_state=42, cluster_std=2.0)\n",
    "\n",
    "labels_result, centers_result, labels_hist, centers_hist = kmeans_custom(\n",
    "    X_test, n_clusters=3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Number of iterations: {len(labels_hist)}\")\n",
    "print(f\"Final labels shape: {labels_result.shape}\")\n",
    "print(f\"Final centers shape: {centers_result.shape}\")\n",
    "\n",
    "# Visualize final result\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=labels_result, s=30, cmap=\"viridis\", alpha=0.6)\n",
    "plt.scatter(\n",
    "    centers_result[:, 0],\n",
    "    centers_result[:, 1],\n",
    "    c=\"red\",\n",
    "    s=200,\n",
    "    marker=\"*\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=2,\n",
    "    label=\"Centers\",\n",
    ")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"K-Means Result\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "assert labels_result.shape == (150,), \"Labels should have shape (150,)\"\n",
    "assert centers_result.shape == (3, 2), \"Centers should have shape (3, 2)\"\n",
    "assert len(labels_hist) > 1, \"Should have recorded multiple iterations\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(np.unique(labels_result)) == 3, \"Should have 3 unique cluster labels\"\n",
    "assert len(labels_hist) == len(centers_hist), \"History lengths should match\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Visualize K-Means Algorithm Progress\n",
    "\n",
    "Complete the visualization function to see how K-Means evolves iteration by iteration. Create a subplot grid showing the cluster assignments at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kmeans_steps(data, labels_history, centers_history):\n",
    "    \"\"\"\n",
    "    Visualize K-Means algorithm progress across iterations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data matrix of shape (n, 2) - must be 2D for visualization.\n",
    "    labels_history : list of np.ndarray\n",
    "        Labels after each iteration.\n",
    "    centers_history : list of np.ndarray\n",
    "        Centers after each iteration.\n",
    "    \"\"\"\n",
    "    n_iterations = len(labels_history)\n",
    "\n",
    "    # Create subplot grid\n",
    "    n_cols = min(4, n_iterations)\n",
    "    n_rows = (n_iterations + n_cols - 1) // n_cols\n",
    "\n",
    "    _fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "    if n_iterations == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx in range(n_iterations):\n",
    "        ax = axes[idx]\n",
    "        labels = labels_history[idx]\n",
    "        centers = centers_history[idx]\n",
    "\n",
    "        # BEGIN SOLUTION\n",
    "        ax.scatter(data[:, 0], data[:, 1], c=labels, s=30, cmap=\"viridis\", alpha=0.6)\n",
    "        ax.scatter(\n",
    "            centers[:, 0],\n",
    "            centers[:, 1],\n",
    "            c=\"red\",\n",
    "            s=200,\n",
    "            marker=\"*\",\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=2,\n",
    "        )\n",
    "        ax.set_title(f\"Iteration {idx}\")\n",
    "        ax.set_xlabel(\"x1\")\n",
    "        ax.set_ylabel(\"x2\")\n",
    "        # END SOLUTION\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_iterations, len(axes)):\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Run K-Means with different random seeds to see different convergence behaviors\n",
    "for seed in [0, 42]:\n",
    "    print(f\"\\nRandom seed: {seed}\")\n",
    "    labels, centers, labels_hist, centers_hist = kmeans_custom(\n",
    "        X_test, n_clusters=3, random_state=seed\n",
    "    )\n",
    "    print(f\"Converged in {len(labels_hist)} iterations\")\n",
    "    visualize_kmeans_steps(X_test, labels_hist, centers_hist)\n",
    "\n",
    "assert len(labels_hist) >= 1, \"Should have at least 1 iteration recorded\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert callable(visualize_kmeans_steps), \"visualize_kmeans_steps should be a function\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** K-Means Convergence (free response)\n",
    "\n",
    "Based on the visualizations above, answer the following:\n",
    "\n",
    "1. Does K-Means always converge to the same result with different random initializations?\n",
    "2. How does the number of iterations to convergence vary with different seeds?\n",
    "3. What does this tell you about the importance of initialization in K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "1. No, K-Means does not always converge to the same result. Different random initializations can lead to different local minima of the WSS objective function.\n",
    "\n",
    "2. The number of iterations varies depending on how close the initial centers are to the final cluster structure. Some seeds may start with centers that are already near optimal, while others require more iterations.\n",
    "\n",
    "3. Initialization matters: poor initialization can lead to suboptimal clustering or slower convergence. In practice, k-means++ initialization or running K-Means multiple times with different seeds can help.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "id": "RNweiwIXFfLl"
   },
   "source": [
    "We have demonstrated how PCA works, but really, moving from 6 variables to 2 principal components is a trivial exercise. PCA is most effective when the data is truly of higher dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "bjAkomnoGonB"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Synthetic Data Generation\n",
    "\n",
    "Generate some data of rank 3. Create 250 observations of features $z \\sim \\text{MVN}([5, 3, 1], I)$. Then, create a low-rank data matrix $X$ such that $X = ZL + \\epsilon$, where $L$ is a 3x50 matrix of standard normal values and $\\epsilon \\sim N(0, 0.25)$.\n",
    "\n",
    "Store your results in the following variables:\n",
    "- `latent_samples`: the 250x3 matrix of multivariate normal samples (Z)\n",
    "- `projection_matrix`: the 3x50 matrix of standard normal values (L)\n",
    "- `data_matrix`: the final 250x50 low-rank data matrix with noise (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742574155875,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "X1rQFgMFGoOu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = [5, 3, 1]\n",
    "cov = np.identity(3)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Generate multivariate normal samples for the latent variables\n",
    "latent_samples = np.random.multivariate_normal(mean, cov, 250)\n",
    "\n",
    "# Generate random projection matrix\n",
    "projection_matrix = np.random.normal(0, 1, (3, 50))\n",
    "\n",
    "# Create low-rank data matrix with noise\n",
    "data_matrix = np.matmul(latent_samples, projection_matrix)\n",
    "data_matrix += np.random.normal(0, 0.25, data_matrix.shape)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert latent_samples.shape == (\n",
    "    250,\n",
    "    3,\n",
    "), f\"Expected latent_samples shape (250, 3), got {latent_samples.shape}\"\n",
    "assert projection_matrix.shape == (\n",
    "    3,\n",
    "    50,\n",
    "), f\"Expected projection_matrix shape (3, 50), got {projection_matrix.shape}\"\n",
    "assert data_matrix.shape == (\n",
    "    250,\n",
    "    50,\n",
    "), f\"Expected data_matrix shape (250, 50), got {data_matrix.shape}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert latent_samples.dtype == np.float64, \"latent_samples should be float64\"\n",
    "assert data_matrix.dtype == np.float64, \"data_matrix should be float64\"\n",
    "assert (\n",
    "    np.abs(latent_samples.mean(axis=0) - np.array([5, 3, 1])).max() < 1.0\n",
    "), \"Latent samples mean should be approximately [5, 3, 1]\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "UpRLI-0AGnuj"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Clustering on Raw Data\n",
    "\n",
    "Create a 2x3 subplot grid showing K-means clustering results with $K \\in \\{2, 3, 4, 5, 6, 7\\}$ on the first two features of the synthetic data matrix `data_matrix` from Problem 5. Each subplot should show a scatter plot of the data colored by cluster assignment.\n",
    "\n",
    "**Note:** Your plot must be organized as a subplot grid. Six separate plots arranged vertically will not receive credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 1166,
     "status": "ok",
     "timestamp": 1742574157044,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "pX_qc3laFe50",
    "outputId": "8764e80c-66ce-4ed0-c033-82e8612a8ba3"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "num_rows, num_cols = 2, 3\n",
    "\n",
    "for cluster_count in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=cluster_count, random_state=0)\n",
    "    kmeans.fit(data_matrix)\n",
    "\n",
    "    row_idx = (cluster_count - 2) // num_cols\n",
    "    col_idx = (cluster_count - 2) % num_cols\n",
    "    axes[row_idx, col_idx].scatter(\n",
    "        data_matrix[:, 0], data_matrix[:, 1], c=kmeans.labels_, cmap=\"viridis\", alpha=0.7\n",
    "    )\n",
    "    axes[row_idx, col_idx].set_title(f\"K={cluster_count}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert fig is not None, \"Figure should be created\"\n",
    "assert axes.shape == (2, 3), f\"Expected 2x3 subplot grid, got {axes.shape}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(fig.get_axes()) == 6, \"Figure should have exactly 6 subplots\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "xZznUrpY73td"
   },
   "source": [
    "The 3 clusters we know to exist from construction do not look very separable. Perhaps using just 2 random features in the input space (which we know is a subset of features from a low-rank matrix) was not ideal. Instead, let us see if projecting to a lower-dimensional space (creating latent features) yields more separable clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "id": "b4iIiWXl75ah"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Recovering Principal Components\n",
    "\n",
    "Run PCA with a reasonable number of components on `data_matrix`. Assess the total variance explained by the principal components. Based on this assessment, decide how many PCs to use for downstream clustering and record your answer in the markdown cell below.\n",
    "\n",
    "Create a plot of the cumulative sum of variance explained by PCA.\n",
    "\n",
    "Store your results in the following variables:\n",
    "- `pca_model`: the fitted PCA model\n",
    "- `explained_variance_cumsum`: a NumPy array of cumulative variance explained ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1742574157212,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "SORrN1Y18PAe",
    "outputId": "f9d79448-66bc-4ecc-e1fe-90063d1c4b61"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "pca_model = PCA(svd_solver=\"full\", n_components=10)\n",
    "pca_model.fit(data_matrix)\n",
    "explained_variance_cumsum = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1, len(explained_variance_cumsum) + 1), explained_variance_cumsum, marker=\"o\")\n",
    "plt.title(\"Cumulative Variance Explained by PCs\")\n",
    "plt.xlabel(\"Number of PCs\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert pca_model is not None, \"PCA model should be created\"\n",
    "assert len(explained_variance_cumsum) == 10, \"Should have 10 cumulative variance values\"\n",
    "assert explained_variance_cumsum[-1] <= 1.0, \"Cumulative variance should not exceed 1.0\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert explained_variance_cumsum[0] > 0.3, \"First PC should explain significant variance\"\n",
    "assert (\n",
    "    explained_variance_cumsum[2] > 0.9\n",
    "), \"First 3 PCs should explain most variance (data is rank 3)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "id": "EsVC-G4o8PVJ"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Number of PCs to use:** 3. The first 3 principal components explain nearly all the variance in the data, which makes sense because the data was constructed to be rank 3.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "id": "9iMSgCwz7I_O"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "id": "VO_ucTRlaaeR"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 8a:** Clustering on PCA-Transformed Data\n",
    "\n",
    "Now that we have established how many principal components to use, visualize how many clusters are separable in the reduced PCA space. Create a 2x3 subplot grid showing K-means clustering results with $K \\in \\{2, 3, 4, 5, 6, 7\\}$ on the PCA-transformed data.\n",
    "\n",
    "**Note:** Your plot must be organized as a subplot grid. Six separate plots arranged vertically will not receive credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1742574158246,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "IO4FFucjXtUf",
    "outputId": "f7c2c564-a52f-4d9e-81bf-932a99ae0c4a"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "fig_pca, axes_pca = plt.subplots(2, 3, figsize=(15, 10))\n",
    "num_rows, num_cols = 2, 3\n",
    "\n",
    "# Transform data using 3 principal components (done once, outside the loop)\n",
    "pca_transform = PCA(svd_solver=\"full\", n_components=3)\n",
    "pca_data = pca_transform.fit_transform(data_matrix)\n",
    "\n",
    "for cluster_count in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=cluster_count, random_state=0)\n",
    "    kmeans.fit(pca_data)\n",
    "\n",
    "    row_idx = (cluster_count - 2) // num_cols\n",
    "    col_idx = (cluster_count - 2) % num_cols\n",
    "    axes_pca[row_idx, col_idx].scatter(\n",
    "        pca_data[:, 0], pca_data[:, 1], c=kmeans.labels_, cmap=\"viridis\", alpha=0.7\n",
    "    )\n",
    "    axes_pca[row_idx, col_idx].set_title(f\"K={cluster_count}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert fig_pca is not None, \"Figure should be created\"\n",
    "assert axes_pca.shape == (2, 3), f\"Expected 2x3 subplot grid, got {axes_pca.shape}\"\n",
    "assert pca_data.shape[1] == 3, \"PCA should reduce to 3 components\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(fig_pca.get_axes()) == 6, \"Figure should have exactly 6 subplots\"\n",
    "assert pca_data.shape[0] == 250, \"Should have 250 observations after PCA\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "id": "_tSDeO3vbLp_"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 8b:** Interpreting PCA Space\n",
    "\n",
    "In 1-2 sentences, explain how to interpret the axes of the cluster plots using PCA data and why the clusters were visually separable in PCA space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "id": "TMstIeoE7eb8"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The axes represent the principal component scores (projections onto the eigenvectors of the covariance matrix), which capture the directions of maximum variance in the data. The clusters are more separable in PCA space because PCA concentrates the true underlying structure into fewer dimensions, removing noisy features that obscure the latent groupings.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "id": "H3rMGwni-9Gp"
   },
   "source": [
    "---\n",
    "\n",
    "## Working with Gene Expression Data\n",
    "\n",
    "Gene expression data is a good example of real data that typically requires dimensionality reduction. Some datasets have tens of thousands of genes. We will work with a dataset that has been reduced for us thanks to the [scanpy](https://scanpy.readthedocs.io/en/stable/generated/scanpy.datasets.pbmc68k_reduced.html) library.\n",
    "\n",
    "We have included the relevant code needed to load the dataset into the notebook. The data is an [annotated data matrix](https://anndata.readthedocs.io/en/stable/). It is a class that handles data organization, usually for single-cell data.\n",
    "\n",
    "Here is a quick summary of each anndata component (you will not need to use them all for this lab):\n",
    "\n",
    "- **X**: The primary two-dimensional data matrix (e.g., gene expression) where rows usually correspond to cells and columns to features (genes).\n",
    "- **obs**: A DataFrame storing per-observation (often per-cell) annotations such as cluster labels or metadata.\n",
    "- **var**: A DataFrame storing per-variable (often per-gene) annotations like gene symbols or feature quality metrics.\n",
    "- **uns**: A dictionary-like structure for unstructured annotations, typically holding things like color schemes, parameter settings, or additional metadata.\n",
    "- **obsm**: A dictionary of matrices aligned with observations (cells), commonly used for embeddings (e.g., PCA, UMAP coordinates).\n",
    "- **varm**: A dictionary of matrices aligned with variables (genes), often used for storing feature loadings in dimensionality reduction.\n",
    "- **layers**: A dictionary of additional data layers (e.g., raw counts, imputed data) that share the same dimensionality as X but may differ in values.\n",
    "- **raw**: An optional structure holding the unmodified or \"raw\" version of the data matrix (plus corresponding var), often used to preserve counts before normalization or filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {
    "id": "wu5uBAw7DSD0"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 9:** Working with Scanpy Datasets\n",
    "\n",
    "We have included the installation of scanpy and the dataset load-in.\n",
    "\n",
    "(a) Using the data matrix `X` and the PCA matrix `X_pca`, determine the number of observations and features in each dataset. Fill in the correct values for `num_cells_x`, `num_genes_x`, `num_cells_pca`, and `num_pcs`.\n",
    "\n",
    "(b) The `bulk_labels` object has the annotations of each cell's cell type. In the markdown cell below, state how many unique cell types there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1742574161657,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "x8F31ENT4XnN",
    "outputId": "74767929-c723-4238-870c-d38463beb071"
   },
   "outputs": [],
   "source": [
    "# JUST RUN, DO NOT EDIT\n",
    "import scanpy as sc\n",
    "\n",
    "# Load the pbmc68k dataset\n",
    "adata = sc.datasets.pbmc68k_reduced()\n",
    "\n",
    "# Inspect the AnnData object\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1742574161675,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "XxkwXisZ4YA-",
    "outputId": "02f6402e-2d0a-4e6a-d1b3-f01a0f35dfe1"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Extract shapes from the data\n",
    "gene_expression_pca = adata.obsm[\"X_pca\"]\n",
    "print(f\"X shape: {adata.X.shape}\")\n",
    "print(f\"X_pca shape: {gene_expression_pca.shape}\")\n",
    "print(f\"Number of unique cell types: {adata.obs['bulk_labels'].nunique()}\")\n",
    "\n",
    "# Fill in the values\n",
    "num_cells_x = adata.X.shape[0]\n",
    "num_genes_x = adata.X.shape[1]\n",
    "num_cells_pca = adata.obsm[\"X_pca\"].shape[0]\n",
    "num_pcs = adata.obsm[\"X_pca\"].shape[1]\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742574161678,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "3RZixL5s8viL"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert num_cells_x == 700, f\"Expected 700 cells in X, got {num_cells_x}\"\n",
    "assert num_genes_x == 765, f\"Expected 765 genes in X, got {num_genes_x}\"\n",
    "assert num_cells_pca == 700, f\"Expected 700 cells in X_pca, got {num_cells_pca}\"\n",
    "assert num_pcs == 50, f\"Expected 50 PCs in X_pca, got {num_pcs}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert adata.X.shape == (700, 765), \"X shape should be (700, 765)\"\n",
    "assert adata.obsm[\"X_pca\"].shape == (700, 50), \"X_pca shape should be (700, 50)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {
    "id": "uLRB1i8sJgEx"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Answer:** There are 10 unique cell types.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "OMkqDZVy5l2P"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {
    "id": "y3IhtbtRILX9"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 10a:** K-Means Clustering on Raw Gene Expression Data\n",
    "\n",
    "Perform K-means clustering on the entire gene expression dataset (`adata.X`). Use $K \\in \\{5, 10, 15\\}$. Store the cluster assignments in the per-observation annotations (`obs`) with keys `kmeans5_ALL`, `kmeans10_ALL`, and `kmeans15_ALL`. The $K=5$ example is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 5686,
     "status": "ok",
     "timestamp": 1742574167375,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "gezC1Oyt-KO_",
    "outputId": "b7da4023-3280-4b18-e884-2fd1c0691973"
   },
   "outputs": [],
   "source": [
    "gene_data = adata.X\n",
    "\n",
    "# kmeans with k=5 (provided example)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(gene_data)\n",
    "adata.obs[\"kmeans5_ALL\"] = kmeans.labels_.astype(str)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# kmeans with k=10\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(gene_data)\n",
    "adata.obs[\"kmeans10_ALL\"] = kmeans.labels_.astype(str)\n",
    "\n",
    "# kmeans with k=15\n",
    "kmeans = KMeans(n_clusters=15, random_state=0).fit(gene_data)\n",
    "adata.obs[\"kmeans15_ALL\"] = kmeans.labels_.astype(str)\n",
    "# END SOLUTION\n",
    "\n",
    "# Visualize clusters in UMAP space\n",
    "sc.pl.umap(adata, color=[\"kmeans5_ALL\", \"kmeans10_ALL\", \"kmeans15_ALL\", \"bulk_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"kmeans5_ALL\" in adata.obs.columns, \"kmeans5_ALL should be in obs\"\n",
    "assert \"kmeans10_ALL\" in adata.obs.columns, \"kmeans10_ALL should be in obs\"\n",
    "assert \"kmeans15_ALL\" in adata.obs.columns, \"kmeans15_ALL should be in obs\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(adata.obs[\"kmeans5_ALL\"].unique()) == 5, \"kmeans5_ALL should have 5 clusters\"\n",
    "assert len(adata.obs[\"kmeans10_ALL\"].unique()) == 10, \"kmeans10_ALL should have 10 clusters\"\n",
    "assert len(adata.obs[\"kmeans15_ALL\"].unique()) == 15, \"kmeans15_ALL should have 15 clusters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {
    "id": "X1xnN3QMLgBs"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 10b:** K-Means Clustering on PCA-Transformed Gene Data\n",
    "\n",
    "Now perform the same K-means clustering ($K \\in \\{5, 10, 15\\}$) but using the principal components as clustering input. Store results with keys `kmeans5_PC`, `kmeans10_PC`, and `kmeans15_PC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 2563,
     "status": "ok",
     "timestamp": 1742574169941,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "saJvkU_26xb4",
    "outputId": "45cc5b2e-9f01-4b16-87d6-018d792f2579"
   },
   "outputs": [],
   "source": [
    "# Extract PCA coordinates\n",
    "gene_pca_data = adata.obsm[\"X_pca\"]\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# kmeans with k=5\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(gene_pca_data)\n",
    "adata.obs[\"kmeans5_PC\"] = kmeans.labels_.astype(str)\n",
    "\n",
    "# kmeans with k=10\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(gene_pca_data)\n",
    "adata.obs[\"kmeans10_PC\"] = kmeans.labels_.astype(str)\n",
    "\n",
    "# kmeans with k=15\n",
    "kmeans = KMeans(n_clusters=15, random_state=0).fit(gene_pca_data)\n",
    "adata.obs[\"kmeans15_PC\"] = kmeans.labels_.astype(str)\n",
    "# END SOLUTION\n",
    "\n",
    "# Visualize clusters in UMAP space\n",
    "sc.pl.umap(adata, color=[\"kmeans5_PC\", \"kmeans10_PC\", \"kmeans15_PC\", \"bulk_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"kmeans5_PC\" in adata.obs.columns, \"kmeans5_PC should be in obs\"\n",
    "assert \"kmeans10_PC\" in adata.obs.columns, \"kmeans10_PC should be in obs\"\n",
    "assert \"kmeans15_PC\" in adata.obs.columns, \"kmeans15_PC should be in obs\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(adata.obs[\"kmeans5_PC\"].unique()) == 5, \"kmeans5_PC should have 5 clusters\"\n",
    "assert len(adata.obs[\"kmeans10_PC\"].unique()) == 10, \"kmeans10_PC should have 10 clusters\"\n",
    "assert len(adata.obs[\"kmeans15_PC\"].unique()) == 15, \"kmeans15_PC should have 15 clusters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "id": "AV2DX2go54U6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {
    "id": "sI--G_kWMoEe"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 11a:** Hierarchical Clustering on Raw Gene Expression Data\n",
    "\n",
    "Perform agglomerative (hierarchical) clustering on the entire gene expression dataset (`gene_data`). Use `n_clusters` $\\in \\{5, 10, 15\\}$ with Euclidean distance and Ward linkage. Store the cluster assignments in obs with keys `hclust_5_ALL`, `hclust_10_ALL`, and `hclust_15_ALL`. The $n=5$ example is provided for you.\n",
    "\n",
    "See [`AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 3919,
     "status": "ok",
     "timestamp": 1742574173858,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "h-SYq8MSKcFa",
    "outputId": "6c7dab83-b3c2-4d45-cb74-f82598ebc197"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Hierarchical clustering with n_clusters=5 (provided example)\n",
    "cluster = AgglomerativeClustering(n_clusters=5, metric=\"euclidean\", linkage=\"ward\")\n",
    "adata.obs[\"hclust_5_ALL\"] = cluster.fit_predict(gene_data).astype(str)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Hierarchical clustering with n_clusters=10\n",
    "cluster = AgglomerativeClustering(n_clusters=10, metric=\"euclidean\", linkage=\"ward\")\n",
    "adata.obs[\"hclust_10_ALL\"] = cluster.fit_predict(gene_data).astype(str)\n",
    "\n",
    "# Hierarchical clustering with n_clusters=15\n",
    "cluster = AgglomerativeClustering(n_clusters=15, metric=\"euclidean\", linkage=\"ward\")\n",
    "adata.obs[\"hclust_15_ALL\"] = cluster.fit_predict(gene_data).astype(str)\n",
    "# END SOLUTION\n",
    "\n",
    "# Visualize clusters in UMAP space\n",
    "sc.pl.umap(adata, color=[\"hclust_5_ALL\", \"hclust_10_ALL\", \"hclust_15_ALL\", \"bulk_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"hclust_5_ALL\" in adata.obs.columns, \"hclust_5_ALL should be in obs\"\n",
    "assert \"hclust_10_ALL\" in adata.obs.columns, \"hclust_10_ALL should be in obs\"\n",
    "assert \"hclust_15_ALL\" in adata.obs.columns, \"hclust_15_ALL should be in obs\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(adata.obs[\"hclust_5_ALL\"].unique()) == 5, \"hclust_5_ALL should have 5 clusters\"\n",
    "assert len(adata.obs[\"hclust_10_ALL\"].unique()) == 10, \"hclust_10_ALL should have 10 clusters\"\n",
    "assert len(adata.obs[\"hclust_15_ALL\"].unique()) == 15, \"hclust_15_ALL should have 15 clusters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {
    "id": "HkzwxeISM2jj"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 11b:** Hierarchical Clustering on PCA-Transformed Gene Data\n",
    "\n",
    "Now perform the same hierarchical clustering (`n_clusters` $\\in \\{5, 10, 15\\}$) but using the principal components as clustering input. Store results with keys `hclust_5_PC`, `hclust_10_PC`, and `hclust_15_PC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 4287,
     "status": "ok",
     "timestamp": 1742574178146,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "As7fsHNs-QeQ",
    "outputId": "1463baa2-7406-49ac-b0df-c94c2774f8f1"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Hierarchical clustering with n_clusters=5\n",
    "cluster = AgglomerativeClustering(n_clusters=5, metric=\"euclidean\", linkage=\"ward\")\n",
    "adata.obs[\"hclust_5_PC\"] = cluster.fit_predict(gene_pca_data).astype(str)\n",
    "\n",
    "# Hierarchical clustering with n_clusters=10\n",
    "cluster = AgglomerativeClustering(n_clusters=10, metric=\"euclidean\", linkage=\"ward\")\n",
    "adata.obs[\"hclust_10_PC\"] = cluster.fit_predict(gene_pca_data).astype(str)\n",
    "\n",
    "# Hierarchical clustering with n_clusters=15\n",
    "cluster = AgglomerativeClustering(n_clusters=15, metric=\"euclidean\", linkage=\"ward\")\n",
    "adata.obs[\"hclust_15_PC\"] = cluster.fit_predict(gene_pca_data).astype(str)\n",
    "# END SOLUTION\n",
    "\n",
    "# Visualize clusters in UMAP space\n",
    "sc.pl.umap(adata, color=[\"hclust_5_PC\", \"hclust_10_PC\", \"hclust_15_PC\", \"bulk_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"hclust_5_PC\" in adata.obs.columns, \"hclust_5_PC should be in obs\"\n",
    "assert \"hclust_10_PC\" in adata.obs.columns, \"hclust_10_PC should be in obs\"\n",
    "assert \"hclust_15_PC\" in adata.obs.columns, \"hclust_15_PC should be in obs\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(adata.obs[\"hclust_5_PC\"].unique()) == 5, \"hclust_5_PC should have 5 clusters\"\n",
    "assert len(adata.obs[\"hclust_10_PC\"].unique()) == 10, \"hclust_10_PC should have 10 clusters\"\n",
    "assert len(adata.obs[\"hclust_15_PC\"].unique()) == 15, \"hclust_15_PC should have 15 clusters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {
    "id": "stN4tKp6GJ2c"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 12:** Evaluating Cluster Assignments\n",
    "\n",
    "We have run all the clustering algorithms, but just looking at the cluster assignments is not a great way to evaluate them. One cluster evaluation metric is the [adjusted Rand index (ARI)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html). You do not need to fully understand the formula; just know that scores closer to 1 mean nearly identical clusters while values 0 or below represent random or discordant clusters.\n",
    "\n",
    "Use [`adjusted_rand_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) to evaluate all previous clusterings against the true `bulk_labels`. Determine which clustering had the highest ARI. In 1-2 sentences, state which clustering method performed best and explain whether this conclusion makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1742574178229,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "vC1p-AfhG1TE",
    "outputId": "7303b549-87ca-427f-fe79-96b9a3a4ff84"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "all_clusters = [\n",
    "    \"kmeans5_ALL\",\n",
    "    \"kmeans10_ALL\",\n",
    "    \"kmeans15_ALL\",\n",
    "    \"kmeans5_PC\",\n",
    "    \"kmeans10_PC\",\n",
    "    \"kmeans15_PC\",\n",
    "    \"hclust_5_ALL\",\n",
    "    \"hclust_10_ALL\",\n",
    "    \"hclust_15_ALL\",\n",
    "    \"hclust_5_PC\",\n",
    "    \"hclust_10_PC\",\n",
    "    \"hclust_15_PC\",\n",
    "]\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Find the clustering with the highest ARI\n",
    "max_ari = 0\n",
    "best_clustering = None\n",
    "\n",
    "for cluster_name in all_clusters:\n",
    "    ari = adjusted_rand_score(adata.obs[cluster_name], adata.obs[\"bulk_labels\"])\n",
    "    print(f\"{cluster_name}: ARI = {ari:.4f}\")\n",
    "    if ari > max_ari:\n",
    "        max_ari = ari\n",
    "        best_clustering = cluster_name\n",
    "\n",
    "print(f\"\\nThe best clustering was {best_clustering} with an ARI of {max_ari:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert best_clustering is not None, \"best_clustering should be identified\"\n",
    "assert max_ari > 0, \"max_ari should be positive\"\n",
    "assert best_clustering in all_clusters, \"best_clustering should be one of the cluster methods\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert max_ari > 0.3, \"Best ARI should be reasonably high (> 0.3)\"\n",
    "assert isinstance(best_clustering, str), \"best_clustering should be a string\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {
    "id": "zhALm05HOKn5"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The best clustering was `kmeans10_PC` with an ARI of approximately 0.59. This makes sense because (1) the true labels have 10 cell types, so K=10 is the correct number of clusters, and (2) using PCA-transformed data removes noise and concentrates signal in fewer dimensions, leading to better cluster separation.\n",
    "> END SOLUTION\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
