{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDQVi_mOlcl1"
   },
   "source": [
    "# DATASCI 503, Group Work 6: Splines and GAMs\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. During lab, feel free to flag down your GSI to ask questions at any point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC7bh2SYukgQ"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0--U9VVcupzE"
   },
   "source": [
    "For this lab we will need to install the `pygam` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5hib9iUZ0br"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygam import LinearGAM, LogisticGAM, f, s\n",
    "from pygam.datasets import default\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    PolynomialFeatures,\n",
    "    SplineTransformer,\n",
    "    StandardScaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtALPJMrqUIi"
   },
   "source": [
    "### `sklearn` [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A7k9nByqhAy"
   },
   "source": [
    "''A sequence of data transformers with an optional final predictor.  \n",
    "Pipeline allows you to sequentially apply a list of transformers to preprocess the data and, if desired, conclude the sequence with a final predictor for predictive modeling.  \n",
    "Intermediate steps of the pipeline must be transformers, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.  \n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1740159348692,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "tI2gaQANcdJq",
    "outputId": "ab401457-0e52-4cb6-be77-d94b564030ad"
   },
   "outputs": [],
   "source": [
    "# Re-defining the data\n",
    "np.random.seed(100)\n",
    "x = np.linspace(0, 10, 30)\n",
    "y = 2 * np.sin(0.5 * x) + np.cos(x) + np.random.randn(30)\n",
    "X = x.reshape(-1, 1)  # Reshaping x for the models\n",
    "\n",
    "# scatter plot of data\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1740159348977,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "pOxczkG_dbWa",
    "outputId": "409b756b-f7b4-4e9c-edac-ad56c1a6abad"
   },
   "outputs": [],
   "source": [
    "# Defining the pipeline with polynomial features\n",
    "poly_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=5)),\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fitting the model\n",
    "poly_pipeline.fit(X, y)\n",
    "# Predicting over the input range\n",
    "X_test = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "y_poly_pred = poly_pipeline.predict(X_test)\n",
    "\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X_test, y_poly_pred, label=\"Polynomial Fit\", color=\"red\")\n",
    "plt.title(\"Polynomial Feature Fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd77mYOlnvzx"
   },
   "source": [
    "Let's check what's inside a `Pipeline` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1740159349000,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "XEXFH5e2iIyK",
    "outputId": "ede96999-6ac7-4282-c7b5-858a2776a250"
   },
   "outputs": [],
   "source": [
    "poly_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1740159349351,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "flnLV3PlgoM7",
    "outputId": "df254c35-35f8-4c02-ff64-cd242ea31a74"
   },
   "outputs": [],
   "source": [
    "degrees = range(0, 12, 2)\n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "\n",
    "for degree in degrees:\n",
    "    poly_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "            (\"scaling\", StandardScaler()),\n",
    "            (\"linear_regression\", LinearRegression()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fitting the model\n",
    "    poly_pipeline.fit(X, y)\n",
    "    # Predicting over the input range\n",
    "    X_test = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "    y_poly_pred = poly_pipeline.predict(X_test)\n",
    "\n",
    "    # Plotting\n",
    "    plt.plot(X_test, y_poly_pred, label=f\"Polynomial Fit degree:{degree}\")\n",
    "\n",
    "plt.title(\"Polynomial Feature Fits for Different Degrees\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsEmE0M5C-5w"
   },
   "source": [
    "### Cubic [Splines](https://en.wikipedia.org/wiki/Spline_(mathematics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMSNrElTuGRH"
   },
   "source": [
    "Consider a simple setup where we have data $\\{(x_i,y_i)\\}_{i=1}^n$, and $x_i\\in \\mathbb{R}^1$. We want to model the non-linear relationship between $y$ and $x$. Cubic spline is a method that fits a piece-wise cubic polynomial on contiguous intervals of $x$. Suppose that the minimum and maximum of all $x_i$ are $\\xi_0,\\xi_{K+1}$, respectively. There are $K$ knots in between that serve as the endpoints of the $K+1$ intervals of $x$: $\\xi_1 < \\xi_2 < \\dots < \\xi_K$. On every interval $[\\xi_{k-1}, \\xi_{k}]$ we fit a cubic polynomial $s_k(x)$ for $y$. Additionally, on each of the knot, we constrain that:\n",
    "\n",
    "- Continuity: $s_k(\\xi_{k}) = s_{k+1}(\\xi_{k})$\n",
    "- First-order continuity: $s_k^{\\prime}(\\xi_{k}) = s_{k+1}^{\\prime}(\\xi_{k})$\n",
    "- Second-order continuity: $s_k^{\\prime\\prime}(\\xi_{k}) = s_{k+1}^{\\prime\\prime}(\\xi_{k})$\n",
    "\n",
    "On every of the $K+1$ intervals, we have a cubic polynomial with 4 parameters, but on every of the $K$ knots we have the 3 constraints above. In aggregate, we have $4(K+1)−3K=K+4$ free parameters for cubic spline.\n",
    "\n",
    "$$\n",
    "s(x) = \\beta_0 h_0(x) + \\beta_1 h_1(x) + \\dots + \\beta_{K+3} h_{K+3}(x)\n",
    "$$\n",
    "\n",
    "where $h_{j}(x) = x^{j}, j = 0, 1, 2, 3$, and $h_{3+l}(x) = (x-\\xi_l)_{+}^3, l=1,2,\\dots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuFDkz38atNn"
   },
   "source": [
    "Documentation: [Spline Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfK70K4OLNPw"
   },
   "source": [
    "**So how many knots?**\n",
    "\n",
    "In general, it turns out that the optimal number of knots is data dependent, and the rule-of-thumb of choosing knot location is to put more knots in the interval where function is varying a lot, and put fewer knots in the interval where the function is very smooth and can be approximated well by very few cubic polynomial. In practice, this requires you to do **exploratory data analysis**! A spline method with automatic knot selection called *smoothing spline* in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1740159349566,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "CTuiDfy3cS9q",
    "outputId": "74643841-4ce7-44f7-ff6d-b5c42e763d8e"
   },
   "outputs": [],
   "source": [
    "# Creating the pipeline with SplineTransformer and LinearRegression\n",
    "spline_pipeline = Pipeline(\n",
    "    [(\"spline\", SplineTransformer(n_knots=4, degree=3)), (\"linear_regression\", LinearRegression())]\n",
    ")\n",
    "\n",
    "# Fitting the pipeline to the data\n",
    "spline_pipeline.fit(X, y)\n",
    "\n",
    "# Generating a range of values for plotting the spline\n",
    "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_spline = spline_pipeline.predict(X_plot)\n",
    "\n",
    "# Plotting the original data and the spline fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, facecolor=\"gray\", label=\"Data\")\n",
    "plt.plot(X_plot, y_spline, label=\"Spline fit\", color=\"dodgerblue\")\n",
    "plt.title(\"Spline Fitting Example\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1740159349960,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "rL7OQpTrDgZX",
    "outputId": "e9eb4e12-ada4-4f09-a447-c6625ba939d7"
   },
   "outputs": [],
   "source": [
    "# Extracting the spline transformer from the pipeline\n",
    "spline_transformer = spline_pipeline.named_steps[\"spline\"]\n",
    "X_splines = spline_transformer.transform(X)\n",
    "# Generating a range of values for plotting the spline\n",
    "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_plot_splines = spline_transformer.transform(X_plot)\n",
    "# Extracting the knot positions\n",
    "knots = spline_transformer.bsplines_[0].t[3:7]\n",
    "\n",
    "# Plotting the original data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, facecolor=\"gray\", label=\"Data\")\n",
    "\n",
    "# Plotting the splines\n",
    "for i in range(X_splines.shape[1]):\n",
    "    plt.plot(X_plot, X_plot_splines[:, i], label=f\"Spline {i + 1}\", linestyle=\"--\")\n",
    "\n",
    "# Plotting the spline fit\n",
    "y_spline = spline_pipeline.predict(X_plot)\n",
    "plt.plot(X_plot, y_spline, label=\"Spline fit\", color=\"dodgerblue\", linewidth=2)\n",
    "\n",
    "\n",
    "# Adding markers at the knot positions\n",
    "plt.scatter(knots, np.zeros_like(knots), color=\"red\", marker=\"x\", s=100, label=\"Knots\", zorder=5)\n",
    "\n",
    "plt.title(\"Spline Features and Fitting Example\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYNxq1Tyrqv9"
   },
   "source": [
    "### Smoothing Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twXsfufmuMcZ"
   },
   "source": [
    "Smoothing splines are particularly useful in situations where you want to fit a curve through data points in a way that balances the fit's closeness to the data points against the smoothness of the fit. In other words, we want to fit a curve that:  \n",
    "\n",
    "- Minimizes Residual Squared Error,\n",
    "- Is Smooth\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n(y_i - g(x_i))^2 + \\lambda \\int (g^{(m)}(t))^2 dt\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a *tuning parameter* and $g^{(m)}$ is the $m$-th derivative\n",
    "\n",
    "The function $g(x)$ that minimizes this objective has special properties:\n",
    "\n",
    "- It is a piecewise polynomial of degree $2m-1$,\n",
    "- It has knots at $x_1, x_2, \\cdots, x_n$, and\n",
    "- In the region outside the knots at the extremes, it is a polynomial of degree $m-1$.\n",
    "\n",
    "\n",
    "**Think About:** How does $\\lambda$ control the bias-variance tradeoff of the smoothing spline?\n",
    "\n",
    "\n",
    "**Think About:** What are the limits of the integral defining the objective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYelWwprgUu"
   },
   "source": [
    "In order to fit a smoothing spline in Python, we use `LinearGAM()` function in the `pygam` library. We start with generating data as follows:\n",
    "\n",
    "\n",
    "For $ x \\in [0, 10]$:\n",
    "\n",
    "$$\n",
    "y = \\sin(0.5x) \\times \\sin(x) + 0.1\\epsilon\n",
    "$$\n",
    "where $\\epsilon\\sim N(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1740159350217,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "xbzqoZ7Nrdww",
    "outputId": "657e3fdc-1547-4453-fdbf-cb3159e06cad"
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "x = np.linspace(0, 10, 30)\n",
    "y = np.sin(0.5 * x) * np.sin(x) + 0.1 * np.random.randn(30)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjyoIfHw4mbH"
   },
   "source": [
    "In `pyGAM`, we can specify the spline terms by `s()`in the basis functional form. [Documentation](https://pygam.readthedocs.io/en/latest/api/api.html#spline-term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1740159350400,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "7QeYkiN5ubnP",
    "outputId": "26dd828a-129f-4d8b-8b34-accd133c6d52"
   },
   "outputs": [],
   "source": [
    "# Reshaping x to be a 2D array\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "gam = LinearGAM(s(0, lam=0.6)).fit(X, y)\n",
    "XX = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(XX, gam.predict(XX), label=rf\"$\\lambda$={0.6}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1740159350831,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "LTK29E2jywgu",
    "outputId": "bb00545f-02e5-431d-9c8a-794925678541"
   },
   "outputs": [],
   "source": [
    "# Lambda values to try\n",
    "lambda_values = [0, 0.1, 1, 10, 100, 100000]\n",
    "\n",
    "# Plotting the original data\n",
    "plt.scatter(X, y, facecolor=\"gray\", label=\"Data\")\n",
    "\n",
    "# Fitting a model for lambda=0 and checking coefficients\n",
    "gam_zero_lambda = LinearGAM(s(0, lam=0)).fit(X, y)\n",
    "\n",
    "# Predicting on original X values for lambda=0\n",
    "y_pred_zero_lambda = gam_zero_lambda.predict(X)\n",
    "plt.plot(X, np.sin(0.5 * x) * np.sin(x), label=\"Original Y\", color=\"black\", ls=\":\", lw=2)\n",
    "\n",
    "# Fitting a model and plotting for each non-zero lambda\n",
    "for lam in lambda_values:\n",
    "    gam = LinearGAM(s(0, lam=lam)).fit(X, y)\n",
    "    XX = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "    plt.plot(XX, gam.predict(XX), label=rf\"$\\lambda$={lam}\")\n",
    "\n",
    "plt.title(\"Smoothing Spline Fits for Different λ Values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDUkqK8tKpRM"
   },
   "source": [
    "**Think About:** How would you measure bias-variance for this family?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43Lx33_T37iK"
   },
   "source": [
    "### Generalized Additive Models ([GAMs](https://en.wikipedia.org/wiki/Generalized_additive_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xhgwt05WEZ3M"
   },
   "source": [
    "GAMs extend the multiple linear regression model\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\epsilon_i,\n",
    "$$\n",
    "to allow non-linear relationships between each feature and the response by replacing each linear compontent $\\beta_j x_{ij}$ with a smooth non-linear function $f_j(x_{ij})$.\n",
    "$$\n",
    "g(x_i) = \\beta_0 + f_1(x_{i1}) + f_2 (x_{i2}) + \\cdots + f_p (x_{ip}) + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "$g(x_i)$ is indeed a linear combination of the smooth functions $f_j(x_{ij})$, but it's important to note that the \"linearity\" here refers to the linearity in the parameters (coefficients of the spline basis functions), not necessarily linearity in the relationship between predictors and the response variable. The model is \"additive\" because it sums up the effects of each predictor, allowing for flexible, non-linear relationships to be captured through the smooth functions $f_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "executionInfo": {
     "elapsed": 7212,
     "status": "ok",
     "timestamp": 1740159358042,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "9erp5NZkeI8A",
    "outputId": "1adccc3c-1cf3-4b1e-a4c4-9f9572294d64"
   },
   "outputs": [],
   "source": [
    "# Dataset URL\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "\n",
    "# Column names for the dataset\n",
    "column_names = [\n",
    "    \"MPG\",\n",
    "    \"Cylinders\",\n",
    "    \"Displacement\",\n",
    "    \"Horsepower\",\n",
    "    \"Weight\",\n",
    "    \"Acceleration\",\n",
    "    \"Model Year\",\n",
    "    \"Origin\",\n",
    "    \"Car Name\",\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url, sep=r\"\\s+\", names=column_names, na_values=\"?\")\n",
    "\n",
    "# Dropping rows with missing values for simplicity\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop 'Car Name' column as it's not needed for this example\n",
    "df = df.drop(columns=[\"Car Name\"])\n",
    "\n",
    "# Convert \"Origin\" into a categorical variable\n",
    "df[\"Origin\"] = df[\"Origin\"].astype(\"category\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnBmnwB5eRma"
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"MPG\", axis=1).values\n",
    "y = df[\"MPG\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9O6bEojgL_S"
   },
   "source": [
    "In `pyGAM`, we can specify the basis functional form using terms:\n",
    "- `l()`: linear terms\n",
    "- `s()`: spline terms\n",
    "- `f()`: factor terms\n",
    "- `te()`: tensor products\n",
    "- `intercept`: included by default\n",
    "\n",
    "In the example below, we fit a spline term on feature 0 and feature 1, and a factor term on feature -1 (last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1740159358130,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "iJcXdkdFfcKh",
    "outputId": "f4fe9ee1-697e-4347-ea7a-16ed6f013d45"
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "gam = LinearGAM(s(feature=0, n_splines=20) + s(feature=1, n_splines=20) + f(feature=-1))\n",
    "gam.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1740159358170,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "YEA8fqzMgUHE",
    "outputId": "0946dad4-c6b4-4e53-fd7c-f43f17681ab9"
   },
   "outputs": [],
   "source": [
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTasPMJ5kTvS"
   },
   "source": [
    "**Think About:** How to choose the best $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG8id9pjo383"
   },
   "source": [
    "Check [here](https://pygam.readthedocs.io/en/latest/api/lineargam.html) for the documentation and [here](https://pygam.readthedocs.io/en/latest/notebooks/quick_start.html) for the tutorial on **Automatic model tuning**. It selects the model with the lowest **generalized cross-validation (GCV)** score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60981,
     "status": "ok",
     "timestamp": 1740159419152,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "BXUsRnKpkUxS",
    "outputId": "7c67e9d4-8c03-4d31-ebd4-b3f78ad859d6"
   },
   "outputs": [],
   "source": [
    "lam = np.logspace(-3, 3, 11)\n",
    "lams = [lam] * 3\n",
    "gam.gridsearch(X, y, lam=lams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1740159419172,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "geK_3_Cko-x6",
    "outputId": "e77724cc-bdec-4c0f-de68-1496534b5ed8"
   },
   "outputs": [],
   "source": [
    "print(f\"Optimal lambda: {gam.lam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG7FwWC738Kj"
   },
   "source": [
    "#### GAMs for Classification\n",
    "Recalling the usual logistic regression model\n",
    "$$\n",
    "\\log(\\frac{p(X)}{1-P(X)}) =  \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\epsilon_i,\n",
    "$$\n",
    "GAMs can also, analogously, extend the logistic regression model to capture non-linear relationships:\n",
    "$$\n",
    "\\log(\\frac{p(X)}{1-P(X)}) = \\beta_0 + f_1(x_{i1}) + f_2 (x_{i2}) + \\cdots + f_p (x_{ip}) + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "For the classification problem, we will use the `LogisticGAM `function from `pygam` package with the `breast_cancer` dataset. The target variable is whether the tumor of malignant (0) or benign (1), and the features are several measurements of the tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 23473,
     "status": "ok",
     "timestamp": 1740159442646,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "WHXhXSwpibbx",
    "outputId": "6e7ef7ad-d695-4dbf-ee59-7e7455a18579"
   },
   "outputs": [],
   "source": [
    "# URL for the Breast Cancer dataset (UCI Machine Learning Repository)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "# Column names for the dataset\n",
    "column_names = [\"ID\", \"Diagnosis\"] + [f\"Feature_{i}\" for i in range(1, 31)]\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'Diagnosis' column\n",
    "# M stands for Malignant\n",
    "# B stands for Benign\n",
    "df[\"Diagnosis\"] = label_encoder.fit_transform(df[\"Diagnosis\"])\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gMhlOD7kLzW"
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"Diagnosis\", axis=1).values\n",
    "y = df[\"Diagnosis\"].values\n",
    "\n",
    "# Fit a LogisticGAM with default settings as a starting point\n",
    "gam = LogisticGAM(s(0) + s(1)).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1740159442705,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "1IYAv40MMp63",
    "outputId": "c3de2f2c-bd60-4152-c007-c0a75c8f5760"
   },
   "outputs": [],
   "source": [
    "gam.accuracy(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1740159442723,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "Lz5lKHDqIDO5",
    "outputId": "c85e397d-b564-488f-f90f-a4cf272ce08c"
   },
   "outputs": [],
   "source": [
    "gam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1740159443096,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "_aVINnPbHJwf",
    "outputId": "bc565bce-4efd-434f-aa9f-3fb4c95f6398"
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "titles = [\"Feature 1\", \"Feature 2\"]\n",
    "for i, ax in enumerate(axs):\n",
    "    # create a nice grid of X data\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
    "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=0.95)[1], c=\"r\", ls=\"--\")\n",
    "    ax.set_ylim(-30, 30)\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKgunzSmh10C"
   },
   "source": [
    "## Group Work Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAINwKjbng_e"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Piecewise Constant Design Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzasC3Mlnw_O"
   },
   "source": [
    "In class, we mentioned that one can perform piecewise constant regression by appropriately modifying the design matrix to a sparse 1, 0 matrix.\n",
    "\n",
    "For the input vector $X$ and knots at $x = 3$ and $x = 6$:\n",
    "$$\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "  1\\\\\n",
    "  2\\\\\n",
    "  4\\\\\n",
    "  6\\\\\n",
    "  7\\\\\n",
    "  9\n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\n",
    "\\begin{bmatrix}\n",
    "  1 & 0 & 0 \\\\  \n",
    "  1 & 0 & 0 \\\\  \n",
    "  0 & 1 & 0 \\\\  \n",
    "  0 & 0 & 1 \\\\  \n",
    "  0 & 0 & 1 \\\\  \n",
    "  0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where the columns are obtained as $\\mathbf{1}(X < 3), \\mathbf{1}(3 \\leq X < 6), \\mathbf{1}(6 \\leq X)$.\n",
    "\n",
    "Implement a function `piecewise_constant_design_matrix` that takes in a vector `X` and a sorted array of `knots`, and returns the corresponding sparse feature matrix where each row has exactly one 1 indicating which interval the value falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0F-a1TThur4"
   },
   "outputs": [],
   "source": [
    "def piecewise_constant_design_matrix(X: np.ndarray, knots: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Construct a design matrix for piecewise constant regression with disjoint bins.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): 1D array of predictor values.\n",
    "        knots (np.ndarray): 1D array of knot positions (sorted).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Design matrix where each column corresponds to an interval\n",
    "                    between consecutive knots, with an additional column for X < min(knots)\n",
    "                    and another column for X >= max(knots).\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Strategy: Create indicator columns for each interval defined by the knots\n",
    "    X = X.reshape(-1, 1)  # Ensure X is a column vector\n",
    "    knots = np.sort(knots)  # Ensure knots are sorted\n",
    "\n",
    "    # Define regions: First region before the first knot, intermediate intervals, and last region\n",
    "    design_matrix = [(knots[0] > X).astype(int)]  # First region: X < first knot\n",
    "\n",
    "    for idx in range(len(knots) - 1):\n",
    "        # Intermediate bins: knots[idx] <= X < knots[idx+1]\n",
    "        design_matrix.append(((knots[idx] <= X) & (knots[idx + 1] > X)).astype(int))\n",
    "\n",
    "    design_matrix.append((knots[-1] <= X).astype(int))  # Last region: X >= last knot\n",
    "\n",
    "    return np.column_stack(design_matrix)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1740159443117,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "Zmdr0Y0torit",
    "outputId": "327097cc-e262-454a-8a7a-07f76f6e89c6"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# First, test with the simple example from the problem statement\n",
    "X_simple = np.array([1, 2, 4, 6, 7, 9])\n",
    "knots_simple = np.array([3, 6])\n",
    "design_simple = piecewise_constant_design_matrix(X_simple, knots_simple)\n",
    "expected_design = np.array(\n",
    "    [\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "assert np.array_equal(design_simple, expected_design), \"Simple example should match expected design\"\n",
    "\n",
    "# Random test with more samples\n",
    "np.random.seed(503)\n",
    "n_samples = 1000\n",
    "upper = np.random.uniform(0, 100)\n",
    "lower = np.random.uniform(0, upper)\n",
    "x = np.random.uniform(lower, upper, n_samples)\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "# Random knots between upper and lower\n",
    "n_knots = 6\n",
    "knots = np.random.uniform(lower, upper, n_knots)\n",
    "knots = np.sort(knots)\n",
    "\n",
    "# Generate feature matrix\n",
    "X_design = piecewise_constant_design_matrix(X, knots)\n",
    "\n",
    "# Visible test assertions\n",
    "assert X_design.sum() == n_samples, f\"Total sum should be {n_samples}, got {X_design.sum()}\"\n",
    "assert X_design.shape[0] == n_samples, f\"Should have {n_samples} rows, got {X_design.shape[0]}\"\n",
    "assert (\n",
    "    X_design.shape[1] == n_knots + 1\n",
    "), f\"Should have {n_knots + 1} columns, got {X_design.shape[1]}\"\n",
    "assert np.all(\n",
    "    X_design.sum(axis=1) == 1\n",
    "), \"Each row should sum to 1 (exactly one indicator per sample)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "expected_sum = np.array([204, 129, 76, 170, 123, 270, 28])\n",
    "assert np.array_equal(X_design.sum(axis=0), expected_sum), \"Column sums should match expected\"\n",
    "\n",
    "# Test edge case with values exactly on knots\n",
    "X_knot = np.array([3.0, 6.0])\n",
    "design_knot = piecewise_constant_design_matrix(X_knot, knots_simple)\n",
    "assert design_knot[0, 1] == 1, \"Value exactly on first knot should be in middle interval\"\n",
    "assert design_knot[1, 2] == 1, \"Value exactly on last knot should be in last interval\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "so2o0r4Jnleb"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Piecewise Constant Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIVxSX0rqYFk"
   },
   "source": [
    "Create a function `piecewise_constant_regression` that takes in `X`, `y`, and a list of `knots` and performs piecewise constant regression using the design matrix from Problem 1.\n",
    "\n",
    "The function should return a fitted `LinearRegression` model (with `fit_intercept=False` since the design matrix already captures the constant pieces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41i4yOvWqYaz"
   },
   "outputs": [],
   "source": [
    "def piecewise_constant_regression(\n",
    "    X: np.ndarray, y: np.ndarray, knots: np.ndarray\n",
    ") -> LinearRegression:\n",
    "    \"\"\"\n",
    "    Perform piecewise constant regression using a design matrix.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): 1D array of predictor values.\n",
    "        y (np.ndarray): 1D array of target values.\n",
    "        knots (np.ndarray): 1D array of knot positions (sorted).\n",
    "\n",
    "    Returns:\n",
    "        LinearRegression: Fitted LinearRegression model for piecewise constant regression.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Strategy: Transform X to design matrix then fit linear regression without intercept\n",
    "    X_design = piecewise_constant_design_matrix(X, knots)\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_design, y)\n",
    "    return model\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1740159443324,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "Zipx20ypjXAL",
    "outputId": "996f7837-5185-40e7-af6d-4d38629acdaf"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "np.random.seed(503)\n",
    "n_samples = 300\n",
    "x = np.random.uniform(0, 10, n_samples)\n",
    "y = 2 * np.sin(0.5 * x) + np.cos(x) + np.sqrt(0.1) * np.random.randn(n_samples)\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "knots = np.array([2, 5, 8])\n",
    "\n",
    "# Fit the piecewise constant model\n",
    "model = piecewise_constant_regression(X, y, knots)\n",
    "\n",
    "# Verify model properties\n",
    "assert hasattr(model, \"coef_\"), \"Model should have coefficients\"\n",
    "assert model.coef_.shape[0] == len(knots) + 1, \"Should have one coefficient per interval\"\n",
    "\n",
    "# Plot the piecewise constant fit\n",
    "X_fit = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "X_fit_design = piecewise_constant_design_matrix(X_fit, knots)\n",
    "y_pred = model.predict(X_fit_design)\n",
    "\n",
    "# Compute sum of squared residuals\n",
    "y_pred_train = model.predict(piecewise_constant_design_matrix(X, knots))\n",
    "ssr = np.mean((y - y_pred_train) ** 2)\n",
    "\n",
    "# Scatter plot data\n",
    "plt.scatter(X, y, color=\"gray\", alpha=0.2, label=\"Data\")\n",
    "plt.plot(X_fit, model.predict(X_fit_design), label=\"Piecewise Constant Fit\", color=\"red\")\n",
    "plt.text(\n",
    "    0.95,\n",
    "    0.95,\n",
    "    f\"MSE: {ssr:.2f}\",\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=12,\n",
    "    verticalalignment=\"top\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify the coefficients are close to expected means in each interval\n",
    "expected_coefs = np.array([1.67, 1.96, -0.47, 1.44])\n",
    "assert np.allclose(model.coef_, expected_coefs, atol=0.1), \"Coefficients should match expected\"\n",
    "\n",
    "# Test that fit_intercept is False\n",
    "assert model.fit_intercept is False, \"Model should not have intercept\"\n",
    "\n",
    "# Test MSE is reasonable\n",
    "assert ssr < 1.0, \"MSE should be less than 1.0\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxsmTCDinn7e"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Loading NHANES Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns_bvhJFnzdw"
   },
   "source": [
    "Import the NHANES datasets and prepare them for analysis:\n",
    "\n",
    "(a) Keep the HDL in mg/dL as your target variable. Also, use the following features for predictive purposes: Gender, Age, Weight, Height, BMI, WaistSize, Household Size, and Ethnicity. You may need to refer to the docs to figure out their variable names:\n",
    "- [HDL_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/HDL_L.htm)\n",
    "- [DEMO_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.htm)\n",
    "- [BMX_L](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/BMX_L.htm)\n",
    "\n",
    "(b) Rename the variable names to be human-readable in Python variable style (e.g., `BMXWT` becomes `Weight`).\n",
    "\n",
    "(c) Drop all missing values.\n",
    "\n",
    "Store the final DataFrame in a variable called `my_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OlDkJDXnpLq"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Strategy: Load SAS files, merge on SEQN, select and rename columns, drop NAs\n",
    "bmx_df = pd.read_sas(\"data/NHANES/BMX_L.xpt\")\n",
    "demo_df = pd.read_sas(\"data/NHANES/DEMO_L.xpt\")\n",
    "hdl_df = pd.read_sas(\"data/NHANES/HDL_L.xpt\")\n",
    "\n",
    "# Inner join on SEQN (sequence number)\n",
    "df = pd.merge(hdl_df, bmx_df, on=\"SEQN\", how=\"inner\")\n",
    "df = pd.merge(df, demo_df, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "# Select and rename columns\n",
    "selected_columns = [\n",
    "    \"LBDHDD\",\n",
    "    \"RIAGENDR\",\n",
    "    \"RIDAGEYR\",\n",
    "    \"BMXWT\",\n",
    "    \"BMXHT\",\n",
    "    \"DMDHHSIZ\",\n",
    "    \"BMXBMI\",\n",
    "    \"BMXWAIST\",\n",
    "    \"RIDRETH1\",\n",
    "]\n",
    "\n",
    "my_df = df[selected_columns].copy()\n",
    "my_df = my_df.rename(\n",
    "    columns={\n",
    "        \"LBDHDD\": \"HDL\",\n",
    "        \"RIAGENDR\": \"Gender\",\n",
    "        \"RIDAGEYR\": \"Age\",\n",
    "        \"BMXWT\": \"Weight\",\n",
    "        \"BMXHT\": \"Height\",\n",
    "        \"BMXBMI\": \"BMI\",\n",
    "        \"BMXWAIST\": \"WaistSize\",\n",
    "        \"DMDHHSIZ\": \"HouseholdSize\",\n",
    "        \"RIDRETH1\": \"Ethnicity\",\n",
    "    }\n",
    ")\n",
    "my_df = my_df.dropna()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3lSAA-Gu1sQ"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert my_df.shape == (6589, 9), f\"Expected shape (6589, 9), got {my_df.shape}\"\n",
    "assert \"HDL\" in my_df.columns, \"DataFrame should have HDL column\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "expected_columns = [\n",
    "    \"HDL\",\n",
    "    \"Gender\",\n",
    "    \"Age\",\n",
    "    \"Weight\",\n",
    "    \"Height\",\n",
    "    \"BMI\",\n",
    "    \"WaistSize\",\n",
    "    \"HouseholdSize\",\n",
    "    \"Ethnicity\",\n",
    "]\n",
    "assert list(my_df.columns) == expected_columns, \"Columns should match expected names\"\n",
    "assert my_df.isna().sum().sum() == 0, \"No missing values should remain\"\n",
    "assert my_df[\"HDL\"].dtype == np.float64, \"HDL should be float\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFCrgAVhntMd"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Spline Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EBm-RfZn0Aq"
   },
   "source": [
    "Create a sklearn `Pipeline` called `pipeline` that performs the following steps:\n",
    "\n",
    "1. Create degree 4 spline features with 8 knots using `SplineTransformer`\n",
    "2. Apply `StandardScaler` for feature scaling\n",
    "3. Fit a `LinearRegression` model\n",
    "\n",
    "Fit the pipeline using BMI as the predictor (`X = my_df[['BMI']]`) and HDL as the target (`y = my_df[['HDL']]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740159443447,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "S54kGSyyntvX",
    "outputId": "4498486a-bfa9-4d59-8627-b8ad8e650493"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Strategy: Chain SplineTransformer, StandardScaler, and LinearRegression in a Pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"spline_features\", SplineTransformer(degree=4, n_knots=8)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = my_df[[\"BMI\"]]\n",
    "y = my_df[[\"HDL\"]]\n",
    "pipeline.fit(X, y)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1740159444175,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "bm2jA0MKwrPi",
    "outputId": "fe84951c-7b99-4821-9c71-42121b44bff1"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "titles = [\"BMI vs. HDL\", \"Residuals of BMI vs. HDL\"]\n",
    "axs[0].scatter(X, y, color=\"gray\", alpha=0.1, label=\"Data\")\n",
    "x_linspace = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "axs[0].plot(x_linspace, pipeline.predict(x_linspace), label=\"Spline Fit\", color=\"red\")\n",
    "axs[0].set_xlabel(\"BMI\")\n",
    "axs[0].set_ylabel(\"HDL\")\n",
    "axs[0].set_title(titles[0])\n",
    "axs[0].legend()\n",
    "axs[1].scatter(my_df[\"BMI\"], pipeline.predict(X) - y, color=\"gray\", alpha=0.1)\n",
    "axs[1].set_xlabel(\"BMI\")\n",
    "axs[1].set_ylabel(\"Residuals\")\n",
    "axs[1].set_title(titles[1])\n",
    "plt.show()\n",
    "\n",
    "# Verify pipeline structure\n",
    "assert len(pipeline.steps) == 3, \"Pipeline should have 3 steps\"\n",
    "assert pipeline.named_steps[\"spline_features\"].degree == 4, \"Spline degree should be 4\"\n",
    "assert pipeline.named_steps[\"spline_features\"].n_knots == 8, \"Number of knots should be 8\"\n",
    "print(f\"Linear Regression Coefficients: {pipeline.named_steps['regressor'].coef_[0]}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify the pipeline components\n",
    "assert \"spline_features\" in pipeline.named_steps, \"Should have spline_features step\"\n",
    "assert \"scaler\" in pipeline.named_steps, \"Should have scaler step\"\n",
    "assert \"regressor\" in pipeline.named_steps, \"Should have regressor step\"\n",
    "\n",
    "# Verify coefficient shape (degree 4 + n_knots 8 = 4 + 8 - 1 = 11 features)\n",
    "n_features = pipeline.named_steps[\"regressor\"].coef_.shape[1]\n",
    "assert n_features == 11, f\"Expected 11 features, got {n_features}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnOtPBNI0UAZ"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** GridSearchCV for Spline Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igb8LbQM0YSQ"
   },
   "source": [
    "We want to decide how many knots and what degree of spline to use. Working with all the predictive features (everything except HDL), use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find the optimal parameter values.\n",
    "\n",
    "Use the following settings:\n",
    "- 10-fold cross-validation\n",
    "- `neg_mean_squared_error` as the scoring metric\n",
    "- Search over these parameters:\n",
    "  - `degree`: 2, 3, 4\n",
    "  - `n_knots`: 4, 8, 16, 32\n",
    "\n",
    "Store your fitted `GridSearchCV` object in a variable called `grid_search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 12245,
     "status": "ok",
     "timestamp": 1740159456421,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "iktUoBe7w7N4",
    "outputId": "169f5a89-9897-448a-8e93-53d6f634389c"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Strategy: Set up pipeline with SplineTransformer and use GridSearchCV to find optimal parameters\n",
    "X = my_df.drop(\"HDL\", axis=1)\n",
    "y = my_df[\"HDL\"]\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"spline_features\", SplineTransformer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"spline_features__n_knots\": [4, 8, 16, 32],\n",
    "    \"spline_features__degree\": [2, 3, 4],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV using neg_mean_squared_error\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid, cv=10, scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRuGee8V3aMX"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "expected = {\"spline_features__degree\": 2, \"spline_features__n_knots\": 4}\n",
    "assert grid_search.best_params_ == expected, f\"Expected {expected}, got {grid_search.best_params_}\"\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.4f}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify GridSearchCV settings\n",
    "assert grid_search.cv == 10, \"Should use 10-fold CV\"\n",
    "assert grid_search.scoring == \"neg_mean_squared_error\", \"Should use neg_mean_squared_error\"\n",
    "\n",
    "# Verify coefficient shape matches expected\n",
    "n_features = 8  # number of predictors\n",
    "n_spline_features = n_features * (4 + 2 - 1)  # n_knots=4, degree=2\n",
    "assert grid_search.best_estimator_.named_steps[\"regressor\"].coef_.shape == (\n",
    "    n_spline_features,\n",
    "), f\"Coefficient shape should be ({n_spline_features},)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRrfA64h6IEu"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Loading the Default Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOQMrKIy68J-"
   },
   "source": [
    "Import the `default` dataset from `pygam.datasets`. See [documentation](https://pygam.readthedocs.io/en/latest/) and [GitHub](https://github.com/dswah/pyGAM/blob/master/pygam/datasets/load_datasets.py).\n",
    "\n",
    "Create an 80-20 train-test split with `random_state=503` and store the results in `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1740159970063,
     "user": {
      "displayName": "Paolo Borello",
      "userId": "18258600971905049184"
     },
     "user_tz": 300
    },
    "id": "wxD82IUw5lHr"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Strategy: Load default dataset and create train-test split\n",
    "X, y = default(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=503)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert X_train.shape[0] == 8000, f\"Expected 8000 training samples, got {X_train.shape[0]}\"\n",
    "assert X_test.shape[0] == 2000, f\"Expected 2000 test samples, got {X_test.shape[0]}\"\n",
    "assert X_train.shape[1] == 3, \"Features should have 3 columns\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert y_train.shape[0] == 8000, \"y_train should have 8000 samples\"\n",
    "assert y_test.shape[0] == 2000, \"y_test should have 2000 samples\"\n",
    "assert X.shape == (10000, 3), \"Full X should be (10000, 3)\"\n",
    "assert set(y) == {0, 1}, \"y should be binary\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OZcTZfoCf7r"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Logistic GAM with Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sem92RzTCiTo"
   },
   "source": [
    "Fit a `LogisticGAM` model with:\n",
    "- A factor term `f()` on the first variable (student status, index 0)\n",
    "- Spline terms `s()` on the remaining variables (balance and income, indices 1 and 2)\n",
    "\n",
    "Use the `gridsearch` method to find optimal lambda values. You can use the default lambda grid which uses `lam=np.logspace(-3, 3, 11)`.\n",
    "\n",
    "Store your fitted GAM in a variable called `gam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dlVWCuEJit7"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Strategy: Use LogisticGAM with factor term on first variable and splines on the rest\n",
    "# Use gridsearch to find optimal lambda values\n",
    "gam = LogisticGAM(f(0) + s(1) + s(2)).gridsearch(X_train, y_train)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1740159464576,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "g_1PDHytBcv4",
    "outputId": "e6fe04c6-2877-4c28-c140-2b262184e842"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify the model has expected structure\n",
    "assert hasattr(gam, \"lam\"), \"GAM should have lambda parameters after gridsearch\"\n",
    "assert len(gam.terms) == 4, \"Should have 3 terms plus intercept\"\n",
    "print(f\"Optimal lambdas: {gam.lam}\")\n",
    "\n",
    "# Plot partial dependence\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "titles = [\"student\", \"balance\", \"income\"]\n",
    "\n",
    "for term_idx, ax in enumerate(axs):\n",
    "    XX = gam.generate_X_grid(term=term_idx)\n",
    "    pdep, confi = gam.partial_dependence(term=term_idx, width=0.95)\n",
    "    ax.plot(XX[:, term_idx], pdep)\n",
    "    ax.plot(XX[:, term_idx], confi, c=\"r\", ls=\"--\")\n",
    "    ax.set_title(titles[term_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify the terms are correctly specified\n",
    "assert gam.terms[0].isintercept is False, \"First term should not be intercept\"\n",
    "assert gam.accuracy(X_test, y_test) > 0.9, \"Model accuracy on test set should be > 90%\"\n",
    "\n",
    "# Verify gridsearch was performed\n",
    "train_accuracy = gam.accuracy(X_train, y_train)\n",
    "assert train_accuracy > 0.9, f\"Training accuracy {train_accuracy} should be > 0.9\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ7LOkl1Dezj"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 8:** Interpreting Partial Dependence Plots (free response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_EBax0nDjJo"
   },
   "source": [
    "Examine the partial dependence plots from Problem 7 and comment on what they reveal about the relationship between each predictor and the probability of default. Consider:\n",
    "\n",
    "- What is the effect of student status on default probability?\n",
    "- How does balance affect the log-odds of default?\n",
    "- What is the relationship between income and default probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qiEfn0yDkG9"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The partial dependence plots reveal the following relationships:\n",
    "\n",
    "1. **Student status**: Being a student (value 1) is associated with a higher log-odds of default compared to non-students (value 0). This suggests that students have a higher probability of defaulting on their credit, possibly due to lower and less stable income during their studies.\n",
    "\n",
    "2. **Balance**: There is a strong positive, roughly linear relationship between credit card balance and the log-odds of default. As balance increases, the probability of default increases substantially. This makes intuitive sense as higher balances indicate more debt and potentially greater financial strain.\n",
    "\n",
    "3. **Income**: Income shows a relatively flat relationship with default probability, with wide confidence intervals indicating uncertainty. This suggests that income, after controlling for balance and student status, has little direct effect on the probability of default. The effect of income may already be captured through its correlation with balance levels.\n",
    "> END SOLUTION\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
