{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQpC4ElvTRx3"
   },
   "source": [
    "# DATASCI 503, Group Work 10: Stochastic Gradient Descent for Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za8bM6U244NC"
   },
   "source": [
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1743180542471,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "AMRr8jXruk3W"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u--4jQqLnbt"
   },
   "source": [
    "## Part 1: Gradient Descent for a Given Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd5wcjZOLnbu"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Gradient Descent for a Function of One Argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk1OnAviLnbu"
   },
   "source": [
    "Recall that gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the function's gradient.\n",
    "\n",
    "Namely, the procedure is as follows:\n",
    "1. Start with a random point $w$.\n",
    "2. Update $w$ by moving in the direction of the negative gradient of the function at $w$: $$w \\leftarrow w - \\eta \\nabla f(w).$$\n",
    "Here, $\\eta$ is the *learning rate*; it controls the size of the step we're taking in the direction of the negative gradient.\n",
    "3. Repeat step 2 until the variable $w$ changes very little between iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLVuZJn63z23"
   },
   "source": [
    "Below, write code to minimize $$f(w) = (w -2)^2$$ using gradient descent. Your function should return the minimizer $w^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1743180542527,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "UeHaeLJy4QR2"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_1d(epsilon: float, initial_w: float, eta: float) -> float:\n",
    "    \"\"\"\n",
    "    Minimize f(w) = (w - 2)^2 using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        epsilon: tolerance for convergence\n",
    "        initial_w: initial value of w\n",
    "        eta: learning rate\n",
    "\n",
    "    Returns:\n",
    "        The minimizer w* (approximately 2.0)\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # The gradient of f(w) = (w-2)^2 is f'(w) = 2*(w-2)\n",
    "    w_prev = float(\"inf\")\n",
    "    weight = initial_w\n",
    "\n",
    "    while abs(weight - w_prev) > epsilon:\n",
    "        w_prev = weight\n",
    "        gradient = 2 * (weight - 2)\n",
    "        weight = weight - eta * gradient\n",
    "\n",
    "    return weight\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1743180542532,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "6WCW0oPVGqCb"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "result = gradient_descent_1d(epsilon=0.0001, initial_w=5.0, eta=0.1)\n",
    "assert abs(result - 2) < 0.001, f\"Expected result near 2, got {result}\"\n",
    "\n",
    "result_neg = gradient_descent_1d(epsilon=0.0001, initial_w=-3.0, eta=0.1)\n",
    "assert abs(result_neg - 2) < 0.001, f\"Expected result near 2 from negative start, got {result_neg}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with different learning rates\n",
    "result_slow = gradient_descent_1d(epsilon=0.0001, initial_w=10.0, eta=0.05)\n",
    "assert abs(result_slow - 2) < 0.001, f\"Failed with slower learning rate: {result_slow}\"\n",
    "\n",
    "result_tight = gradient_descent_1d(epsilon=0.00001, initial_w=0.0, eta=0.2)\n",
    "assert abs(result_tight - 2) < 0.0001, f\"Failed with tighter epsilon: {result_tight}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aatb8VZeLnbw"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Gradient Descent for a Function of Two Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hum3fSMg4gOc"
   },
   "source": [
    "Now we want to minimize the Rosenbrock function:\n",
    "\n",
    "$$f(w_1, w_2) = (1-w_1)^2 + 100 (w_2 - w_1^2)^2$$\n",
    "\n",
    "using gradient descent. The minimum is at $(1, 1)$.\n",
    "\n",
    "**Hint:** You can treat $w$ as a vector, but this is not necessary.\n",
    "\n",
    "**Hint:** If your algorithm is not converging, try adjusting the learning rate first. The Rosenbrock function has a narrow curved valley, so a smaller learning rate may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1743180542535,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "trdL2MdU4xfp"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_2d(epsilon: float, initial_w: np.ndarray, eta: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Minimize the Rosenbrock function using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        epsilon: tolerance for convergence\n",
    "        initial_w: initial weight vector [w1, w2]\n",
    "        eta: learning rate\n",
    "\n",
    "    Returns:\n",
    "        The minimizer (approximately [1.0, 1.0])\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Gradient: df/dw1 = -2(1-w1) + 100*2*(w2-w1^2)*(-2*w1)\n",
    "    #           df/dw2 = 100*2*(w2-w1^2)\n",
    "    w_prev = np.array([float(\"inf\"), float(\"inf\")])\n",
    "    weights = initial_w.copy()\n",
    "\n",
    "    while np.linalg.norm(weights - w_prev, 2) > epsilon:\n",
    "        w1, w2 = weights\n",
    "        grad_w1 = -2 * (1 - w1) + 400 * (w2 - w1**2) * (-w1)\n",
    "        grad_w2 = 200 * (w2 - w1**2)\n",
    "        gradient = np.array([grad_w1, grad_w2])\n",
    "\n",
    "        w_prev = weights.copy()\n",
    "        weights = weights - eta * gradient\n",
    "\n",
    "    return weights\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1743180542890,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "uvPgyFGEGMmn"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "result = gradient_descent_2d(epsilon=0.00001, initial_w=np.array([2.0, 1.0]), eta=0.001)\n",
    "expected = np.array([1.0, 1.0])\n",
    "assert np.linalg.norm(result - expected, 2) < 0.05, f\"Expected near [1, 1], got {result}\"\n",
    "\n",
    "result2 = gradient_descent_2d(epsilon=0.00001, initial_w=np.array([0.0, 0.0]), eta=0.001)\n",
    "assert (\n",
    "    np.linalg.norm(result2 - expected, 2) < 0.05\n",
    "), f\"Expected near [1, 1] from origin, got {result2}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "result3 = gradient_descent_2d(epsilon=0.000001, initial_w=np.array([1.5, 2.0]), eta=0.001)\n",
    "assert np.linalg.norm(result3 - expected, 2) < 0.02, f\"Failed with different start: {result3}\"\n",
    "\n",
    "result4 = gradient_descent_2d(epsilon=0.00001, initial_w=np.array([-0.5, 0.5]), eta=0.001)\n",
    "assert np.linalg.norm(result4 - expected, 2) < 0.05, f\"Failed with negative start: {result4}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Oz8u8ELnbx"
   },
   "source": [
    "## Part 2: Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5EwK9jCLnbx"
   },
   "source": [
    "The previous functions just depended on some parameters $(w_1, w_2)$. Now we are interested in optimizing loss functions that depend on data $(X, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iAKs7h3Lnbx"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeYL2LKdLnbx"
   },
   "source": [
    "We are interested in:\n",
    "\n",
    "1. Fitting a simple linear model:\n",
    "$$\\widehat{y}_i = \\alpha + \\beta x_i$$\n",
    "\n",
    "2. By minimizing the mean squared error loss:\n",
    "$$L(\\alpha, \\beta) = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2$$\n",
    "\n",
    "   (Note: The $\\frac{1}{2}$ factor is a convention in machine learning; it simplifies the gradient and does not change the minimizer.)\n",
    "\n",
    "3. Using gradient descent updates:\n",
    "$$\\alpha_k = \\alpha_{k-1} - \\eta \\frac{\\partial L}{\\partial \\alpha}$$\n",
    "$$\\beta_k = \\beta_{k-1} - \\eta \\frac{\\partial L}{\\partial \\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-MHhEZMLnby"
   },
   "source": [
    "You can check convergence of gradient descent in either of two ways:\n",
    "\n",
    "- Check if the parameter changes are smaller than epsilon:\n",
    "$$|\\alpha_k - \\alpha_{k-1}| + |\\beta_k - \\beta_{k-1}| < \\epsilon$$\n",
    "\n",
    "- Or check whether the change in the loss function is smaller than epsilon:\n",
    "$$|L_k - L_{k-1}| < \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1743180542916,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "Kc7vAQZXLnby"
   },
   "outputs": [],
   "source": [
    "def simple_linear_regression_gd(\n",
    "    features: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    eta: float,\n",
    "    initial_alpha: float,\n",
    "    initial_beta: float,\n",
    "    epsilon: float,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Fit simple linear regression using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        features: input feature array (1D)\n",
    "        targets: target values array (1D)\n",
    "        eta: learning rate\n",
    "        initial_alpha: initial intercept value\n",
    "        initial_beta: initial slope value\n",
    "        epsilon: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (alpha, beta) - the fitted intercept and slope\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Gradients: dL/dalpha = -mean(y - alpha - beta*x)\n",
    "    #            dL/dbeta = -mean((y - alpha - beta*x) * x)\n",
    "\n",
    "    def compute_loss(alpha, beta):\n",
    "        residuals = targets - alpha - beta * features\n",
    "        return 0.5 * np.mean(residuals**2)\n",
    "\n",
    "    def grad_alpha(alpha, beta):\n",
    "        return -np.mean(targets - alpha - beta * features)\n",
    "\n",
    "    def grad_beta(alpha, beta):\n",
    "        return -np.mean((targets - alpha - beta * features) * features)\n",
    "\n",
    "    alpha_prev, beta_prev = float(\"inf\"), float(\"inf\")\n",
    "    alpha, beta = initial_alpha, initial_beta\n",
    "    loss_prev = float(\"inf\")\n",
    "\n",
    "    while True:\n",
    "        loss_current = compute_loss(alpha, beta)\n",
    "        param_change = abs(alpha - alpha_prev) + abs(beta - beta_prev)\n",
    "        loss_change = abs(loss_current - loss_prev)\n",
    "\n",
    "        if param_change < epsilon or loss_change < epsilon:\n",
    "            break\n",
    "\n",
    "        alpha_prev, beta_prev = alpha, beta\n",
    "        loss_prev = loss_current\n",
    "\n",
    "        # Compute both gradients at the current point before updating\n",
    "        grad_a = grad_alpha(alpha, beta)\n",
    "        grad_b = grad_beta(alpha, beta)\n",
    "        alpha = alpha - eta * grad_a\n",
    "        beta = beta - eta * grad_b\n",
    "\n",
    "    return alpha, beta\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obqsSWOrLnby"
   },
   "source": [
    "Use the following cells to test your solution. The obtained coefficients do not need to be exactly the same as scikit-learn's, but should be close. Making epsilon smaller will yield more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1743180542944,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "Hk7FaN8CLnby"
   },
   "outputs": [],
   "source": [
    "# Generate some random data for testing\n",
    "np.random.seed(42)\n",
    "x_test = np.random.rand(100)\n",
    "true_alpha = np.random.rand(1)\n",
    "true_beta = np.random.rand(1)\n",
    "y_test = true_alpha + x_test * true_beta + np.random.rand(100) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_eHmUL3Lnby"
   },
   "source": [
    "Check the solution using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7141,
     "status": "ok",
     "timestamp": 1743180550093,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "DVwrVsBSLnby",
    "outputId": "b88543b0-06c2-4f76-c88e-c71468692625"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_simple = LinearRegression()\n",
    "_ = lr_simple.fit(x_test[:, None], y_test)\n",
    "print(f\"Sklearn intercept: {lr_simple.intercept_}, slope: {lr_simple.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1743180550122,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "zRr24qCtLnby",
    "outputId": "395aae6d-c1c3-46ff-ee71-f8f28648db94"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "eta, initial_alpha, initial_beta, epsilon = 0.1, 0, 0, 0.0001\n",
    "alpha_hat, beta_hat = simple_linear_regression_gd(\n",
    "    x_test, y_test, eta, initial_alpha, initial_beta, epsilon\n",
    ")\n",
    "print(f\"Gradient descent intercept: {alpha_hat}, slope: {beta_hat}\")\n",
    "\n",
    "sklearn_params = np.array([lr_simple.intercept_, lr_simple.coef_[0]])\n",
    "gd_params = np.array([alpha_hat, beta_hat])\n",
    "assert (\n",
    "    np.linalg.norm(gd_params - sklearn_params, 1) < 0.1\n",
    "), f\"Parameters differ too much from sklearn: {gd_params} vs {sklearn_params}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with different initial values\n",
    "alpha_hat2, beta_hat2 = simple_linear_regression_gd(\n",
    "    x_test, y_test, eta=0.2, initial_alpha=1.0, initial_beta=1.0, epsilon=0.00001\n",
    ")\n",
    "assert (\n",
    "    np.linalg.norm(np.array([alpha_hat2, beta_hat2]) - sklearn_params, 1) < 0.05\n",
    "), \"Failed with different initial values\"\n",
    "\n",
    "# Test with smaller learning rate\n",
    "alpha_hat3, beta_hat3 = simple_linear_regression_gd(\n",
    "    x_test, y_test, eta=0.05, initial_alpha=0.5, initial_beta=0.5, epsilon=0.0001\n",
    ")\n",
    "assert (\n",
    "    np.linalg.norm(np.array([alpha_hat3, beta_hat3]) - sklearn_params, 1) < 0.1\n",
    "), \"Failed with smaller learning rate\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl7KunfcLnby"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5tdpdlhLnb5"
   },
   "source": [
    "Now implement gradient descent for multivariate linear regression with multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOb6dU_HLnb5"
   },
   "source": [
    "The loss function for multivariate linear regression is:\n",
    "\n",
    "$$L(w) = \\frac{1}{2n}\\sum_{i=1}^n (y_i - x_i^T w)^2 = \\frac{1}{2n}||Y - X w||^2$$\n",
    "\n",
    "where each $x_i$ is a row vector (or correspondingly, $X$ is a matrix with rows as samples), and $y_i$ is the corresponding element of the target vector $Y$.\n",
    "\n",
    "The gradient of this loss function is:\n",
    "\n",
    "$$\\nabla L = \\frac{1}{n} X^T(Xw - Y)$$\n",
    "\n",
    "The gradient descent update rule in vector form is:\n",
    "\n",
    "$$w_k = w_{k-1} - \\eta \\nabla L$$\n",
    "\n",
    "where $w$ is the weight vector, $\\eta$ is the learning rate, and $\\nabla L$ is the gradient.\n",
    "\n",
    "Gradient descent converges when either:\n",
    "- The weight change is smaller than epsilon: $||w_k - w_{k-1}|| < \\epsilon$\n",
    "- The loss change is smaller than epsilon: $|L_k - L_{k-1}| < \\epsilon$\n",
    "\n",
    "Unlike in the previous question, please also keep track of the loss at each iteration and return the complete loss history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBpi9X97Lnb5"
   },
   "source": [
    "**Note:** Your function should automatically add a column of ones to X to fit the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1743180550136,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "eCGv13RPLnb5"
   },
   "outputs": [],
   "source": [
    "def multivariate_linear_regression_gd(\n",
    "    features: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    eta: float,\n",
    "    initial_weights: np.ndarray,\n",
    "    epsilon: float,\n",
    ") -> tuple[np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Fit multivariate linear regression using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        features: input feature matrix (n_samples x n_features)\n",
    "        targets: target values array (n_samples,)\n",
    "        eta: learning rate\n",
    "        initial_weights: initial weight vector (n_features + 1,) including intercept\n",
    "        epsilon: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (weights, losses) where weights is the fitted weight vector\n",
    "        and losses is the list of loss values at each iteration\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    num_samples = features.shape[0]\n",
    "\n",
    "    # Add column of ones for intercept\n",
    "    ones_column = np.ones((num_samples, 1))\n",
    "    design_matrix = np.hstack((ones_column, features))\n",
    "    target_vector = np.array(targets).reshape((num_samples, 1))\n",
    "\n",
    "    def compute_loss(weights):\n",
    "        residuals = target_vector - design_matrix @ weights\n",
    "        return 0.5 * np.mean(residuals**2)\n",
    "\n",
    "    def compute_gradient(weights):\n",
    "        return design_matrix.T @ (design_matrix @ weights - target_vector) / num_samples\n",
    "\n",
    "    losses = []\n",
    "    weights = initial_weights.copy()\n",
    "    weights_prev = np.ones_like(weights) * 100.0\n",
    "    loss_prev = float(\"inf\")\n",
    "\n",
    "    while True:\n",
    "        loss_current = compute_loss(weights)\n",
    "        weight_change = np.linalg.norm(weights - weights_prev, 1)\n",
    "        loss_change = abs(loss_current - loss_prev)\n",
    "\n",
    "        if weight_change < epsilon or loss_change < epsilon:\n",
    "            break\n",
    "\n",
    "        weights_prev = weights.copy()\n",
    "        loss_prev = loss_current\n",
    "        losses.append(loss_current)\n",
    "\n",
    "        gradient = compute_gradient(weights)\n",
    "        weights = weights - eta * gradient\n",
    "\n",
    "    return weights, losses\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMQ0obUkLnb5"
   },
   "source": [
    "We will use real-world data to test our solution. The coefficients obtained may not match sklearn exactly, but the training loss should decrease monotonically and be comparable to sklearn's solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3325,
     "status": "ok",
     "timestamp": 1743180553464,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "4BI3jm8lLnb5"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the California housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing_data = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "housing_features = housing_data[0]\n",
    "housing_targets = housing_data[1]\n",
    "housing_train_features, housing_val_features, housing_train_targets, housing_val_targets = (\n",
    "    train_test_split(housing_features, housing_targets, test_size=0.3, random_state=42)\n",
    ")\n",
    "housing_scaler = StandardScaler()\n",
    "housing_train_scaled = housing_scaler.fit_transform(housing_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "executionInfo": {
     "elapsed": 1895,
     "status": "ok",
     "timestamp": 1743180555357,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "NkW-Qha-Lnb5",
    "outputId": "b7543f1d-70cf-4302-d2f0-8e79b9b4d658"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "np.random.seed(42)\n",
    "housing_num_weights = housing_features.shape[1] + 1\n",
    "housing_weights_gd, housing_loss_history = multivariate_linear_regression_gd(\n",
    "    housing_train_scaled,\n",
    "    housing_train_targets,\n",
    "    eta=0.01,\n",
    "    initial_weights=np.random.randn(housing_num_weights, 1),\n",
    "    epsilon=0.001,\n",
    ")\n",
    "\n",
    "# Compute predictions on validation set\n",
    "housing_val_scaled = housing_scaler.transform(housing_val_features)\n",
    "housing_val_with_intercept = np.hstack(\n",
    "    (np.ones((housing_val_features.shape[0], 1)), housing_val_scaled)\n",
    ")\n",
    "housing_predictions_val = housing_val_with_intercept @ housing_weights_gd\n",
    "housing_val_targets_reshaped = np.array(housing_val_targets).reshape(\n",
    "    (housing_val_targets.shape[0], 1)\n",
    ")\n",
    "\n",
    "plt.plot(np.arange(1, len(housing_loss_history) + 1), housing_loss_history, color=\"red\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Iterations\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {housing_loss_history[-1]}\")\n",
    "val_loss = 0.5 * np.mean((housing_val_targets_reshaped - housing_predictions_val) ** 2)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "# Verify loss decreases (allowing small numerical fluctuations)\n",
    "assert len(housing_loss_history) > 1, \"Should have multiple iterations\"\n",
    "assert housing_loss_history[-1] < housing_loss_history[0], \"Loss should decrease overall\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check loss is decreasing monotonically (with small tolerance for numerical issues)\n",
    "for idx in range(1, len(housing_loss_history)):\n",
    "    assert (\n",
    "        housing_loss_history[idx] <= housing_loss_history[idx - 1] + 1e-6\n",
    "    ), f\"Loss not monotonically decreasing at {idx}\"\n",
    "\n",
    "# Check final loss is reasonable (should be close to sklearn's ~0.26)\n",
    "assert housing_loss_history[-1] < 0.5, f\"Final loss too high: {housing_loss_history[-1]}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1743180555365,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "-QGyB_8mLnb6",
    "outputId": "d2940dcd-f134-4e01-eee1-29a9ee860486"
   },
   "outputs": [],
   "source": [
    "# Compare with sklearn's LinearRegression\n",
    "housing_sklearn_model = LinearRegression(fit_intercept=True)\n",
    "_ = housing_sklearn_model.fit(housing_train_scaled, housing_train_targets)\n",
    "print(f\"Sklearn intercept: {housing_sklearn_model.intercept_}\")\n",
    "print(f\"Sklearn coefficients: {housing_sklearn_model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1743180555369,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "t779nJXyLnb6",
    "outputId": "06d4ff37-bede-4c9a-ff1b-50838dcea029"
   },
   "outputs": [],
   "source": [
    "# Compare losses between gradient descent and sklearn\n",
    "sklearn_train_pred = housing_sklearn_model.predict(housing_train_scaled)\n",
    "housing_sklearn_train_loss = 0.5 * np.mean((housing_train_targets - sklearn_train_pred) ** 2)\n",
    "sklearn_val_pred = housing_sklearn_model.predict(housing_val_scaled)\n",
    "housing_sklearn_val_loss = 0.5 * np.mean((housing_val_targets - sklearn_val_pred) ** 2)\n",
    "print(f\"Sklearn training loss: {housing_sklearn_train_loss}\")\n",
    "print(f\"Sklearn validation loss: {housing_sklearn_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oKJ2sl-Lnb6"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdqW5d-NLnb6"
   },
   "source": [
    "In mini-batch gradient descent, we apply gradient descent to a subset of the data (called a mini-batch or batch) at each iteration. The algorithm is as follows:\n",
    "\n",
    "**Inputs:** $X, Y, w_0, \\eta, \\epsilon, |B|$ (batch size)\n",
    "\n",
    "**Step 0:** Set $n$ = number of samples and calculate the number of batches as $m = \\lceil n/|B| \\rceil$\n",
    "\n",
    "**Step 1:** Initialize weights $w_k = w_0$\n",
    "\n",
    "**Step 2:** Repeat until $||w_k - w_{k-1}|| < \\epsilon$:\n",
    "\n",
    "- **Step 2a:** Randomly shuffle the data and split into $m$ batches: $B_1, B_2, \\ldots, B_m$\n",
    "\n",
    "- **Step 2b:** For each batch $j \\in \\{1, 2, \\ldots, m\\}$, apply the gradient descent update using only that batch's data:\n",
    "$$w_k = w_{k-1} - \\eta \\frac{1}{|B_j|} X_{B_j}^T(X_{B_j} w_{k-1} - Y_{B_j})$$\n",
    "\n",
    "- **Step 2c:** After processing all batches, compute and store the loss on the **full** dataset:\n",
    "$$L_k = \\frac{1}{2n}||Y - X w_k||^2$$\n",
    "\n",
    "**Output:** $w_k$, $[L_1, L_2, \\ldots, L_k]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyZvd49jLnb7"
   },
   "source": [
    "Write a function for mini-batch gradient descent that returns $\\widehat{w}$ and the loss history. This function should work with any batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKwrmSI6Lnb7"
   },
   "source": [
    "**Hint:** You can use `np.random.permutation(n)` to get a random permutation of indices, then use these to shuffle both X and y together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oESR-JHLnb7"
   },
   "source": [
    "```python\n",
    "# Shuffle X and y together using the same permutation\n",
    "shuffle_indices = np.random.permutation(num_samples)\n",
    "X_shuffled = X[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Get the j-th minibatch\n",
    "X_batch = X_shuffled[j * batch_size : (j + 1) * batch_size]\n",
    "y_batch = y_shuffled[j * batch_size : (j + 1) * batch_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D87iK0zXLnb7"
   },
   "source": [
    "Note that $X$ and $y$ must **always be shuffled together** (think about why this is necessary!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa9HOECbLnb7"
   },
   "source": [
    "Use a for loop with index $j$ going from 0 to $\\lceil n / |B| \\rceil - 1$ and apply a gradient descent step on the data from the $j$-th batch. If $|B|$ does not divide $n$ evenly, the last batch will be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1743180555404,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "FHZJbX4PLnb7"
   },
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(\n",
    "    features: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    eta: float,\n",
    "    initial_weights: np.ndarray,\n",
    "    epsilon: float,\n",
    "    batch_size: int,\n",
    ") -> tuple[np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Fit multivariate linear regression using mini-batch gradient descent.\n",
    "\n",
    "    Args:\n",
    "        features: input feature matrix (n_samples x n_features)\n",
    "        targets: target values array (n_samples,)\n",
    "        eta: learning rate\n",
    "        initial_weights: initial weight vector (n_features + 1,) including intercept\n",
    "        epsilon: convergence tolerance\n",
    "        batch_size: size of each mini-batch\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (weights, losses) where weights is the fitted weight vector\n",
    "        and losses is the list of loss values at the end of each epoch\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    num_samples = features.shape[0]\n",
    "\n",
    "    # Add column of ones for intercept\n",
    "    ones_column = np.ones((num_samples, 1))\n",
    "    design_matrix = np.hstack((ones_column, features))\n",
    "    target_vector = np.array(targets).reshape((num_samples, 1))\n",
    "\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "\n",
    "    def compute_loss(weights):\n",
    "        residuals = target_vector - design_matrix @ weights\n",
    "        return 0.5 * np.mean(residuals**2)\n",
    "\n",
    "    def compute_batch_gradient(x_batch, y_batch, weights, current_batch_size):\n",
    "        return x_batch.T @ (x_batch @ weights - y_batch) / current_batch_size\n",
    "\n",
    "    losses = []\n",
    "    weights = initial_weights.copy()\n",
    "    weights_prev = np.ones_like(weights) * 100.0\n",
    "\n",
    "    while np.linalg.norm(weights - weights_prev, 1) > epsilon:\n",
    "        weights_prev = weights.copy()\n",
    "\n",
    "        # Shuffle data at the start of each epoch\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "        design_shuffled = design_matrix[shuffle_indices]\n",
    "        target_shuffled = target_vector[shuffle_indices]\n",
    "\n",
    "        # Process each batch\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "            x_batch = design_shuffled[start_idx:end_idx]\n",
    "            y_batch = target_shuffled[start_idx:end_idx]\n",
    "            current_batch_size = end_idx - start_idx\n",
    "\n",
    "            gradient = compute_batch_gradient(x_batch, y_batch, weights, current_batch_size)\n",
    "            weights = weights - eta * gradient\n",
    "\n",
    "        # Record loss after each epoch\n",
    "        losses.append(compute_loss(weights))\n",
    "\n",
    "    return weights, losses\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV4wvuNOLnb7"
   },
   "source": [
    "We can reuse the test setup from the previous question, as we are again solving multivariate linear regression, this time with mini-batch optimization. The same caveats apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "executionInfo": {
     "elapsed": 902,
     "status": "ok",
     "timestamp": 1743180556307,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "v7aahJy9Lnb7",
    "outputId": "21cf1347-ea2b-46f8-8803-41f9806030a0"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "np.random.seed(42)\n",
    "housing_weights_minibatch, housing_loss_history_minibatch = minibatch_gradient_descent(\n",
    "    housing_train_scaled,\n",
    "    housing_train_targets,\n",
    "    eta=0.01,\n",
    "    initial_weights=np.random.randn(housing_num_weights, 1),\n",
    "    epsilon=0.001,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "# Compute predictions on validation set\n",
    "housing_predictions_minibatch = housing_val_with_intercept @ housing_weights_minibatch\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(1, len(housing_loss_history_minibatch) + 1),\n",
    "    housing_loss_history_minibatch,\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs (Mini-batch GD)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {housing_loss_history_minibatch[-1]}\")\n",
    "minibatch_val_loss = 0.5 * np.mean(\n",
    "    (housing_val_targets_reshaped - housing_predictions_minibatch) ** 2\n",
    ")\n",
    "print(f\"Validation loss: {minibatch_val_loss}\")\n",
    "\n",
    "# Verify loss decreases overall\n",
    "assert len(housing_loss_history_minibatch) > 1, \"Should have multiple epochs\"\n",
    "assert (\n",
    "    housing_loss_history_minibatch[-1] < housing_loss_history_minibatch[0]\n",
    "), \"Loss should decrease overall\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check final loss is reasonable (should converge to similar value as full-batch)\n",
    "assert (\n",
    "    housing_loss_history_minibatch[-1] < 0.5\n",
    "), f\"Final loss too high: {housing_loss_history_minibatch[-1]}\"\n",
    "\n",
    "# Test with different batch size\n",
    "np.random.seed(123)\n",
    "housing_weights_small_batch, housing_losses_small_batch = minibatch_gradient_descent(\n",
    "    housing_train_scaled,\n",
    "    housing_train_targets,\n",
    "    eta=0.01,\n",
    "    initial_weights=np.random.randn(housing_num_weights, 1),\n",
    "    epsilon=0.001,\n",
    "    batch_size=128,\n",
    ")\n",
    "assert (\n",
    "    housing_losses_small_batch[-1] < 0.5\n",
    "), f\"Failed with smaller batch size: {housing_losses_small_batch[-1]}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1743180556407,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "m58Xg5MsLnb7",
    "outputId": "56d8cb65-a698-4b1b-8fee-8345a1fbe149"
   },
   "outputs": [],
   "source": [
    "# Compare with sklearn's LinearRegression (same as before)\n",
    "print(f\"Sklearn intercept: {housing_sklearn_model.intercept_}\")\n",
    "print(f\"Sklearn coefficients: {housing_sklearn_model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1743180556418,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "Hlpa5lbaLnb7",
    "outputId": "cef98833-b7dc-4835-b1a7-0ee0c058debc"
   },
   "outputs": [],
   "source": [
    "# Compare losses\n",
    "print(f\"Sklearn training loss: {housing_sklearn_train_loss}\")\n",
    "print(f\"Sklearn validation loss: {housing_sklearn_val_loss}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "17ZK1ZUfq_OuaAVZfTQ22yVLf02XmLaAF",
     "timestamp": 1742741149041
    },
    {
     "file_id": "15JmXyiW2rgvRniG4B4z8Gb8NxgMOD-ht",
     "timestamp": 1738362220103
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
