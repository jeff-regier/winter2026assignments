{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 503, Homework 6: Splines and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment covers **regression splines** (piecewise polynomials joined at knots), **smoothing splines** (which balance fit and smoothness via a penalty term), and **generalized additive models (GAMs)** that extend these ideas to multiple predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Piecewise Cubic Functions (Chapter 7, Exercise 1)\n",
    "\n",
    "We begin by finding functions $f_1$ and $f_2$ so that $f(x)$ can be written in the form\n",
    "\n",
    "$$f(x)=\\begin{cases}\n",
    "f_1(x) & x<\\xi \\\\\n",
    "f_2(x) & x>\\xi \n",
    "\\end{cases}$$\n",
    "\n",
    "#### (a)\n",
    "\n",
    "Find coefficients $a_1, b_1, c_1, d_1$ such that $f_1(x) = a_1 + b_1 x + c_1 x^2 + d_1 x^3$.\n",
    "\n",
    "#### (b)\n",
    "\n",
    "Find coefficients $a_2, b_2, c_2, d_2$ such that $f_2(x) = a_2 + b_2 x + c_2 x^2 + d_2 x^3$.\n",
    "\n",
    "#### (c)\n",
    "\n",
    "Show that $f_1(\\xi) = f_2(\\xi)$ (continuity at the knot).\n",
    "\n",
    "#### (d)\n",
    "\n",
    "Show that $f_1'(\\xi) = f_2'(\\xi)$ (first derivative continuity at the knot).\n",
    "\n",
    "#### (e)\n",
    "\n",
    "Show that $f_1''(\\xi) = f_2''(\\xi)$ (second derivative continuity at the knot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "#### (a)\n",
    "\n",
    "We find that $f_1(x)=\\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$.\n",
    "\n",
    "In particular, we can let\n",
    "\n",
    "* $a_1 = \\beta_0$,\n",
    "* $b_1 = \\beta_1$,\n",
    "* $c_1 = \\beta_2$,\n",
    "* $d_1 = \\beta_3$,\n",
    "\n",
    "and write $f_1(x)=a_1 + b_1 x + c_1 x^2 + d_1 x^3$. \n",
    "\n",
    "#### (b)\n",
    "\n",
    "We calculate that \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f_2(x) &= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 (x - \\xi)^3 \\\\\n",
    "    &= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 (x^3 - 3 \\xi x^2 + 3 \\xi^2 x - \\xi^3) \\\\\n",
    "    &= (\\beta_0 - \\beta_4 \\xi^3) + (\\beta_1 + 3 \\beta_4 \\xi^2) x + (\\beta_2 - 3 \\beta_4 \\xi) x^2 + (\\beta_3 + \\beta_4) x^3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus we can take\n",
    "\n",
    "* $a_2 = \\beta_0 - \\beta_4 \\xi^3$,\n",
    "* $b_2 = \\beta_1 + 3 \\beta_4 \\xi^2$,\n",
    "* $c_2 = \\beta_2 - 3 \\beta_4 \\xi$,  and\n",
    "* $d_2 = \\beta_3 + \\beta_4$.\n",
    "\n",
    "\n",
    "Then $f_2(x)=a_2 + b_2 x + c_2 x^2 + d_2 x^3$.\n",
    "\n",
    "\n",
    "#### (c)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f_2(\\xi) &= \\beta_0 + \\beta_1 \\xi + \\beta_2 \\xi^2 + \\beta_3 \\xi^3 + \\beta_4 (\\xi - \\xi)^3 \\\\\n",
    "    &= \\beta_0 + \\beta_1 \\xi + \\beta_2 \\xi^2 + \\beta_3 \\xi^3 = f_1(\\xi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### (d)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f_1'(\\xi) &= \\beta_1 + 2 \\beta_2 \\xi + 3 \\beta_3 \\xi^2 \\\\\n",
    "    f_2'(\\xi) &= (\\beta_1 + 3 \\beta_4 \\xi^2) + 2(\\beta_2 - 3 \\beta_4 \\xi) \\xi + 3(\\beta_3 + \\beta_4) \\xi^2 \\\\\n",
    "    &= \\beta_1 + 2 \\beta_2 \\xi + 3 \\beta_3 \\xi^2 = f_1'(\\xi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### (e)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f_1''(\\xi) &= 2 \\beta_2 + 6 \\beta_3 \\xi \\\\\n",
    "    f_2''(\\xi) &= 2(\\beta_2 - 3 \\beta_4 \\xi) + 6 (\\beta_3 + \\beta_4) \\xi \\\\\n",
    "    &= 2 \\beta_2 + 6 \\beta_3 \\xi = f_1''(\\xi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Smoothing Spline Behavior (Chapter 7, Exercise 2)\n",
    "\n",
    "Consider the smoothing spline optimization problem:\n",
    "\n",
    "$$\\min_g \\sum_{i=1}^{n}(y_i - g(x_i))^2 + \\lambda \\int g^{(m)}(x)^2 dx$$\n",
    "\n",
    "where $g^{(m)}$ is the $m$-th derivative of $g$. For each of the following scenarios, sketch (or produce in code) what $\\hat{g}$ will look like. Generate some sample data using:\n",
    "\n",
    "```python\n",
    "np.random.seed(1)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = x ** 3 + 4 * x ** 2 + 3 * x + 1 + np.random.randn(100)\n",
    "```\n",
    "\n",
    "#### (a)\n",
    "\n",
    "$\\lambda = \\infty$, $m = 0$\n",
    "\n",
    "#### (b)\n",
    "\n",
    "$\\lambda = \\infty$, $m = 1$\n",
    "\n",
    "#### (c)\n",
    "\n",
    "$\\lambda = \\infty$, $m = 2$\n",
    "\n",
    "#### (d)\n",
    "\n",
    "$\\lambda = \\infty$, $m = 3$\n",
    "\n",
    "#### (e)\n",
    "\n",
    "$\\lambda = 0$, $m = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer, StandardScaler\n",
    "\n",
    "np.random.seed(1)\n",
    "x_vals = np.linspace(-5, 5, 100)\n",
    "y_vals = x_vals**3 + 4 * x_vals**2 + 3 * x_vals + 1 + np.random.randn(100)\n",
    "X_data = x_vals.reshape(-1, 1)\n",
    "\n",
    "# (a) lambda = infinity, m = 0: g(x) = 0 (minimize integral of g^0 = g itself)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_data, y_vals, label=\"Original Data Points\")\n",
    "plt.plot(X_data, np.zeros(X_data.size), label=\"$\\\\hat{g}$\", color=\"red\")\n",
    "plt.title(\"(a) When $\\\\lambda = \\\\infty, m = 0$\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# (b) lambda = infinity, m = 1: g(x) = constant (minimize integral of g')\n",
    "poly_pipeline_b = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=0)),\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "poly_pipeline_b.fit(X_data, y_vals)\n",
    "y_pred_b = poly_pipeline_b.predict(X_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_data, y_vals, label=\"Original Data Points\")\n",
    "plt.plot(X_data, y_pred_b, label=\"$\\\\hat{g}$\", color=\"red\")\n",
    "plt.title(\"(b) When $\\\\lambda = \\\\infty, m = 1$\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# (c) lambda = infinity, m = 2: g(x) = linear (minimize integral of g'')\n",
    "poly_pipeline_c = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=1)),\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "poly_pipeline_c.fit(X_data, y_vals)\n",
    "y_pred_c = poly_pipeline_c.predict(X_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_data, y_vals, label=\"Original Data Points\")\n",
    "plt.plot(X_data, y_pred_c, label=\"$\\\\hat{g}$\", color=\"red\")\n",
    "plt.title(\"(c) When $\\\\lambda = \\\\infty, m = 2$\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# (d) lambda = infinity, m = 3: g(x) = quadratic (minimize integral of g''')\n",
    "poly_pipeline_d = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=2)),\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "poly_pipeline_d.fit(X_data, y_vals)\n",
    "y_pred_d = poly_pipeline_d.predict(X_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_data, y_vals, label=\"Original Data Points\")\n",
    "plt.plot(X_data, y_pred_d, label=\"$\\\\hat{g}$\", color=\"red\")\n",
    "plt.title(\"(d) When $\\\\lambda = \\\\infty, m = 3$\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# (e) lambda = 0, m = 3: g(x) interpolates all data points (no smoothing penalty)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_data, y_vals, label=\"Original Data Points\")\n",
    "plt.plot(X_data, y_vals, label=\"$\\\\hat{g}$\", color=\"red\")\n",
    "plt.title(\"(e) When $\\\\lambda = 0, m = 3$\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify the data was generated correctly\n",
    "assert len(x_vals) == 100, \"Should have 100 data points\"\n",
    "assert len(y_vals) == 100, \"Should have 100 y values\"\n",
    "assert X_data.shape == (100, 1), \"X_data should be reshaped to (100, 1)\"\n",
    "\n",
    "# Verify predictions have the correct shape\n",
    "assert len(y_pred_b) == 100, \"Constant prediction should have 100 values\"\n",
    "assert len(y_pred_c) == 100, \"Linear prediction should have 100 values\"\n",
    "assert len(y_pred_d) == 100, \"Quadratic prediction should have 100 values\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify the constant prediction is approximately the mean\n",
    "assert abs(y_pred_b.mean() - y_vals.mean()) < 0.1, \"Constant should be close to mean\"\n",
    "# Verify linear prediction has some variance\n",
    "assert y_pred_c.std() > 0, \"Linear prediction should vary\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Sketching a Truncated Power Function (Chapter 7, Exercise 3)\n",
    "\n",
    "Given the function $f(x) = 1 + x - 2(x-1)^2_+$ where $(x)_+ = \\max(0, x)$, sketch this curve and identify key features such as:\n",
    "- Intercepts with the axes\n",
    "- Slope in different regions\n",
    "- The location and nature of the knot at $x = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot the truncated power function f(x) = 1 + x - 2(x-1)^2_+\n",
    "x_vals = np.linspace(-2, 2, 1000)\n",
    "y_vals = 1 + x_vals - 2 * ((x_vals - 1) ** 2) * (x_vals >= 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_vals, y_vals, label=\"$f(x) = 1 + x - 2(x-1)^2_+$\", color=\"red\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.axhline(0, color=\"blue\")\n",
    "plt.axvline(0, color=\"blue\")\n",
    "plt.annotate(\n",
    "    \"intersects x-axis at (-1, 0)\",\n",
    "    xy=(-1, 0),\n",
    "    xytext=(-1.8, 0.5),\n",
    "    arrowprops={\"arrowstyle\": \"<|-|>\", \"color\": \"pink\", \"lw\": 3.5, \"ls\": \"--\"},\n",
    ")\n",
    "plt.annotate(\n",
    "    \"crosses y-axis at (0, 1)\",\n",
    "    xy=(0, 1),\n",
    "    xytext=(0.1, 0.5),\n",
    "    arrowprops={\"arrowstyle\": \"<|-|>\", \"color\": \"pink\", \"lw\": 3.5, \"ls\": \"--\"},\n",
    ")\n",
    "plt.annotate(\"slope is 1 for x < 1\", xy=(-0.5, 1))\n",
    "plt.annotate(\"knot at x = 1\", xy=(1, 2), xytext=(1.2, 2.3))\n",
    "plt.legend()\n",
    "plt.title(\"Truncated Power Function with Knot at x = 1\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify key properties of the truncated power function\n",
    "assert len(x_vals) == 1000, \"Should have 1000 points for smooth plotting\"\n",
    "assert len(y_vals) == 1000, \"y_vals should match x_vals length\"\n",
    "\n",
    "# Verify y-intercept: f(0) = 1 + 0 - 0 = 1\n",
    "y_at_zero = 1 + 0 - 2 * max(0, 0 - 1) ** 2\n",
    "assert abs(y_at_zero - 1.0) < 0.01, \"y-intercept should be 1\"\n",
    "\n",
    "# Verify x-intercept: f(-1) = 1 + (-1) - 0 = 0\n",
    "y_at_neg_one = 1 + (-1) - 2 * max(0, -1 - 1) ** 2\n",
    "assert abs(y_at_neg_one - 0.0) < 0.01, \"f(-1) should be 0\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify value at knot: f(1) = 1 + 1 - 0 = 2\n",
    "y_at_knot = 1 + 1 - 2 * max(0, 1 - 1) ** 2\n",
    "assert abs(y_at_knot - 2.0) < 0.01, \"f(1) should be 2\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Sketching Basis Function Expansions (Chapter 7, Exercise 4)\n",
    "\n",
    "Given the model:\n",
    "$$Y = \\beta_0 + \\beta_1 b_1(X) + \\beta_2 b_2(X) + \\varepsilon$$\n",
    "\n",
    "where:\n",
    "- $b_1(X) = I(0 \\le X \\le 2) - (X-1)I(1 \\le X \\le 2)$\n",
    "- $b_2(X) = (X-3)I(3 \\le X \\le 4) + I(4 < X \\le 5)$\n",
    "\n",
    "and $\\beta_0 = 1$, $\\beta_1 = 1$, $\\beta_2 = 3$, sketch the estimated curve and annotate its key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot the basis function expansion\n",
    "x_vals = np.linspace(-2, 6, 800)\n",
    "b_1 = ((x_vals >= 0) & (x_vals <= 2)) - (x_vals - 1) * ((x_vals >= 1) & (x_vals <= 2))\n",
    "b_2 = (x_vals - 3) * ((x_vals >= 3) & (x_vals <= 4)) + ((x_vals > 4) & (x_vals <= 5))\n",
    "y_vals = 1 + b_1 + 3 * b_2\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_vals, y_vals, label=\"Estimated Curve\", color=\"red\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.axhline(0, color=\"blue\")\n",
    "plt.axvline(0, color=\"blue\")\n",
    "plt.annotate(\n",
    "    \"crosses y-axis at (0, 2)\",\n",
    "    xy=(0, 2),\n",
    "    xytext=(0.1, 2.5),\n",
    "    arrowprops={\"arrowstyle\": \"<|-|>\", \"color\": \"pink\", \"lw\": 3.5, \"ls\": \"--\"},\n",
    ")\n",
    "plt.annotate(\"slope = 0\", xy=(-1, 1.2))\n",
    "plt.annotate(\"slope = 0\", xy=(0.1, 1.8))\n",
    "plt.annotate(\"slope = -1\", xy=(0.8, 1.2))\n",
    "plt.annotate(\"slope = 0\", xy=(2, 1.2))\n",
    "plt.annotate(\"slope = 3\", xy=(3.5, 2))\n",
    "plt.annotate(\"slope = 0\", xy=(4.1, 3.8))\n",
    "plt.annotate(\"slope = 0\", xy=(5.1, 1.2))\n",
    "plt.legend()\n",
    "plt.title(\"Basis Function Expansion\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify the data arrays have correct shapes\n",
    "assert len(x_vals) == 800, \"Should have 800 points for smooth plotting\"\n",
    "assert len(y_vals) == 800, \"y_vals should match x_vals length\"\n",
    "assert len(b_1) == 800, \"b_1 should match x_vals length\"\n",
    "assert len(b_2) == 800, \"b_2 should match x_vals length\"\n",
    "\n",
    "# Verify y-intercept: at x=0, b_1=1, b_2=0, so y = 1 + 1 + 0 = 2\n",
    "# Find index closest to x=0\n",
    "idx_zero = np.argmin(np.abs(x_vals - 0))\n",
    "assert abs(y_vals[idx_zero] - 2.0) < 0.1, \"y-intercept should be approximately 2\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify value at x=4.5 (in range where b_2=1): y = 1 + 0 + 3*1 = 4\n",
    "idx_4_5 = np.argmin(np.abs(x_vals - 4.5))\n",
    "assert abs(y_vals[idx_4_5] - 4.0) < 0.1, \"y(4.5) should be approximately 4\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** True or False - Splines and Polynomials\n",
    "\n",
    "For each statement, indicate whether it is True or False and briefly explain your reasoning.\n",
    "\n",
    "(a) A cubic regression spline with 3 knots has 7 degrees of freedom.\n",
    "\n",
    "(b) A natural cubic spline with 3 knots has 7 degrees of freedom.\n",
    "\n",
    "(c) As $\\lambda \\to \\infty$ in a smoothing spline, the fitted curve approaches the OLS regression line.\n",
    "\n",
    "(d) A natural cubic spline is linear beyond the boundary knots because we assume the second derivative equals zero in those regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "(a) **True.** A cubic regression spline with K knots has K + 4 degrees of freedom (4 for the cubic polynomial basis, plus 1 additional basis function for each knot). With 3 knots: 3 + 4 = 7 degrees of freedom.\n",
    "\n",
    "(b) **False.** A natural cubic spline with K knots has K degrees of freedom (the boundary constraints reduce it from K + 4).\n",
    "\n",
    "(c) **True.** As $\\lambda \\to \\infty$, the penalty term dominates, forcing the second derivative to be zero everywhere, which means the function must be linear.\n",
    "\n",
    "(d) **False.** A natural cubic spline is linear beyond the boundary knots because we impose that the second AND third derivatives equal zero at the boundaries (not just the second derivative).\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Comparing GAMs and Linear Regression\n",
    "\n",
    "What is the primary difference between a generalized additive model (GAM) and a standard linear regression model? Choose the best answer:\n",
    "\n",
    "(a) GAMs can only handle binary outcomes\n",
    "\n",
    "(b) GAMs require more training data than linear regression\n",
    "\n",
    "(c) GAMs allow for non-linear relationships between predictors and the response through smooth functions\n",
    "\n",
    "(d) GAMs cannot include interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The correct answer is **(c)**. GAMs allow for non-linear relationships between predictors and the response by replacing the linear terms $\\beta_j X_j$ with smooth functions $f_j(X_j)$. This is the defining characteristic that distinguishes GAMs from standard linear regression.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Model Comparison on Boston Housing Data\n",
    "\n",
    "Using the Boston housing dataset (provided in `boston.csv`), predict nitrogen oxide concentration (`nox`) from the proportion of non-retail business acres (`indus`).\n",
    "\n",
    "#### (a)\n",
    "\n",
    "Compare three modeling approaches using 4-fold cross-validation:\n",
    "1. A cubic spline with knots at 0, 12, 17, and 30\n",
    "2. A polynomial regression of degree 12\n",
    "3. K-nearest neighbors with k=13\n",
    "\n",
    "Which method achieves the lowest mean squared error? Plot the fitted curves for all three methods and discuss their relative merits beyond just the MSE.\n",
    "\n",
    "#### (b)\n",
    "\n",
    "Fit a GAM using `pygam` to predict `nox` from three predictors: `dis` (distance to employment centers), `indus`, and `rad` (accessibility to radial highways). Use grid search to find optimal smoothing parameters. Plot the partial dependence functions and discuss what they reveal about the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Part (a): Compare three modeling approaches\n",
    "\n",
    "# Load the Boston housing data\n",
    "boston_data = pd.read_csv(\"./data/boston.csv\")\n",
    "\n",
    "features = boston_data[\"indus\"].values.reshape(-1, 1)\n",
    "target = boston_data[\"nox\"].values.reshape(-1)\n",
    "\n",
    "# Set up cross-validation\n",
    "cv_splitter = KFold(n_splits=4, random_state=3, shuffle=True)\n",
    "\n",
    "# Define the three models\n",
    "spline_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"spline\", SplineTransformer(degree=3, knots=np.array([0, 12, 17, 30]).reshape(-1, 1))),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "poly_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=12)),\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=13)\n",
    "\n",
    "# Evaluate each model\n",
    "spline_mse = -cross_val_score(\n",
    "    spline_pipeline, features, target, scoring=\"neg_mean_squared_error\", cv=cv_splitter\n",
    ")\n",
    "print(f\"Spline MSE: {spline_mse.mean():.4f}\")\n",
    "\n",
    "poly_mse = -cross_val_score(\n",
    "    poly_pipeline, features, target, scoring=\"neg_mean_squared_error\", cv=cv_splitter\n",
    ")\n",
    "print(f\"Polynomial MSE: {poly_mse.mean():.4f}\")\n",
    "\n",
    "knn_mse = -cross_val_score(\n",
    "    knn_model, features, target, scoring=\"neg_mean_squared_error\", cv=cv_splitter\n",
    ")\n",
    "print(f\"KNN MSE: {knn_mse.mean():.4f}\")\n",
    "\n",
    "# Fit models and plot\n",
    "spline_pipeline.fit(features, target)\n",
    "poly_pipeline.fit(features, target)\n",
    "knn_model.fit(features, target)\n",
    "\n",
    "x_plot = np.linspace(features.min(), features.max(), 1000).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(features, target, facecolor=\"gray\", label=\"Data\", alpha=0.3)\n",
    "plt.plot(x_plot, spline_pipeline.predict(x_plot), label=\"Spline\", color=\"dodgerblue\")\n",
    "plt.plot(x_plot, poly_pipeline.predict(x_plot), label=\"Polynomial\", color=\"green\")\n",
    "plt.plot(x_plot, knn_model.predict(x_plot), label=\"KNN\", color=\"purple\")\n",
    "plt.xlabel(\"indus\")\n",
    "plt.ylabel(\"nox\")\n",
    "plt.title(\"Predicted NOx Concentration vs Industrial Proportion\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "Discussion: The polynomial achieves the lowest MSE, but the spline provides a smoother,\n",
    "more interpretable fit. The KNN curve is rough and difficult to interpret. The polynomial\n",
    "shows concerning behavior at the endpoints, suggesting potential overfitting.\n",
    "\"\"\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Part (b): Fit a GAM with multiple predictors\n",
    "features_multi = boston_data[[\"dis\", \"indus\", \"rad\"]].values\n",
    "target = boston_data[\"nox\"].values.reshape(-1)\n",
    "\n",
    "# Fit GAM with grid search for optimal lambda values\n",
    "gam = LinearGAM(s(0) + s(1) + s(2))\n",
    "lambda_values = [0.5, 1, 5, 10]\n",
    "lambda_grid = [lambda_values] * 3\n",
    "gam.gridsearch(features_multi, target, lam=lambda_grid)\n",
    "print(f\"Optimal lambda values: {gam.lam}\")\n",
    "\n",
    "# Plot partial dependence functions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "feature_names = [\"dis\", \"indus\", \"rad\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    xx = gam.generate_X_grid(term=i)\n",
    "    ax.plot(xx[:, i], gam.partial_dependence(term=i, X=xx))\n",
    "    ax.plot(\n",
    "        xx[:, i],\n",
    "        gam.partial_dependence(term=i, X=xx, width=0.95)[1],\n",
    "        c=\"r\",\n",
    "        ls=\"--\",\n",
    "        label=\"95% CI\",\n",
    "    )\n",
    "    ax.set_ylim(-0.2, 0.2)\n",
    "    ax.set_xlabel(feature_names[i])\n",
    "    ax.set_ylabel(\"Partial Dependence\")\n",
    "    ax.set_title(f\"Effect of {feature_names[i]} on nox\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "Discussion: The partial dependence plots reveal:\n",
    "- dis: NOx decreases as distance from employment centers increases (expected)\n",
    "- indus: NOx increases with industrial proportion, leveling off at high values\n",
    "- rad: The relationship is unclear with wide confidence intervals, suggesting\n",
    "  insufficient data to reliably estimate this effect. The wiggly pattern likely\n",
    "  reflects noise rather than a true relationship.\n",
    "\"\"\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify the data was loaded correctly\n",
    "assert boston_data is not None, \"boston_data should be loaded\"\n",
    "assert \"nox\" in boston_data.columns, \"boston_data should have nox column\"\n",
    "assert \"indus\" in boston_data.columns, \"boston_data should have indus column\"\n",
    "\n",
    "# Verify models were fitted\n",
    "assert spline_pipeline is not None, \"spline_pipeline should be defined\"\n",
    "assert poly_pipeline is not None, \"poly_pipeline should be defined\"\n",
    "assert knn_model is not None, \"knn_model should be defined\"\n",
    "\n",
    "# Verify MSE values are reasonable (should be small positive numbers)\n",
    "assert 0 < spline_mse.mean() < 1, \"Spline MSE should be small positive number\"\n",
    "assert 0 < poly_mse.mean() < 1, \"Polynomial MSE should be small positive number\"\n",
    "assert 0 < knn_mse.mean() < 1, \"KNN MSE should be small positive number\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify GAM was fitted successfully\n",
    "assert gam is not None, \"GAM model should be defined\"\n",
    "assert len(gam.lam) == 3, \"GAM should have 3 smoothing parameters\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
