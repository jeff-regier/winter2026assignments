{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "KIZDykK5hj3Z"
   },
   "source": [
    "# DATASCI 503, Group Work 8: Support Vector Machines\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. During lab, feel free to flag down your GSI to ask questions at any point!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "cEfQaLyHjdp6"
   },
   "source": [
    "## Overview of SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "ympnVjMjhj3c"
   },
   "source": [
    "In this section, we will experiment with SVMs and how to optimize SVM parameters using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "_GlcPOS1hj3c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "FAaWjEuzhj3d"
   },
   "source": [
    "We will again use our old friend: the iris dataset.\n",
    "\n",
    "This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are not linearly separable from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "_uz-OB1Qhj3d"
   },
   "outputs": [],
   "source": [
    "# Loading a new sample of data for the next example\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "6S2VBdA6hj3d"
   },
   "source": [
    "Instead of directly running SVC (Support Vector Classification) method, we define the parameter space and ask the model to find the optimal parameter in the defined space using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "9OnamInIhj3d"
   },
   "outputs": [],
   "source": [
    "# Defining the parameter space to do the search\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": np.logspace(-3, 3, 25),  # from .001 to 1000\n",
    "    \"gamma\": np.logspace(-3, 3, 25),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "executionInfo": {
     "elapsed": 8640,
     "status": "ok",
     "timestamp": 1741526997195,
     "user": {
      "displayName": "Jeffrey Regier",
      "userId": "17304790169060975378"
     },
     "user_tz": 240
    },
    "id": "4Emn60_Whj3e",
    "outputId": "abb55c51-0e31-4aa1-fdb7-9760ed4da97c"
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel=\"rbf\")\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring=\"accuracy\")  # 1-misclassification rate\n",
    "grid_search.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "xbylcX88hj3e"
   },
   "source": [
    "By using the following code, we can see the parameters leading to best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1741526997196,
     "user": {
      "displayName": "Jeffrey Regier",
      "userId": "17304790169060975378"
     },
     "user_tz": 240
    },
    "id": "iafO6SH1hj3e",
    "outputId": "44c62784-2d59-4386-afa5-f499de77a5da"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "H7oS9TQwhj3e"
   },
   "source": [
    "Note that there could be two different hyperparmeter choices that both lead to the highest cross-validated accuracy.  The hyperparamers listed above indicate one possible choice that leads to highest cross-validated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "2Ean-T4Ehj3e"
   },
   "source": [
    "For a more comprehensive look, we can inspect `grid_search.cv_results_`.  This contains a dictionary of all the evaluation metrics from the gridsearch, usually we use a pd dataframe to visualize it properly.\n",
    "\n",
    "I find a good explanation different columns and their meanings [here](https://stackoverflow.com/questions/54608088/what-is-gridsearch-cv-results-could-any-explain-all-the-things-in-that-i-e-me)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741526997197,
     "user": {
      "displayName": "Jeffrey Regier",
      "userId": "17304790169060975378"
     },
     "user_tz": 240
    },
    "id": "siSw7dQ3hj3f",
    "outputId": "e025b57f-4921-4dac-f18d-d58af870b390"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "QIU0u6oahj3f"
   },
   "source": [
    "For a variety of choices of gamma, let's look at how cross-validated performance varies with $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1741526998056,
     "user": {
      "displayName": "Jeffrey Regier",
      "userId": "17304790169060975378"
     },
     "user_tz": 240
    },
    "id": "-SpyfUt_hj3f",
    "outputId": "87db8a27-11b2-4541-cc8d-4a7a4ddbdfa3"
   },
   "outputs": [],
   "source": [
    "# choose a few values of gamma,\n",
    "gamchoices = [param_grid[\"gamma\"][5], param_grid[\"gamma\"][8], param_grid[\"gamma\"][10]]\n",
    "\n",
    "for gam in gamchoices:\n",
    "    subresults = results.loc[results[\"param_gamma\"] == gam]\n",
    "    plt.plot(subresults.param_C, subresults.mean_test_score, label=f\"$\\\\gamma={gam}$\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.xlabel(\"C\")\n",
    "plt.legend(bbox_to_anchor=[1, 1])\n",
    "plt.gca().set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "GsyhChj7hj3f"
   },
   "source": [
    "Note that $\\gamma=0.017$ and $\\gamma=0.1$ both achieved the maximum cross-validated accuracy (98.67%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "RoV-dFQPhj3f"
   },
   "source": [
    "This lab is adapted from [this github file](https://github.com/jpcolino/IPython_notebooks/blob/master/Cross-Validation%20in%20SVM.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "1KJVN5GqjidP"
   },
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "zhm6jBK_jpHk"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "g8PprCEQj116"
   },
   "source": [
    "Using `make_blobs` generate an approximately linearly separable dataset. You will need to have 100 total samples and 2 binary classes and 2 features, the centers will need to be within $(-4,4) \\times (-4, 4)\\subset \\mathbb{R}^2$. Use a cluster standard deviation of 1.5 and random state 503.\n",
    "\n",
    "Store the features in `X_blobs` and labels in `y_blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a synthetic dataset for linear SVMs using make_blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "X_blobs, y_blobs = make_blobs(\n",
    "    n_samples=100, centers=2, center_box=(-4, 4), random_state=503, cluster_std=1.5\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert X_blobs.shape == (100, 2), f\"Expected shape (100, 2), got {X_blobs.shape}\"\n",
    "assert y_blobs.shape == (100,), f\"Expected shape (100,), got {y_blobs.shape}\"\n",
    "assert set(y_blobs) == {0, 1}, f\"Expected 2 classes (0 and 1), got {set(y_blobs)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert X_blobs.min() > -10 and X_blobs.max() < 10, \"Centers should be within reasonable bounds\"\n",
    "assert (y_blobs == 0).sum() == 50, \"Expected 50 samples in class 0\"\n",
    "assert (y_blobs == 1).sum() == 50, \"Expected 50 samples in class 1\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "JMq0SmD5j2aW"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Visualize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "g5Wj59Lyj4ih"
   },
   "source": [
    "Visualize your dataset by scattering `X_blobs` features and coloring them according to `y_blobs` labels. The zero class should be orange and the positive class skyblue. Use black contours to the dots and a transparency of 0.75. Add axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize X_blobs, y_blobs, colors should be orange and skyblue\n",
    "# BEGIN SOLUTION\n",
    "colors_blobs = [\"orange\" if label == 0 else \"skyblue\" for label in y_blobs]\n",
    "# scatter points\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=colors_blobs, alpha=0.75, edgecolors=\"black\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"colors_blobs\" in dir(), \"colors_blobs variable should be defined\"\n",
    "assert len(colors_blobs) == len(y_blobs), \"colors_blobs should have same length as y_blobs\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert colors_blobs[0] in [\"orange\", \"skyblue\"], \"colors should be orange or skyblue\"\n",
    "# Verify color mapping is correct\n",
    "for i in range(len(y_blobs)):\n",
    "    expected_color = \"orange\" if y_blobs[i] == 0 else \"skyblue\"\n",
    "    assert colors_blobs[i] == expected_color, f\"Color at index {i} should be {expected_color}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "DUCtJzzIoGrG"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Train a Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "i6Uv2LKqqs02"
   },
   "source": [
    "Split the blobs data into training and test (80-20 split). Train a linear SVM and report the test accuracy. Store your model in `svm_blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "X_blobs_train, X_blobs_test, y_blobs_train, y_blobs_test = train_test_split(\n",
    "    X_blobs, y_blobs, test_size=0.2, random_state=503\n",
    ")\n",
    "\n",
    "svm_blobs = SVC(kernel=\"linear\")\n",
    "svm_blobs.fit(X_blobs_train, y_blobs_train)\n",
    "\n",
    "accuracy_blobs = svm_blobs.score(X_blobs_test, y_blobs_test)\n",
    "print(f\"Test accuracy: {accuracy_blobs}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert X_blobs_train.shape[0] == 80, f\"Expected 80 training samples, got {X_blobs_train.shape[0]}\"\n",
    "assert X_blobs_test.shape[0] == 20, f\"Expected 20 test samples, got {X_blobs_test.shape[0]}\"\n",
    "assert hasattr(svm_blobs, \"coef_\"), \"SVM should be fitted and have coef_ attribute\"\n",
    "assert 0 <= accuracy_blobs <= 1, \"Accuracy should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert svm_blobs.kernel == \"linear\", \"Should use linear kernel\"\n",
    "assert accuracy_blobs >= 0.9, \"Accuracy should be at least 0.9 for this dataset\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "2ykERFw-oGD7"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Plot Decision Boundary and Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "OPv1qm0gtSpJ"
   },
   "source": [
    "Extract the weight vector and bias from `svm_blobs`. Then plot the decision boundary\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0\n",
    "$$\n",
    "as a black dashed line. Also plot the margins\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = \\pm 1\n",
    "$$\n",
    "as black dashed lines with dots `-.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "w_blobs = svm_blobs.coef_[0]\n",
    "b_blobs = svm_blobs.intercept_[0]\n",
    "\n",
    "support_vectors_blobs = svm_blobs.support_vectors_\n",
    "\n",
    "x_min, x_max = X_blobs[:, 0].min() - 1, X_blobs[:, 0].max() + 1\n",
    "y_min, y_max = X_blobs[:, 1].min() - 1, X_blobs[:, 1].max() + 1\n",
    "\n",
    "xx = np.linspace(x_min, x_max, 100)\n",
    "yy_boundary = -(w_blobs[0] * xx + b_blobs) / w_blobs[1]\n",
    "yy_upper = -(w_blobs[0] * xx + b_blobs + 1) / w_blobs[1]\n",
    "yy_lower = -(w_blobs[0] * xx + b_blobs - 1) / w_blobs[1]\n",
    "\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=colors_blobs, alpha=0.75, edgecolors=\"black\")\n",
    "plt.scatter(\n",
    "    support_vectors_blobs[:, 0],\n",
    "    support_vectors_blobs[:, 1],\n",
    "    s=100,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"black\",\n",
    ")\n",
    "plt.plot(xx, yy_boundary, \"k--\")\n",
    "plt.plot(xx, yy_upper, \"k-.\")\n",
    "plt.plot(xx, yy_lower, \"k-.\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert w_blobs.shape == (2,), f\"Weight vector should have 2 components, got {w_blobs.shape}\"\n",
    "assert isinstance(b_blobs, float), \"Bias should be a scalar\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(support_vectors_blobs) >= 2, \"Should have at least 2 support vectors\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "nFvnQlMdumQv"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Count Observations Within Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "bZKhzcwL4PGt"
   },
   "source": [
    "Compute how many observations in `X_blobs` are within the margins of `svm_blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute how many observations are within the margins\n",
    "# BEGIN SOLUTION\n",
    "obs_within_margins = (np.abs(svm_blobs.decision_function(X_blobs)) <= 1).sum()\n",
    "print(f\"Number of observations within the margins: {obs_within_margins}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(obs_within_margins, int | np.integer), \"Should be an integer\"\n",
    "assert obs_within_margins >= 0, \"Count should be non-negative\"\n",
    "assert obs_within_margins <= len(X_blobs), \"Count should not exceed total observations\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    obs_within_margins == 11\n",
    "), f\"Expected 11 observations within margins, got {obs_within_margins}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "dk-HEQtXXb-u"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Generate Non-Linearly Separable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "GC9EVeMbYZbN"
   },
   "source": [
    "Use `make_circles` to generate a non-linearly separable dataset. Your sample should have 100 datapoints, noise level of 0.15, a scale factor of 0.25 and random state 503.\n",
    "\n",
    "Store the features in `X_circles` and labels in `y_circles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "X_circles, y_circles = make_circles(n_samples=100, noise=0.15, factor=0.25, random_state=503)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert X_circles.shape == (100, 2), f\"Expected shape (100, 2), got {X_circles.shape}\"\n",
    "assert y_circles.shape == (100,), f\"Expected shape (100,), got {y_circles.shape}\"\n",
    "assert set(y_circles) == {0, 1}, f\"Expected 2 classes (0 and 1), got {set(y_circles)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Data should roughly form concentric circles\n",
    "inner_points = X_circles[y_circles == 1]\n",
    "outer_points = X_circles[y_circles == 0]\n",
    "inner_dist = np.sqrt((inner_points**2).sum(axis=1)).mean()\n",
    "outer_dist = np.sqrt((outer_points**2).sum(axis=1)).mean()\n",
    "assert inner_dist < outer_dist, \"Inner circle should have smaller radius\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "J1n4hLGoZZ_5"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Linear SVM on Circles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "RtRFjRr-ZsyB"
   },
   "source": [
    "Split the circles data with a 80-20 train test split. Train a linear SVM (store it as `svm_circles`). Plot the resulting linear boundary and margins. Scatter the points with orange for class 0 and skyblue for class 1. Then add a box to the bottom left of the plot with the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data and train linear SVM\n",
    "# BEGIN SOLUTION\n",
    "X_circles_train, X_circles_test, y_circles_train, y_circles_test = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.2, random_state=503\n",
    ")\n",
    "\n",
    "svm_circles = SVC(kernel=\"linear\")\n",
    "svm_circles.fit(X_circles_train, y_circles_train)\n",
    "\n",
    "test_accuracy_circles = svm_circles.score(X_circles_test, y_circles_test)\n",
    "\n",
    "w_circles = svm_circles.coef_[0]\n",
    "b_circles = svm_circles.intercept_[0]\n",
    "support_vectors_circles = svm_circles.support_vectors_\n",
    "\n",
    "x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5\n",
    "y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5\n",
    "\n",
    "xx = np.linspace(x_min, x_max, 100)\n",
    "yy_boundary = -(w_circles[0] * xx + b_circles) / w_circles[1]\n",
    "yy_upper = -(w_circles[0] * xx + b_circles + 1) / w_circles[1]\n",
    "yy_lower = -(w_circles[0] * xx + b_circles - 1) / w_circles[1]\n",
    "\n",
    "# visualize X_circles, y_circles, colors should be orange and skyblue\n",
    "colors_circles = [\"orange\" if label == 0 else \"skyblue\" for label in y_circles]\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=colors_circles, alpha=0.75, edgecolors=\"black\")\n",
    "plt.scatter(\n",
    "    support_vectors_circles[:, 0],\n",
    "    support_vectors_circles[:, 1],\n",
    "    s=100,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"black\",\n",
    ")\n",
    "plt.plot(xx, yy_boundary, \"k--\")\n",
    "plt.plot(xx, yy_upper, \"k-.\")\n",
    "plt.plot(xx, yy_lower, \"k-.\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.text(0.05, 0.05, f\"Test accuracy: {test_accuracy_circles:.2f}\", transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert (\n",
    "    X_circles_train.shape[0] == 80\n",
    "), f\"Expected 80 training samples, got {X_circles_train.shape[0]}\"\n",
    "assert hasattr(svm_circles, \"coef_\"), \"SVM should be fitted\"\n",
    "assert 0 <= test_accuracy_circles <= 1, \"Accuracy should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Linear SVM should perform poorly on circles data\n",
    "assert test_accuracy_circles < 0.8, \"Linear SVM should perform poorly on non-linear data\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "zAvRrxzoavcF"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 8:** Kernel SVM and Quadratic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "1FKxyKzza35f"
   },
   "source": [
    "Fit a Kernel SVM with polynomial kernel of degree 2. Then fill in the quadratic features function. It should transform each row $(x_1, x_2)$ of our data matrix $X$ into\n",
    "$$\n",
    "\\phi(x_1, x_2) = (1, \\sqrt{2} x_1, \\sqrt{2} x_2, x_1^2, \\sqrt{2} x_1 x_2, x_2^2)$$\n",
    "Then use this function to fit a linear svm on this feature space. Print both methods accuracies. They should match!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_features(features: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate quadratic features.\n",
    "\n",
    "    Transforms each row (x1, x2) into:\n",
    "    (1, sqrt(2)*x1, sqrt(2)*x2, x1^2, sqrt(2)*x1*x2, x2^2)\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    num_samples = features.shape[0]\n",
    "    features_new = np.zeros((num_samples, 6))\n",
    "    features_new[:, 0] = 1\n",
    "    features_new[:, 1] = np.sqrt(2) * features[:, 0]\n",
    "    features_new[:, 2] = np.sqrt(2) * features[:, 1]\n",
    "    features_new[:, 3] = features[:, 0] ** 2\n",
    "    features_new[:, 4] = np.sqrt(2) * features[:, 0] * features[:, 1]\n",
    "    features_new[:, 5] = features[:, 1] ** 2\n",
    "    return features_new\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_input = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "test_output = quadratic_features(test_input)\n",
    "assert test_output.shape == (2, 6), f\"Expected shape (2, 6), got {test_output.shape}\"\n",
    "assert test_output[0, 0] == 1.0, \"First column should be 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "expected_row0 = np.array([1, np.sqrt(2), 2 * np.sqrt(2), 1, 2 * np.sqrt(2), 4])\n",
    "assert np.allclose(test_output[0], expected_row0), \"Quadratic features computation incorrect\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit kernel SVM and linear SVM on quadratic features\n",
    "# BEGIN SOLUTION\n",
    "poly_svm = SVC(kernel=\"poly\", degree=2, gamma=1)\n",
    "poly_svm.fit(X_circles_train, y_circles_train)\n",
    "\n",
    "X_circles_train_quad = quadratic_features(X_circles_train)\n",
    "X_circles_test_quad = quadratic_features(X_circles_test)\n",
    "\n",
    "lin_feature_svm = SVC(kernel=\"linear\")\n",
    "lin_feature_svm.fit(X_circles_train_quad, y_circles_train)\n",
    "\n",
    "poly_accuracy = poly_svm.score(X_circles_test, y_circles_test)\n",
    "lin_feature_accuracy = lin_feature_svm.score(X_circles_test_quad, y_circles_test)\n",
    "\n",
    "print(f\"Polynomial SVM accuracy: {poly_accuracy}\")\n",
    "print(f\"Linear SVM with quadratic features accuracy: {lin_feature_accuracy}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert hasattr(poly_svm, \"support_vectors_\"), \"poly_svm should be fitted\"\n",
    "assert hasattr(lin_feature_svm, \"coef_\"), \"lin_feature_svm should be fitted\"\n",
    "assert 0 <= poly_accuracy <= 1, \"poly_accuracy should be between 0 and 1\"\n",
    "assert 0 <= lin_feature_accuracy <= 1, \"lin_feature_accuracy should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert np.isclose(poly_accuracy, lin_feature_accuracy), \"Both methods should give same accuracy\"\n",
    "assert poly_accuracy >= 0.9, \"Accuracy should be high for kernel SVM\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "TQ2cZ0vwe1bG"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 9:** Compare Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "XFp6JCks7HOI"
   },
   "source": [
    "Plot the decision boundary for both methods. Here is an outline:\n",
    "\n",
    "\n",
    "1.   create a meshgrid\n",
    "2.   evaluate $w^\\top\\phi(x) + b$ on the meshgrid using `svc.decision_function` for both methods. Note that you have to pass the grid through `quadratic_features` for the second method.\n",
    "3.   scatter data points\n",
    "4.   use `plt.contour` to get the decision boundary for the 2 methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for both methods\n",
    "# BEGIN SOLUTION\n",
    "# 1. Create a mesh (grid) of points covering the region of X_circles\n",
    "x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5\n",
    "y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# 2. Evaluate the decision function for poly_svm on this grid\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z_poly = poly_svm.decision_function(grid_points)\n",
    "\n",
    "# 3. Evaluate the decision function for lin_feature_svm on transformed grid\n",
    "Z_feature = lin_feature_svm.decision_function(quadratic_features(grid_points))\n",
    "\n",
    "# Reshape results to match the shape of xx (for contour plotting)\n",
    "Z_poly = Z_poly.reshape(xx.shape)\n",
    "Z_feature = Z_feature.reshape(xx.shape)\n",
    "\n",
    "# 4. Scatter the original data points\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=colors_circles, alpha=0.75, edgecolors=\"black\")\n",
    "\n",
    "# 5. Plot the zero-level contour (decision boundary) for each model\n",
    "CS1 = plt.contour(xx, yy, Z_poly, levels=[0], colors=\"red\", linestyles=\"--\")\n",
    "CS2 = plt.contour(xx, yy, Z_feature, levels=[0], colors=\"green\", linestyles=\"-.\")\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Comparison of Decision Boundaries: Kernel vs. Manual Feature Mapping\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert Z_poly.shape == xx.shape, \"Z_poly should match meshgrid shape\"\n",
    "assert Z_feature.shape == xx.shape, \"Z_feature should match meshgrid shape\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Decision boundaries should be similar (both are quadratic)\n",
    "# Boundaries should have similar sign patterns (both classify similarly)\n",
    "assert (np.sign(Z_poly) == np.sign(Z_feature)).mean() > 0.9, \"Boundaries should be similar\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
