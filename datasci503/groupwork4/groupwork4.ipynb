{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTxDJGovJweS"
   },
   "source": [
    "# DATASCI 503, Group Work 4: Linear and Quadratic Discriminant Analysis\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!** Upon completion, one member of the team should submit their team's work through Canvas as HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3622,
     "status": "ok",
     "timestamp": 1739555888029,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "oG0aB_0WWVrG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis as LDA,\n",
    ")\n",
    "from sklearn.discriminant_analysis import (\n",
    "    QuadraticDiscriminantAnalysis as QDA,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErYxCtxns4w1"
   },
   "source": [
    "## Bayes Classifier\n",
    "\n",
    "The **Bayes Classifier** is the theoretical optimal classifier that minimizes the probability of misclassification. It assigns a new observation $ X $ to the class $ k $ that has the highest posterior probability:\n",
    "\n",
    "$$\n",
    "P(Y = k \\mid X = x) = \\frac{P(X = x \\mid Y = k) P(Y = k)}{P(X = x)}\n",
    "$$\n",
    "\n",
    "By Bayes' Theorem, the **Bayes Classifier** assigns $ X $ to the class:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\arg\\max_k P(Y = k \\mid X = x)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ P(Y = k) $ is the prior probability of class $ k $,\n",
    "- $ P(X = x \\mid Y = k) $ is the class-conditional density,\n",
    "- $ P(X = x) $ is the marginal density of $ X $.\n",
    "\n",
    "If we know the true class-conditional distributions, the Bayes Classifier is optimal. However, in practice, these distributions are unknown, and we approximate them using models like **LDA** and **QDA**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Xi5Ao2hNX7_"
   },
   "source": [
    "### On Bayes Classifier Error\n",
    "\n",
    "Even though the Bayes Classifier is optimal, it is not perfect unless the classes are completely separable. The Bayes Error Rate is the probability that the classifier makes a mistake.\n",
    "\n",
    "This error arises because of overlapping class distributions. When two classes have multivariate normal likelihoods with mean vectors $\\mu_0$  and  $\\mu_1$  and covariances $\\Sigma_0, \\Sigma_1$, the posterior probabilities are computed using Bayes' Theorem and the Bayes decision boundary is given by the set of points where:\n",
    "\n",
    "$$\n",
    "P(Y = 0 \\mid X = x) = P(Y = 1 \\mid X = x)\n",
    "$$\n",
    "\n",
    "which, in the case of multivariate normal likelihoods, leads to the quadratic equation:\n",
    "\n",
    "$$\n",
    "(x - \\mu_0)^T \\Sigma_0^{-1} (x - \\mu_0) - 2 \\log P(Y = 0) = (x - \\mu_1)^T \\Sigma_1^{-1} (x - \\mu_1) - 2 \\log P(Y = 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1739555888943,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "LFq9Vg9WxPK3",
    "outputId": "e654df86-1747-4dd8-c0d1-121db1ae3f1d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for Class 0\n",
    "mu_0 = np.array([2, 2])\n",
    "sigma_0 = np.array([[1, 0.5], [0.5, 1]])\n",
    "\n",
    "# Parameters for Class 1\n",
    "mu_1 = np.array([5, 5])\n",
    "sigma_1 = np.array([[1, -0.3], [-0.3, 1]])\n",
    "\n",
    "# Generate data\n",
    "n_samples = 400  # Total number of samples\n",
    "prior_0 = 0.6  # Prior probability for class 0\n",
    "prior_1 = 1 - prior_0  # Prior probability for class 1\n",
    "# Sample class labels according to the prior\n",
    "assignment = np.random.choice([0, 1], size=n_samples, p=[prior_0, prior_1])\n",
    "# Count samples per class\n",
    "n_samples_0 = np.sum(assignment == 0)\n",
    "n_samples_1 = np.sum(assignment == 1)\n",
    "X0 = np.random.multivariate_normal(mu_0, sigma_0, n_samples_0)\n",
    "X1 = np.random.multivariate_normal(mu_1, sigma_1, n_samples_1)\n",
    "\n",
    "# set up figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot data\n",
    "plt.scatter(X0[:, 0], X0[:, 1], label=\"Class 0\", alpha=0.5)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], label=\"Class 1\", alpha=0.5)\n",
    "# plot density contours\n",
    "x, y = np.linspace(-3, 10, 100), np.linspace(-3, 10, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "density0 = multivariate_normal(mu_0, sigma_0).pdf(pos)\n",
    "density1 = multivariate_normal(mu_1, sigma_1).pdf(pos)\n",
    "plt.contour(X, Y, density0, levels=[0.01, 0.05, 0.1], colors=\"r\", linestyles=\"dashed\")\n",
    "plt.contour(X, Y, density1, levels=[0.01, 0.05, 0.1], colors=\"b\", linestyles=\"dashed\")\n",
    "# plot means as crosses\n",
    "plt.scatter(mu_0[0], mu_0[1], marker=\"x\", color=\"r\", s=100, label=\"Mean Class 0\")\n",
    "plt.scatter(mu_1[0], mu_1[1], marker=\"x\", color=\"b\", s=100, label=\"Mean Class 1\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Simulated Data with Class Means\")\n",
    "plt.legend()\n",
    "# set axis to [-3,8] and [-2,8]\n",
    "plt.xlim([-1, 8])\n",
    "plt.ylim([-1, 8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40rg1vf5T856"
   },
   "source": [
    "To illustrate how the error changes as a function of $x$, we compute the Bayes error rate along the segment connecting  $\\mu_0$  and  $\\mu_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1739555889534,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "G7ZzwOE3M44C",
    "outputId": "c6d08635-9490-44be-b258-e608a8a1cad2"
   },
   "outputs": [],
   "source": [
    "# Generate points along the line connecting mu_0 and mu_1\n",
    "t_values = np.linspace(0, 1, 100)\n",
    "points = np.outer(1 - t_values, mu_0) + np.outer(t_values, mu_1)\n",
    "\n",
    "# Compute class conditional densities with different covariances\n",
    "pdf_0 = multivariate_normal.pdf(points, mean=mu_0, cov=sigma_0)\n",
    "pdf_1 = multivariate_normal.pdf(points, mean=mu_1, cov=sigma_1)\n",
    "\n",
    "# Compute posterior probabilities (assuming equal priors)\n",
    "posterior_0 = (pdf_0 * prior_0) / (pdf_0 * prior_0 + pdf_1 * prior_1)\n",
    "posterior_1 = (pdf_1 * prior_1) / (pdf_0 * prior_0 + pdf_1 * prior_1)\n",
    "\n",
    "# Compute Bayes error rate at each point\n",
    "bayes_error = 1 - np.maximum(posterior_0, posterior_1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(t_values, bayes_error, label=\"Bayes Error Rate\", color=\"blue\", linewidth=2)\n",
    "plt.xlabel(r\"Position along segment from $\\mu_0$ to $\\mu_1$\")\n",
    "plt.ylabel(\"Bayes Error Rate\")\n",
    "plt.title(\"Bayes Error Rate with Different Covariance Matrices\")\n",
    "plt.legend()\n",
    "plt.grid(visible=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cx9SM0rTo6u"
   },
   "source": [
    "Unfortunately we do not know the prior and likelihood to perform this calculations and achieve the Bayes optimal classifier. We thus have to make assumptions on the likelihoods and priors and further estimates their parameters to approximate the Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXooZp0eRM2D"
   },
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "LDA assumes that the different classes generate data based on Gaussian distributions with means that are distinct but share the same covariance matrix. This assumption allows LDA to find a linear combination of features that characterizes or separates two or more classes of objects or events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQZNKGzqqgxW"
   },
   "source": [
    "### Assumptions\n",
    "\n",
    "- Data is drawn from **$k$** multivariate normal distributions, where each one of these distributions can have a different mean vector $\\mu_k$, but all share the same covariance structure $\\Sigma$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739555889534,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "Qd9aW6p0VG8L",
    "outputId": "1fe23111-944f-48a6-fbea-d142ee46d531"
   },
   "outputs": [],
   "source": [
    "# Step 1: Generate cluster centers\n",
    "centers = [[-1, -4], [0, 0], [3, 4]]  # Define centers for 3 clusters\n",
    "X, labels = make_blobs(n_samples=300, centers=centers, cluster_std=1, random_state=42)\n",
    "\n",
    "# Step 2: Apply different covariance matrices to each cluster\n",
    "# Define different covariance matrices\n",
    "covariances = [\n",
    "    np.array([[3, 1], [1, 2]]),  # Covariance for the first cluster\n",
    "    np.array([[3, 1], [1, 2]]),  # Covariance for the second cluster\n",
    "    np.array([[3, 1], [1, 2]]),  # Covariance for the third cluster\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize an empty array for transformed data\n",
    "X_transformed = np.zeros(X.shape)\n",
    "\n",
    "for label, cov in zip(range(len(centers)), covariances):\n",
    "    # Select data points belonging to the current cluster\n",
    "    cluster_data = X[labels == label]\n",
    "    # Apply the covariance matrix (linear transformation)\n",
    "    transformed_cluster_data = cluster_data.dot(cov)\n",
    "    # Store the transformed data back\n",
    "    X_transformed[labels == label] = transformed_cluster_data\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in range(len(centers)):\n",
    "    # Plot each cluster using transformed data\n",
    "    plt.scatter(\n",
    "        X_transformed[labels == label][:, 0],\n",
    "        X_transformed[labels == label][:, 1],\n",
    "        label=f\"Cluster {label + 1}\",\n",
    "    )\n",
    "\n",
    "plt.title(\"3 Clusters with Different Covariance Structures\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(visible=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPK_BDsDrbA0"
   },
   "source": [
    "You can show that the Bayes classifier assigns class $k$ to observation $X=x$ if:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = x^{T}\\Sigma^{-1}\\mu_k-\\frac{1}{2} \\mu_k^{T}\\Sigma^{-1}\\mu_k+ \\log \\pi_k\n",
    "$$\n",
    "is the largest among the $\\{\\delta_1(x), \\delta_2(x), \\cdots, \\delta_k(x)\\}$. This will determine regions that partition the space, decision boundaries, and a new point will be classified according to the region it is contained in.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1739555890098,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "bRtTk2FvuW9O",
    "outputId": "994870af-7ed0-493e-daf2-e2ee8bbdef6e"
   },
   "outputs": [],
   "source": [
    "# Step 3: Train LDA on the transformed data\n",
    "lda = LDA()\n",
    "lda.fit(X_transformed, labels)\n",
    "\n",
    "# Step 4: Visualize the decision boundaries\n",
    "# Create a mesh to plot the decision boundaries\n",
    "x_min, x_max = X_transformed[:, 0].min() - 1, X_transformed[:, 0].max() + 1\n",
    "y_min, y_max = X_transformed[:, 1].min() - 1, X_transformed[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Predict the class for each mesh point\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=labels, s=20, edgecolor=\"k\")\n",
    "\n",
    "plt.title(\"LDA Decision Boundaries\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ilu3PJoTWQ2J"
   },
   "source": [
    "## Problems: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwkZVcUTWhlg"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Bayes Classifier Definition\n",
    "\n",
    "What is the Bayes Classifier? Give the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDkNolnlW7ab"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The Bayes Classifier is the theoretical optimal classifier that minimizes the probability of misclassification. It assigns a new observation $X$ to the class $k$ that has the highest posterior probability:\n",
    "\n",
    "$$\\hat{Y} = \\arg\\max_k P(Y = k \\mid X = x)$$\n",
    "\n",
    "By Bayes' Theorem, this can be expressed as:\n",
    "\n",
    "$$P(Y = k \\mid X = x) = \\frac{P(X = x \\mid Y = k) P(Y = k)}{P(X = x)}$$\n",
    "\n",
    "where $P(Y = k)$ is the prior probability of class $k$, $P(X = x \\mid Y = k)$ is the class-conditional density (likelihood), and $P(X = x)$ is the marginal density of $X$.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi6KYJo7W8Sn"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** LDA Discriminant Function\n",
    "\n",
    "Implement the discriminant function for LDA. For class $k$, the discriminant function is:\n",
    "\n",
    "$$\\delta_k(x) = x^{T}\\Sigma^{-1}\\mu_k - \\frac{1}{2} \\mu_k^{T}\\Sigma^{-1}\\mu_k + \\log \\pi_k$$\n",
    "\n",
    "where $\\Sigma$ is the shared covariance matrix, $\\mu_k$ is the mean vector for class $k$, and $\\pi_k$ is the prior probability of class $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1739555890098,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "PzuF0dVEWhUK"
   },
   "outputs": [],
   "source": [
    "# Discriminant function for LDA\n",
    "def discriminant(x: np.ndarray, mu_k: np.ndarray, sigma: np.ndarray, pi: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the LDA discriminant function for class k.\n",
    "\n",
    "    Parameters:\n",
    "    x (np.ndarray): The input data point (d-dimensional).\n",
    "    mu_k (np.ndarray): Mean vector of class k (d-dimensional).\n",
    "    sigma (np.ndarray): Shared covariance matrix (d x d).\n",
    "    pi (float): Prior probability of class k.\n",
    "\n",
    "    Returns:\n",
    "    float: The discriminant score for class k.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute inverse of shared covariance matrix\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    # Compute linear term: x^T @ Sigma^{-1} @ mu_k\n",
    "    linear_term = np.dot(sigma_inv, mu_k)\n",
    "    # Compute constant term: -0.5 * mu_k^T @ Sigma^{-1} @ mu_k + log(pi)\n",
    "    constant_term = -0.5 * np.dot(mu_k.T, np.dot(sigma_inv, mu_k)) + np.log(pi)\n",
    "    return np.dot(x, linear_term) + constant_term\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "TGY4-pCwX9ue",
    "outputId": "e8d04e36-5872-4a30-8516-4dc98d416c78"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test case 1: Simple 2D identity covariance\n",
    "x1 = np.array([1, 2])\n",
    "mu_k1 = np.array([0, 0])\n",
    "sigma1 = np.array([[1, 0], [0, 1]])\n",
    "pi1 = 0.5\n",
    "result1 = discriminant(x1, mu_k1, sigma1, pi1)\n",
    "expected1 = -0.69314718056\n",
    "assert np.isclose(result1, expected1), f\"Test case 1 failed: {result1} != {expected1}\"\n",
    "\n",
    "# Test case 2: Non-zero mean\n",
    "x4 = np.array([1, 2])\n",
    "mu_k4 = np.array([2, 2])\n",
    "sigma4 = np.array([[1, 0], [0, 1]])\n",
    "pi4 = 1.0\n",
    "result4 = discriminant(x4, mu_k4, sigma4, pi4)\n",
    "expected4 = 2\n",
    "assert np.isclose(result4, expected4), f\"Test case 2 failed: {result4} != {expected4}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test case: 3D identity covariance\n",
    "x3 = np.array([1, 2, 3])\n",
    "mu_k3 = np.array([0, 0, 0])\n",
    "sigma3 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "pi3 = 0.3\n",
    "result3 = discriminant(x3, mu_k3, sigma3, pi3)\n",
    "expected3 = -1.2039728043259361\n",
    "assert np.isclose(result3, expected3), f\"Hidden test 1 failed: {result3} != {expected3}\"\n",
    "\n",
    "# Test case: Edge case - zero input\n",
    "x5 = np.array([0, 0])\n",
    "mu_k5 = np.array([1, 1])\n",
    "sigma5 = np.array([[1, 0], [0, 1]])\n",
    "pi5 = 0.5\n",
    "result5 = discriminant(x5, mu_k5, sigma5, pi5)\n",
    "expected5 = -0.5 * (1 + 1) + np.log(0.5)\n",
    "assert np.isclose(result5, expected5), f\"Hidden test 2 failed: {result5} != {expected5}\"\n",
    "\n",
    "# Test case: High-dimensional with random covariance\n",
    "d = 20\n",
    "np.random.seed(42)\n",
    "x_hd = np.random.normal(size=d)\n",
    "mu_k_hd = np.random.normal(size=d)\n",
    "sigma_hd = np.random.normal(size=(d, d))\n",
    "sigma_hd = sigma_hd.dot(sigma_hd.T)\n",
    "pi_hd = np.random.rand()\n",
    "result6 = discriminant(x_hd, mu_k_hd, sigma_hd, pi_hd)\n",
    "expected6 = -0.22459130958043083\n",
    "assert np.isclose(result6, expected6), f\"Hidden test 3 failed: {result6} != {expected6}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZpczW7mct6s"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** LDA Prediction\n",
    "\n",
    "Implement a function that predicts the class by taking the argmax of the discriminant function over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "MXBaS06ldKT5"
   },
   "outputs": [],
   "source": [
    "def predict(x: np.ndarray, mu: np.ndarray, sigma: np.ndarray, pi: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Computes the predicted class index using LDA.\n",
    "\n",
    "    Parameters:\n",
    "    x (np.ndarray): The input data point. shape = (d,)\n",
    "    mu (np.ndarray): Mean vectors of classes. shape = (k, d)\n",
    "    sigma (np.ndarray): Shared covariance matrix. shape = (d, d)\n",
    "    pi (np.ndarray): Prior probabilities. shape = (k,)\n",
    "\n",
    "    Returns:\n",
    "    int: The index of the predicted class.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    num_classes = mu.shape[0]\n",
    "    scores = np.zeros(num_classes)\n",
    "    for class_idx in range(num_classes):\n",
    "        scores[class_idx] = discriminant(x, mu[class_idx], sigma, pi[class_idx])\n",
    "    return np.argmax(scores)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "NehCOJcIeolO",
    "outputId": "a28277a7-e801-4563-a566-75980466fefd"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test case 1: Simple 2D example with two classes\n",
    "x_test = np.array([1, 2])\n",
    "mu_test = np.array([[0, 0], [2, 2]])\n",
    "sigma_test = np.array([[1, 0.2], [0.2, 1]])\n",
    "pi_test = np.array([0.5, 0.5])\n",
    "expected_class_1 = 1\n",
    "result_1 = predict(x_test, mu_test, sigma_test, pi_test)\n",
    "assert (\n",
    "    result_1 == expected_class_1\n",
    "), f\"Test case 1 failed: expected {expected_class_1}, got {result_1}\"\n",
    "\n",
    "# Test case 2: Higher-dimensional case with three classes\n",
    "x_test2 = np.array([3, 2, 1])\n",
    "mu_test2 = np.array([[1, 1, 1], [4, 4, 4], [0, 0, 0]])\n",
    "sigma_test2 = np.array([[1, 0.1, 0.2], [0.1, 1, 0.3], [0.2, 0.3, 1]])\n",
    "pi_test2 = np.array([0.4, 0.4, 0.2])\n",
    "expected_class_2 = 0\n",
    "result_2 = predict(x_test2, mu_test2, sigma_test2, pi_test2)\n",
    "assert (\n",
    "    result_2 == expected_class_2\n",
    "), f\"Test case 2 failed: expected {expected_class_2}, got {result_2}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test case 3: Imbalanced priors\n",
    "x_test3 = np.array([-1, -1])\n",
    "mu_test3 = np.array([[1, 1], [-1, -1]])\n",
    "sigma_test3 = np.array([[1, 0], [0, 1]])\n",
    "pi_test3 = np.array([0.9, 0.1])\n",
    "expected_class_3 = 1\n",
    "result_3 = predict(x_test3, mu_test3, sigma_test3, pi_test3)\n",
    "assert (\n",
    "    result_3 == expected_class_3\n",
    "), f\"Hidden test 1 failed: expected {expected_class_3}, got {result_3}\"\n",
    "\n",
    "# Test case 4: Priors affect decision\n",
    "x_test4 = np.array([0.5, 0.5])\n",
    "mu_test4 = np.array([[0, 0], [1, 1]])\n",
    "sigma_test4 = np.array([[1, 0], [0, 1]])\n",
    "pi_test4 = np.array([0.1, 0.9])\n",
    "result_4 = predict(x_test4, mu_test4, sigma_test4, pi_test4)\n",
    "assert result_4 == 1, f\"Hidden test 2 failed: expected 1, got {result_4}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1-ydN8AenQP"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Vectorized Discriminant Scores\n",
    "\n",
    "Using for loops is slow. Implement a vectorized function that directly returns an array of discriminant scores for all classes.\n",
    "\n",
    "**Hint:** You can use `np.einsum` to perform the computation of each of the two terms of the discriminants in one single call. For example, `np.einsum('ij,jk->ik', A, B)` computes matrix multiplication. See the [NumPy einsum documentation](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "YvB9CGqiem2L"
   },
   "outputs": [],
   "source": [
    "def discriminant_scores_single(\n",
    "    x: np.ndarray, mu: np.ndarray, sigma: np.ndarray, pi: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes all discriminant scores for LDA for a single data point.\n",
    "\n",
    "    Parameters:\n",
    "    x (np.ndarray): The input data point. shape = (d,)\n",
    "    mu (np.ndarray): Mean vectors of classes. shape = (k, d)\n",
    "    sigma (np.ndarray): Shared covariance matrix. shape = (d, d)\n",
    "    pi (np.ndarray): Prior probabilities. shape = (k,)\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array of discriminant scores. shape = (k,)\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute inverse of shared covariance matrix\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    # Compute linear term using einsum: x^T @ Sigma^{-1} @ mu_k for all k\n",
    "    linear_term = np.einsum(\"i, ij, kj -> k\", x, sigma_inv, mu)\n",
    "    # Compute constant term: -0.5 * mu_k^T @ Sigma^{-1} @ mu_k + log(pi_k)\n",
    "    constant_term = -0.5 * np.einsum(\"ki, ij, kj -> k\", mu, sigma_inv, mu) + np.log(pi)\n",
    "    return linear_term + constant_term\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "dthNRb307_qb"
   },
   "outputs": [],
   "source": [
    "def predict_vectorized_single(\n",
    "    x: np.ndarray, mu: np.ndarray, sigma: np.ndarray, pi: np.ndarray\n",
    ") -> int:\n",
    "    \"\"\"Helper function that uses discriminant_scores_single to predict.\"\"\"\n",
    "    return np.argmax(discriminant_scores_single(x, mu, sigma, pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "F-MiDfWcjOEn",
    "outputId": "aa326673-241c-4ce6-e6c5-7b1c018cfac0"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test case 1: Simple 2D example with two classes\n",
    "x_test = np.array([1, 2])\n",
    "mu_test = np.array([[0, 0], [2, 2]])\n",
    "sigma_test = np.array([[1, 0.2], [0.2, 1]])\n",
    "pi_test = np.array([0.5, 0.5])\n",
    "expected_class_1 = 1\n",
    "result_1 = predict_vectorized_single(x_test, mu_test, sigma_test, pi_test)\n",
    "assert (\n",
    "    result_1 == expected_class_1\n",
    "), f\"Test case 1 failed: expected {expected_class_1}, got {result_1}\"\n",
    "\n",
    "# Test case 2: Higher-dimensional case with three classes\n",
    "x_test2 = np.array([3, 2, 1])\n",
    "mu_test2 = np.array([[1, 1, 1], [4, 4, 4], [0, 0, 0]])\n",
    "sigma_test2 = np.array([[1, 0.1, 0.2], [0.1, 1, 0.3], [0.2, 0.3, 1]])\n",
    "pi_test2 = np.array([0.4, 0.4, 0.2])\n",
    "expected_class_2 = 0\n",
    "result_2 = predict_vectorized_single(x_test2, mu_test2, sigma_test2, pi_test2)\n",
    "assert (\n",
    "    result_2 == expected_class_2\n",
    "), f\"Test case 2 failed: expected {expected_class_2}, got {result_2}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test case 3: Imbalanced priors\n",
    "x_test3 = np.array([-1, -1])\n",
    "mu_test3 = np.array([[1, 1], [-1, -1]])\n",
    "sigma_test3 = np.array([[1, 0], [0, 1]])\n",
    "pi_test3 = np.array([0.9, 0.1])\n",
    "expected_class_3 = 1\n",
    "result_3 = predict_vectorized_single(x_test3, mu_test3, sigma_test3, pi_test3)\n",
    "assert (\n",
    "    result_3 == expected_class_3\n",
    "), f\"Hidden test 1 failed: expected {expected_class_3}, got {result_3}\"\n",
    "\n",
    "# Test case 4: Verify scores match non-vectorized version\n",
    "np.random.seed(123)\n",
    "x_rand = np.random.normal(size=5)\n",
    "mu_rand = np.random.normal(size=(3, 5))\n",
    "sigma_rand = np.random.normal(size=(5, 5))\n",
    "sigma_rand = sigma_rand.dot(sigma_rand.T)\n",
    "pi_rand = np.array([0.3, 0.4, 0.3])\n",
    "scores_vec = discriminant_scores_single(x_rand, mu_rand, sigma_rand, pi_rand)\n",
    "for class_idx in range(3):\n",
    "    score_loop = discriminant(x_rand, mu_rand[class_idx], sigma_rand, pi_rand[class_idx])\n",
    "    assert np.isclose(\n",
    "        scores_vec[class_idx], score_loop\n",
    "    ), f\"Hidden test 2 failed for class {class_idx}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWKIZ2QpleeH"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Batch Discriminant Scores\n",
    "\n",
    "Now vectorize the function over multiple data points to handle batched inputs.\n",
    "\n",
    "**Hint:** You can use `np.einsum` to perform the computation of each of the two terms of the discriminants in one single call. See the [NumPy einsum documentation](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "1h24lJAulc4O"
   },
   "outputs": [],
   "source": [
    "def discriminant_scores(\n",
    "    x: np.ndarray, mu: np.ndarray, sigma: np.ndarray, pi: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes all discriminant scores for LDA at several data points.\n",
    "\n",
    "    Parameters:\n",
    "    x (np.ndarray): The input data points. shape = (n, d)\n",
    "    mu (np.ndarray): Mean vectors of classes. shape = (k, d)\n",
    "    sigma (np.ndarray): Shared covariance matrix. shape = (d, d)\n",
    "    pi (np.ndarray): Prior probabilities. shape = (k,)\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array of discriminant scores. shape = (n, k)\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute inverse of shared covariance matrix\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    # Compute linear term (n, k) using einsum\n",
    "    linear_term = np.einsum(\"li, ij, kj -> lk\", x, sigma_inv, mu)\n",
    "    # Compute constant term (k,)\n",
    "    constant_term = -0.5 * np.einsum(\"ki, ij, kj -> k\", mu, sigma_inv, mu) + np.log(pi)\n",
    "    return linear_term + constant_term\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739555890099,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "94nInkuHoxYi",
    "outputId": "3ed801b2-0894-4c14-962c-aca016f532fd"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "def predict_vectorized(\n",
    "    x: np.ndarray, mu: np.ndarray, sigma: np.ndarray, pi: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Helper function to predict classes for batched inputs.\"\"\"\n",
    "    return np.argmax(discriminant_scores(x, mu, sigma, pi), axis=1)\n",
    "\n",
    "\n",
    "# Test case 1: Basic shape test\n",
    "num_samples = 4\n",
    "num_features = 3\n",
    "num_classes = 2\n",
    "np.random.seed(503)\n",
    "x_batch = np.random.normal(size=(num_samples, num_features))\n",
    "mu_batch = np.random.normal(size=(num_classes, num_features))\n",
    "sigma_batch = np.random.normal(size=(num_features, num_features))\n",
    "sigma_batch = sigma_batch.dot(sigma_batch.T)\n",
    "pi_batch = np.random.rand(num_classes)\n",
    "\n",
    "result = discriminant_scores(x_batch, mu_batch, sigma_batch, pi_batch)\n",
    "expected = np.array(\n",
    "    [\n",
    "        [-0.79650917, -1.79941157],\n",
    "        [-0.63978301, 2.06795532],\n",
    "        [0.10567327, 1.54700591],\n",
    "        [0.04314746, 1.44296768],\n",
    "    ]\n",
    ")\n",
    "assert np.allclose(result, expected), f\"Test case 1 failed: {result} != {expected}\"\n",
    "\n",
    "# Test case 2: Larger test with shape and statistics check\n",
    "num_samples = 100\n",
    "num_features = 20\n",
    "num_classes = 5\n",
    "np.random.seed(42)\n",
    "x_large = np.random.normal(size=(num_samples, num_features))\n",
    "mu_large = np.random.normal(size=(num_classes, num_features))\n",
    "sigma_large = np.random.normal(size=(num_features, num_features))\n",
    "sigma_large = sigma_large.dot(sigma_large.T)\n",
    "pi_large = np.random.rand(num_classes)\n",
    "\n",
    "result = discriminant_scores(x_large, mu_large, sigma_large, pi_large)\n",
    "assert result.shape == (\n",
    "    num_samples,\n",
    "    num_classes,\n",
    "), f\"Shape test failed: {result.shape} != {(num_samples, num_classes)}\"\n",
    "assert np.isclose(result.mean(), -41.130043376305416), f\"Mean test failed: {result.mean()}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert np.isclose(result.std(), 95.2601173142212), f\"Hidden test 1 failed: std {result.std()}\"\n",
    "assert np.isclose(result.min(), -525.4935904371674), f\"Hidden test 2 failed: min {result.min()}\"\n",
    "assert np.isclose(result.max(), 252.6084971655994), f\"Hidden test 3 failed: max {result.max()}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1t0ctkRr0AT"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** LDA Parameter Count\n",
    "\n",
    "We now know how to predict data points given a list of means, a covariance matrix, and a vector of prior probabilities.\n",
    "\n",
    "LDA starts by estimating all of these. How many parameters are estimated with $k$ classes and $d$ dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JAKLTDbsh6J"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "LDA estimates the following parameters:\n",
    "\n",
    "- **Mean vectors**: $k$ class means, each of dimension $d$, giving $k \\cdot d$ parameters.\n",
    "- **Shared covariance matrix**: One $d \\times d$ symmetric matrix, giving $\\frac{d(d+1)}{2}$ unique parameters.\n",
    "- **Prior probabilities**: $k$ priors, but since they sum to 1, only $k-1$ are free parameters.\n",
    "\n",
    "**Total parameters**: $k \\cdot d + \\frac{d(d+1)}{2} + (k-1)$\n",
    "\n",
    "This simplifies to: $kd + \\frac{d^2 + d}{2} + k - 1$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP_mn2VCsuCj"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** LDA Parameter Estimation\n",
    "\n",
    "Write a function that takes in $X$, $y$ and computes all LDA parameters using the following formulas:\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{1}{\\#\\{y_i = k\\}}\\sum_{y_i = k} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n - K} \\sum_{k=1}^{K} \\sum_{y_i = k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{\\#\\{y_i = k\\}}{n}\n",
    "$$\n",
    "\n",
    "where $\\mu_k$ is the mean vector for class $k$, $\\Sigma$ is the shared covariance matrix, and $\\pi_k$ is the prior probability of class $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1739555890418,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "7mjbz-rsshYC"
   },
   "outputs": [],
   "source": [
    "def lda_estimator(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Estimates the parameters for Linear Discriminant Analysis (LDA).\n",
    "\n",
    "    Parameters:\n",
    "    X: np.ndarray, shape = (n, d). Feature matrix for n samples.\n",
    "    y: np.ndarray, shape = (n,). Class labels for n samples.\n",
    "\n",
    "    Returns:\n",
    "    mu: np.ndarray, shape = (k, d) - Mean vectors for each class.\n",
    "    sigma: np.ndarray, shape = (d, d) - Shared covariance matrix.\n",
    "    pi: np.ndarray, shape = (k,) - Prior probabilities for each class.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    classes = np.unique(y)\n",
    "    num_classes = len(classes)\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "    # Compute priors: pi_k = (# samples in class k) / (total samples)\n",
    "    pi = np.array([np.mean(y == c) for c in classes])\n",
    "\n",
    "    # Compute means: mu_k = mean of X for samples in class k\n",
    "    mu = np.array([X[y == c].mean(axis=0) for c in classes])\n",
    "\n",
    "    # Compute shared covariance matrix using pooled within-class scatter\n",
    "    X_centered = X - mu[y]\n",
    "    sigma = (X_centered.T @ X_centered) / (num_samples - num_classes)\n",
    "\n",
    "    return mu, sigma, pi\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739555890418,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "4yCQyGzCrzgm",
    "outputId": "a33d433b-deac-41e6-84f8-01181fa16016"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Generate random test dataset\n",
    "np.random.seed(42)\n",
    "n_samples_per_class = 50\n",
    "num_features = 2\n",
    "num_classes = 2\n",
    "\n",
    "# Generate random means for each class\n",
    "mu_true = np.array([[2, 3], [9, 8]])\n",
    "sigma_true = np.array([[1, 0.5], [0.5, 1]])\n",
    "\n",
    "# Generate random data\n",
    "X_class_0 = np.random.multivariate_normal(mu_true[0], sigma_true, n_samples_per_class)\n",
    "X_class_1 = np.random.multivariate_normal(mu_true[1], sigma_true, n_samples_per_class)\n",
    "\n",
    "# Combine data\n",
    "X_test = np.vstack([X_class_0, X_class_1])\n",
    "y_test = np.hstack(\n",
    "    [np.zeros(n_samples_per_class, dtype=int), np.ones(n_samples_per_class, dtype=int)]\n",
    ")\n",
    "\n",
    "# Compute LDA parameters\n",
    "mu_test, sigma_test, pi_test = lda_estimator(X_test, y_test)\n",
    "\n",
    "# Expected values\n",
    "expected_mu = np.array([[2.15350725, 3.08148984], [9.01263359, 8.15269564]])\n",
    "expected_pi = np.array([0.5, 0.5])\n",
    "expected_sigma = np.array([[0.82666485, 0.30615014], [0.30615014, 0.78206076]])\n",
    "\n",
    "# Test means\n",
    "assert np.allclose(mu_test, expected_mu, atol=1e-2), f\"Mean test failed: {mu_test}\"\n",
    "\n",
    "# Test priors\n",
    "assert np.allclose(pi_test, expected_pi, atol=1e-2), f\"Prior test failed: {pi_test}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test covariance\n",
    "assert np.allclose(\n",
    "    sigma_test, expected_sigma, atol=1e-2\n",
    "), f\"Hidden test 1 (covariance) failed: {sigma_test}\"\n",
    "\n",
    "# Test with different class sizes\n",
    "np.random.seed(99)\n",
    "X_imbalanced = np.vstack(\n",
    "    [\n",
    "        np.random.multivariate_normal([0, 0], np.eye(2), 30),\n",
    "        np.random.multivariate_normal([3, 3], np.eye(2), 70),\n",
    "    ]\n",
    ")\n",
    "y_imbalanced = np.array([0] * 30 + [1] * 70)\n",
    "mu_imb, sigma_imb, pi_imb = lda_estimator(X_imbalanced, y_imbalanced)\n",
    "assert np.isclose(pi_imb[0], 0.3), f\"Hidden test 2 (priors) failed: {pi_imb[0]}\"\n",
    "assert np.isclose(pi_imb[1], 0.7), f\"Hidden test 3 (priors) failed: {pi_imb[1]}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWQrDxYcv4Co"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 8:** LDA Class Implementation\n",
    "\n",
    "Now implement a complete LDA classifier class. Use the functions we wrote above to fill in this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739555890418,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "yBJIoEMtv3Iq"
   },
   "outputs": [],
   "source": [
    "class LDAClassifier:\n",
    "    \"\"\"Linear Discriminant Analysis classifier.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        self.pi = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Estimates the parameters for Linear Discriminant Analysis (LDA).\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray, shape = (n, d). Feature matrix for n samples.\n",
    "        y: np.ndarray, shape = (n,). Class labels for n samples.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # BEGIN SOLUTION\n",
    "        self.mu, self.sigma, self.pi = lda_estimator(X, y)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def discriminant_scores(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the discriminant scores for LDA.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray, shape = (n, d). Feature matrix for n samples\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray, shape = (n, k) - Discriminant scores for each class.\n",
    "        \"\"\"\n",
    "        # BEGIN SOLUTION\n",
    "        return discriminant_scores(X, self.mu, self.sigma, self.pi)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts class labels for given data points.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray, shape = (n, d). Feature matrix for n samples.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray, shape = (n,). Predicted class labels for n samples.\n",
    "        \"\"\"\n",
    "        # BEGIN SOLUTION\n",
    "        return np.argmax(self.discriminant_scores(X), axis=1)\n",
    "        # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739555890418,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "EEJSX7O_xsu7",
    "outputId": "980f468b-4870-40d6-ba92-e5985ddfcb5f"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_breast_cancer()\n",
    "X = StandardScaler().fit_transform(data.data)\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit our LDA classifier\n",
    "lda_clf = LDAClassifier()\n",
    "lda_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and compute accuracy\n",
    "y_pred = lda_clf.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# Test that accuracy is reasonable (above 90%)\n",
    "assert accuracy > 0.90, f\"Accuracy too low: {accuracy}\"\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=\"viridis\")\n",
    "plt.title(\"Predicted Classes on Test Set\")\n",
    "plt.xlabel(\"Feature 1 (scaled)\")\n",
    "plt.ylabel(\"Feature 2 (scaled)\")\n",
    "plt.text(\n",
    "    0.95,\n",
    "    0.05,\n",
    "    f\"Test Accuracy: {accuracy:.2f}\",\n",
    "    transform=plt.gca().transAxes,\n",
    "    ha=\"right\",\n",
    "    va=\"bottom\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test that fit properly sets attributes\n",
    "assert lda_clf.mu is not None, \"Hidden test 1 failed: mu not set\"\n",
    "assert lda_clf.sigma is not None, \"Hidden test 2 failed: sigma not set\"\n",
    "assert lda_clf.pi is not None, \"Hidden test 3 failed: pi not set\"\n",
    "\n",
    "# Test shapes\n",
    "assert lda_clf.mu.shape == (2, 30), f\"Hidden test 4 failed: mu shape {lda_clf.mu.shape}\"\n",
    "assert lda_clf.sigma.shape == (\n",
    "    30,\n",
    "    30,\n",
    "), f\"Hidden test 5 failed: sigma shape {lda_clf.sigma.shape}\"\n",
    "assert lda_clf.pi.shape == (2,), f\"Hidden test 6 failed: pi shape {lda_clf.pi.shape}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTV1Gkd5U-i9"
   },
   "source": [
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "QDA relaxes the LDA assumption that classes share the same covariance matrix. Instead, each class has its own covariance matrix $\\Sigma_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqjv6SgX6T9f"
   },
   "source": [
    "### QDA Assumptions\n",
    "\n",
    "- Data is drawn from $k$ multivariate normal distributions, where each distribution can have a different mean vector $\\mu_k$ and a different covariance structure $\\Sigma_k$.\n",
    "\n",
    "QDA allows for each class to have its own covariance matrix, making it a more flexible approach that can capture more complex structures. However, it requires estimating more parameters and thus may not perform as well as LDA when training data is limited.\n",
    "\n",
    "The QDA discriminant function assigns class $k$ to observation $x$ if:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = -\\frac{1}{2}x^{T}\\Sigma^{-1}_k x + x^{T}\\Sigma^{-1}_k\\mu_k - \\frac{1}{2} \\mu_k^{T}\\Sigma^{-1}_k\\mu_k - \\frac{1}{2}\\log|\\Sigma_k| + \\log \\pi_k\n",
    "$$\n",
    "\n",
    "is the largest among all classes. This induces quadratic decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP9YeOmc6q7j"
   },
   "source": [
    "## Problems: QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASKeVhFz683R"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 9:** QDA Parameter Count\n",
    "\n",
    "How many parameters do you have to estimate in QDA with $k$ classes and $d$ dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rVLk7o37FQx"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "QDA estimates the following parameters:\n",
    "\n",
    "- **Mean vectors**: $k$ class means, each of dimension $d$, giving $k \\cdot d$ parameters.\n",
    "- **Class-specific covariance matrices**: $k$ symmetric $d \\times d$ matrices, each with $\\frac{d(d+1)}{2}$ unique parameters, giving $k \\cdot \\frac{d(d+1)}{2}$ total.\n",
    "- **Prior probabilities**: $k$ priors, but since they sum to 1, only $k-1$ are free parameters.\n",
    "\n",
    "**Total parameters**: $k \\cdot d + k \\cdot \\frac{d(d+1)}{2} + (k-1)$\n",
    "\n",
    "This simplifies to: $k \\left( d + \\frac{d(d+1)}{2} \\right) + k - 1 = k \\cdot \\frac{d^2 + 3d + 2}{2} - 1$\n",
    "\n",
    "Compared to LDA, QDA has $(k-1) \\cdot \\frac{d(d+1)}{2}$ more parameters due to the class-specific covariance matrices.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrLJVaiO7L7Q"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 10:** QDA on NHANES Dataset\n",
    "\n",
    "Fit QDA using `sklearn` on the NHANES dataset. The data files are located in `data/NHANES/`. Pick two features (BMI and RatioToPoverty) and focus on HDL as an outcome. Threshold HDL to generate three class labels:\n",
    "\n",
    "- Group 0: HDL < 40\n",
    "- Group 1: 40 <= HDL < 70\n",
    "- Group 2: HDL >= 70\n",
    "\n",
    "Before fitting the model, scale your data and split it into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "error",
     "timestamp": 1739555891418,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "tYKjR_Yh7KRM",
    "outputId": "55a4f518-088e-4f0e-83ba-4af37f007d24"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Load NHANES data from data folder\n",
    "data_path = \"data/NHANES/\"\n",
    "hdl = pd.read_sas(data_path + \"HDL_L.xpt\")\n",
    "bmx = pd.read_sas(data_path + \"BMX_L.xpt\")\n",
    "demo = pd.read_sas(data_path + \"DEMO_L.xpt\")\n",
    "\n",
    "# Join on SEQN\n",
    "df = pd.merge(hdl, bmx, on=\"SEQN\", how=\"inner\")\n",
    "df = pd.merge(df, demo, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "# Keep relevant features\n",
    "my_df = df[[\"SEQN\", \"LBDHDD\", \"BMXBMI\", \"RIDAGEYR\", \"INDFMPIR\"]]\n",
    "my_df = my_df.rename(\n",
    "    columns={\n",
    "        \"LBDHDD\": \"HDL\",\n",
    "        \"BMXBMI\": \"BMI\",\n",
    "        \"INDFMPIR\": \"RatioToPoverty\",\n",
    "        \"RIDAGEYR\": \"ScreeningAge\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Remove NAs\n",
    "my_df = my_df.dropna()\n",
    "\n",
    "# Threshold HDL into three groups\n",
    "my_df[\"HDL_group\"] = np.where(my_df[\"HDL\"] < 40, 0, np.where(my_df[\"HDL\"] < 70, 1, 2))\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(my_df.drop(columns=[\"SEQN\"]).corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix with Outcome\")\n",
    "plt.show()\n",
    "\n",
    "# Features and labels\n",
    "X = my_df[[\"BMI\", \"RatioToPoverty\"]].values\n",
    "y = my_df[\"HDL_group\"].values\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit QDA\n",
    "qda = QDA()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = qda.predict(X_test)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test):.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify QDA model was fitted\n",
    "assert hasattr(qda, \"means_\"), \"QDA model should be fitted\"\n",
    "assert len(qda.means_) == 3, \"Should have 3 class means\"\n",
    "\n",
    "# Verify accuracy is reasonable (better than random guessing with 3 classes)\n",
    "qda_accuracy = np.mean(y_pred == y_test)\n",
    "assert qda_accuracy > 0.4, f\"Accuracy should be better than random: {qda_accuracy}\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify data was properly scaled\n",
    "assert np.abs(X_scaled.mean()) < 0.1, \"Data should be centered near 0\"\n",
    "assert np.abs(X_scaled.std() - 1) < 0.5, \"Data should be standardized\"\n",
    "\n",
    "# Verify train/test split\n",
    "assert len(X_train) > len(X_test), \"Training set should be larger than test set\"\n",
    "assert len(X_train) + len(X_test) == len(X_scaled), \"Total samples should match\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0qrZLy59HXh"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 11:** QDA Decision Boundaries\n",
    "\n",
    "Plot the decision boundaries for the QDA model fitted in Problem 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1739555891418,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 300
    },
    "id": "0FgT6HgI9hyR"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Set min and max values with padding\n",
    "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
    "step_size = 0.02\n",
    "\n",
    "# Generate a grid of points\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n",
    "\n",
    "# Predict the class for the whole grid\n",
    "Z = qda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries and training data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap=\"viridis\")\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, s=20, edgecolor=\"k\", cmap=\"viridis\")\n",
    "plt.title(\"QDA Decision Boundaries\")\n",
    "plt.xlabel(\"BMI (scaled)\")\n",
    "plt.ylabel(\"RatioToPoverty (scaled)\")\n",
    "plt.colorbar(label=\"Class\")\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify the grid was created properly\n",
    "assert xx.shape == yy.shape, \"Grid shapes should match\"\n",
    "assert Z.shape == xx.shape, \"Prediction grid shape should match input grid\"\n",
    "\n",
    "# Verify predictions are valid class labels\n",
    "assert set(np.unique(Z)).issubset({0, 1, 2}), \"Predictions should be valid class labels\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify grid covers the data range\n",
    "assert xx.min() < X_scaled[:, 0].min(), \"Grid should extend beyond data range\"\n",
    "assert xx.max() > X_scaled[:, 0].max(), \"Grid should extend beyond data range\"\n",
    "assert yy.min() < X_scaled[:, 1].min(), \"Grid should extend beyond data range\"\n",
    "assert yy.max() > X_scaled[:, 1].max(), \"Grid should extend beyond data range\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
