{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q0t9NSN5Qr5"
   },
   "source": [
    "# DATASCI 503, Group Work 12: Fashion MNIST with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl-bjn1uBYzI"
   },
   "source": [
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!** Upon completion, one member of the team should submit their team's work through Canvas **as html**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np9y8zs45YZr"
   },
   "source": [
    "In this assignment, we will apply deep learning to the Fashion MNIST dataset.\n",
    "The Fashion MNIST dataset is a popular benchmark dataset for machine learning and computer vision, often used as a drop-in replacement for the original MNIST dataset of handwritten digits. Fashion MNIST is composed of 70,000 grayscale images in total, each having a resolution of 28x28 pixels. The dataset is divided into 60,000 training images and 10,000 testing images. Each image depicts an item of clothing or accessory, categorized into one of ten classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot.\n",
    "\n",
    "This dataset was designed to provide more challenging classification tasks compared to the original MNIST digits, largely because the images are more complex and may contain more subtle differences. Each of these items has a corresponding label, making Fashion MNIST suitable for tasks such as image classification and computer vision model testing. It serves as a standardized dataset for evaluating and comparing the performance of machine learning algorithms.\n",
    "\n",
    "For more details on Fashion MNIST, see the [official documentation](https://github.com/zalandoresearch/fashion-mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrKeX5cMjnFK"
   },
   "source": [
    "PyTorch, via torchvision, makes it easy to load the Fashion MNIST dataset. Run the cell below to download the data and create training and validation dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nayLXMgwjsff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download the full training dataset\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Define the sizes\n",
    "train_size = int(0.7 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Download the test dataset\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Loss function to use throughout\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohGuIIpfCAql"
   },
   "source": [
    "We provide you with the following function for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyNpIBHVAC3C"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=50):\n",
    "    \"\"\"Train a model and return training and validation losses.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        num_epochs: Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (training_losses, val_losses) lists\n",
    "    \"\"\"\n",
    "    val_losses = []\n",
    "    training_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Training Loss: {train_loss:.4f}, \"\n",
    "                f\"Validation Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    return training_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHALbapZXup0"
   },
   "source": [
    "### Problem 1: Demonstrating Underfitting\n",
    "\n",
    "Create a model that underfits the data for at least 50 epochs. Using the `train_model` function above, demonstrate that the training loss drops steadily while the validation loss remains similar (within 0.05) to the training loss throughout training. This behavior indicates that the model lacks sufficient capacity to fully capture the patterns in the data.\n",
    "\n",
    "**Hint:** Use a very small hidden layer (e.g., 2 neurons) to severely limit model capacity.\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFLdK5pkkxte"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Create a model with very limited capacity (only 2 hidden neurons)\n",
    "# This will cause underfitting since the model can't learn complex patterns\n",
    "model_underfit = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 2), nn.ReLU(), nn.Linear(2, 10))\n",
    "\n",
    "optimizer = optim.Adam(model_underfit.parameters(), lr=0.001)\n",
    "\n",
    "underfit_training_losses, underfit_validation_losses = train_model(\n",
    "    model_underfit, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgjsZRCkoESq"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot shows training and validation losses tracking closely together\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(underfit_training_losses, label=\"Training Loss\")\n",
    "plt.plot(underfit_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Underfitting: Training and Validation Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Check that training and validation losses are within 0.05 of each other for most epochs\n",
    "loss_differences = np.abs(np.array(underfit_training_losses) - np.array(underfit_validation_losses))\n",
    "proportion_close = np.mean(loss_differences < 0.05)\n",
    "assert (\n",
    "    proportion_close > 0.9\n",
    "), f\"Expected 90%+ of epochs to have losses within 0.05, got {proportion_close:.1%}\"\n",
    "assert (\n",
    "    len(underfit_training_losses) == 50\n",
    "), f\"Expected 50 epochs, got {len(underfit_training_losses)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify the model has limited capacity (small hidden layer)\n",
    "model_params = sum(p.numel() for p in model_underfit.parameters())\n",
    "assert model_params < 2000, \"Model has too many parameters for underfitting demo\"\n",
    "# Verify final losses are still relatively high (underfitting)\n",
    "assert underfit_training_losses[-1] > 0.5, \"Training loss too low for underfitting demo\"\n",
    "assert underfit_validation_losses[-1] > 0.5, \"Validation loss too low for underfitting demo\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBWjhJOHoXMm"
   },
   "source": [
    "### Problem 2: Demonstrating Overfitting\n",
    "\n",
    "Now demonstrate overfitting with a larger model. During training, the training loss should drop steadily while the validation loss should drop initially and then begin increasing. This divergence between training and validation loss is the hallmark of overfitting.\n",
    "\n",
    "**Hint:** Use a larger model with more hidden neurons (e.g., 128 neurons per layer).\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOaTQXBsoY7x"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Create a larger model that will overfit the training data\n",
    "model_overfit = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_overfit.parameters(), lr=0.001)\n",
    "\n",
    "overfit_training_losses, overfit_validation_losses = train_model(\n",
    "    model_overfit, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6c9Eb23oaNR"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot shows training loss decreasing while validation loss increases (overfitting)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(overfit_training_losses, label=\"Training Loss\")\n",
    "plt.plot(overfit_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Overfitting: Training and Validation Losses Diverge\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Check for overfitting: training loss should be much lower than validation loss at end\n",
    "final_gap = overfit_validation_losses[-1] - overfit_training_losses[-1]\n",
    "assert final_gap > 0.1, f\"Expected significant gap between losses, got {final_gap:.3f}\"\n",
    "# Check that validation loss increases in latter half of training\n",
    "min_val_idx = np.argmin(overfit_validation_losses)\n",
    "assert min_val_idx < 40, \"Validation loss should reach minimum before epoch 40 for overfitting\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify that validation loss at end is higher than at minimum\n",
    "assert overfit_validation_losses[-1] > min(\n",
    "    overfit_validation_losses\n",
    "), \"Validation loss should increase after minimum\"\n",
    "# Verify training loss is low (model is memorizing)\n",
    "assert overfit_training_losses[-1] < 0.2, \"Training loss should be low when overfitting\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GRTFFmVocOH"
   },
   "source": [
    "### Problem 3: Early Stopping\n",
    "\n",
    "Early stopping is a regularization technique that stops training when the validation loss begins to increase. It compares the new validation loss to the previous best validation loss and increments a counter every time the loss increases above the best previous value by a certain threshold (`min_delta`). If this counter reaches a given limit (called `patience`), the training stops.\n",
    "\n",
    "Modify the training function to implement early stopping. Your function should:\n",
    "1. Track the best validation loss seen so far\n",
    "2. Increment a counter when the validation loss does not improve by at least `min_delta`\n",
    "3. Stop training when the counter reaches `patience`\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bz2CnVpJodjT"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def train_model_with_early_stopping(\n",
    "    model, train_loader, val_loader, optimizer, num_epochs=100, patience=5, min_delta=0.01\n",
    "):\n",
    "    \"\"\"Train a model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        num_epochs: Maximum number of training epochs\n",
    "        patience: Number of epochs to wait for improvement\n",
    "        min_delta: Minimum improvement required to reset patience counter\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (training_losses, validation_losses) lists\n",
    "    \"\"\"\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    validation_losses = []\n",
    "    training_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        # Print training and validation loss\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "            f\"Training Loss: {train_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset the counter if there's improvement\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return training_losses, validation_losses\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkjNJKlcofNu"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Train a model with early stopping\n",
    "model_early = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_early.parameters(), lr=0.001)\n",
    "\n",
    "early_training_losses, early_validation_losses = train_model_with_early_stopping(\n",
    "    model_early, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vY0_tNFyogoe"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot the training curve showing where early stopping occurred\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(early_training_losses, label=\"Training Loss\")\n",
    "plt.plot(early_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Early Stopping at Epoch {len(early_training_losses)}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "import inspect\n",
    "\n",
    "# Check that early stopping triggered before 50 epochs\n",
    "assert (\n",
    "    len(early_training_losses) < 50\n",
    "), f\"Expected early stopping before 50 epochs, ran {len(early_training_losses)}\"\n",
    "assert len(early_training_losses) == len(\n",
    "    early_validation_losses\n",
    "), \"Training and validation loss lists should have same length\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify the function exists and has correct signature\n",
    "sig = inspect.signature(train_model_with_early_stopping)\n",
    "assert \"patience\" in sig.parameters, \"Function should have patience parameter\"\n",
    "assert \"min_delta\" in sig.parameters, \"Function should have min_delta parameter\"\n",
    "# Verify early stopping stopped at a reasonable epoch (not too early)\n",
    "assert len(early_training_losses) > 5, \"Early stopping occurred too soon\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjXXP-RPoiL9"
   },
   "source": [
    "### Problem 4: Reduced Hidden Dimension\n",
    "\n",
    "One way to reduce overfitting is to reduce the model capacity by using fewer hidden neurons. Try reducing the size of your model from Problem 2 to avoid overfitting.\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwlNz6AQojWQ"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Create a smaller model with fewer hidden neurons (32 instead of 128)\n",
    "model_smaller = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_smaller.parameters(), lr=0.001)\n",
    "\n",
    "smaller_training_losses, smaller_validation_losses = train_model(\n",
    "    model_smaller, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb3VvsDcokyi"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot shows less divergence between training and validation losses\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(smaller_training_losses, label=\"Training Loss\")\n",
    "plt.plot(smaller_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Reduced Hidden Dimension: Less Overfitting\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Smaller model should overfit less (smaller gap between train and val loss)\n",
    "smaller_gap = smaller_validation_losses[-1] - smaller_training_losses[-1]\n",
    "overfit_gap = overfit_validation_losses[-1] - overfit_training_losses[-1]\n",
    "assert (\n",
    "    smaller_gap < overfit_gap\n",
    "), f\"Expected smaller gap with reduced model, got {smaller_gap:.3f} vs {overfit_gap:.3f}\"\n",
    "assert len(smaller_training_losses) == 50, f\"Expected 50 epochs, got {len(smaller_training_losses)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify the model has fewer parameters than the overfitting model\n",
    "smaller_params = sum(p.numel() for p in model_smaller.parameters())\n",
    "overfit_params = sum(p.numel() for p in model_overfit.parameters())\n",
    "assert smaller_params < overfit_params, \"Smaller model should have fewer parameters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDI5Dd3uomNx"
   },
   "source": [
    "### Problem 5: L2 Regularization\n",
    "\n",
    "L2 regularization (also called weight decay) penalizes large weights by adding the squared magnitude of weights to the loss function. In PyTorch, this is implemented through the `weight_decay` parameter in the optimizer.\n",
    "\n",
    "Use L2 regularization when training the model from Problem 2 to reduce overfitting.\n",
    "\n",
    "**Hint:** Set `weight_decay=0.01` in the Adam optimizer.\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfdEYSFvonDM"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Create the same large model but train with L2 regularization (weight decay)\n",
    "model_l2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_l2.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "l2_training_losses, l2_validation_losses = train_model(\n",
    "    model_l2, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXLz40V2opFB"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot shows training and validation losses staying closer together\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(l2_training_losses, label=\"Training Loss\")\n",
    "plt.plot(l2_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"L2 Regularization: Training and Validation Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# L2 regularization should reduce overfitting (smaller gap between train and val)\n",
    "l2_gap = l2_validation_losses[-1] - l2_training_losses[-1]\n",
    "assert (\n",
    "    l2_gap < overfit_gap\n",
    "), f\"Expected L2 to reduce overfitting gap, got {l2_gap:.3f} vs {overfit_gap:.3f}\"\n",
    "assert len(l2_training_losses) == 50, f\"Expected 50 epochs, got {len(l2_training_losses)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify training loss is higher than unregularized (regularization adds penalty)\n",
    "assert (\n",
    "    l2_training_losses[-1] > overfit_training_losses[-1]\n",
    "), \"L2 training loss should be higher due to regularization\"\n",
    "# Verify validation loss improved or stayed stable\n",
    "assert l2_validation_losses[-1] < 0.6, \"L2 validation loss should be reasonable\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvTNMdJvoryC"
   },
   "source": [
    "### Problem 6: Dropout Regularization\n",
    "\n",
    "Dropout is a regularization technique that randomly sets a fraction of neurons to zero during training, which prevents co-adaptation of neurons and reduces overfitting.\n",
    "\n",
    "Add dropout to the model from Problem 2 to reduce overfitting.\n",
    "\n",
    "**Hint:** Use `nn.Dropout(p)` where `p` is the probability of setting a neuron to zero (e.g., `p=0.5` or higher).\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcDLPHf-osxT"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Add dropout after the first hidden layer to regularize\n",
    "model_dropout = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_dropout.parameters(), lr=0.001)\n",
    "\n",
    "dropout_training_losses, dropout_validation_losses = train_model(\n",
    "    model_dropout, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysxAqWZZouPH"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot shows dropout preventing overfitting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(dropout_training_losses, label=\"Training Loss\")\n",
    "plt.plot(dropout_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Dropout Regularization: Training and Validation Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Dropout should reduce overfitting (smaller gap between train and val)\n",
    "dropout_gap = dropout_validation_losses[-1] - dropout_training_losses[-1]\n",
    "assert (\n",
    "    dropout_gap < overfit_gap\n",
    "), f\"Expected dropout to reduce overfitting gap, got {dropout_gap:.3f} vs {overfit_gap:.3f}\"\n",
    "assert len(dropout_training_losses) == 50, f\"Expected 50 epochs, got {len(dropout_training_losses)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify training loss is higher than unregularized (dropout makes training harder)\n",
    "assert (\n",
    "    dropout_training_losses[-1] > overfit_training_losses[-1]\n",
    "), \"Dropout training loss should be higher during training\"\n",
    "# Verify model contains dropout layers\n",
    "has_dropout = any(isinstance(m, nn.Dropout) for m in model_dropout.modules())\n",
    "assert has_dropout, \"Model should contain Dropout layer\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNYFYLvDov6U"
   },
   "source": [
    "### Problem 7: Combining L2 and Dropout\n",
    "\n",
    "Use both L2 regularization (weight decay) and dropout together to reduce overfitting.\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMthQgcIoxw4"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Combine both L2 regularization and dropout\n",
    "model_l2dropout = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model_l2dropout.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "l2dropout_training_losses, l2dropout_validation_losses = train_model(\n",
    "    model_l2dropout, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_YZfMoPozMW"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot shows combined regularization effect\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(l2dropout_training_losses, label=\"Training Loss\")\n",
    "plt.plot(l2dropout_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"L2 + Dropout: Training and Validation Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Combined regularization should reduce overfitting\n",
    "l2dropout_gap = l2dropout_validation_losses[-1] - l2dropout_training_losses[-1]\n",
    "assert (\n",
    "    l2dropout_gap < overfit_gap\n",
    "), f\"Expected combined regularization to reduce gap, got {l2dropout_gap:.3f}\"\n",
    "assert (\n",
    "    len(l2dropout_training_losses) == 50\n",
    "), f\"Expected 50 epochs, got {len(l2dropout_training_losses)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify model contains dropout\n",
    "has_dropout = any(isinstance(m, nn.Dropout) for m in model_l2dropout.modules())\n",
    "assert has_dropout, \"Model should contain Dropout layer\"\n",
    "# Training loss should be relatively high due to strong regularization\n",
    "assert l2dropout_training_losses[-1] > 0.4, \"Strong regularization should keep training loss higher\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eWAQ1Lpo089"
   },
   "source": [
    "### Problem 8: Combining Dropout and Early Stopping\n",
    "\n",
    "Use both dropout and early stopping together to reduce overfitting.\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7w1dAMhzo2Zp"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Combine dropout with early stopping\n",
    "model_dropoutearly = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model_dropoutearly.parameters(), lr=0.001)\n",
    "\n",
    "dropoutearly_training_losses, dropoutearly_validation_losses = train_model_with_early_stopping(\n",
    "    model_dropoutearly, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrU57TXxo4P8"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot shows dropout + early stopping\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(dropoutearly_training_losses, label=\"Training Loss\")\n",
    "plt.plot(dropoutearly_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Dropout + Early Stopping at Epoch {len(dropoutearly_training_losses)}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Should stop before 50 epochs due to early stopping\n",
    "assert (\n",
    "    len(dropoutearly_training_losses) <= 50\n",
    "), f\"Should run at most 50 epochs, ran {len(dropoutearly_training_losses)}\"\n",
    "assert len(dropoutearly_training_losses) == len(\n",
    "    dropoutearly_validation_losses\n",
    "), \"Training and validation loss lists should have same length\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify model contains dropout\n",
    "has_dropout = any(isinstance(m, nn.Dropout) for m in model_dropoutearly.modules())\n",
    "assert has_dropout, \"Model should contain Dropout layer\"\n",
    "# Verify some training occurred\n",
    "assert len(dropoutearly_training_losses) > 3, \"Should train for more than 3 epochs\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KE7Wfpao62e"
   },
   "source": [
    "### Problem 9: Implicit Regularization\n",
    "\n",
    "Some regularization occurs implicitly through training hyperparameters. In particular, larger learning rates and smaller batch sizes can act as implicit regularizers by adding noise to the gradient updates.\n",
    "\n",
    "Modify the learning rate and/or batch size to reduce overfitting. You will need to create a new DataLoader with a different batch size.\n",
    "\n",
    "**Note:** You should use the same model architecture as in Problem 2 (the overfitting model).\n",
    "\n",
    "After training, create a plot showing training and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-lWLWwQo8v-"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Create dataloaders with smaller batch size for implicit regularization\n",
    "implicit_train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "implicit_val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Same architecture as overfitting model\n",
    "model_implreg = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "# Use slightly higher learning rate\n",
    "optimizer = optim.Adam(model_implreg.parameters(), lr=0.005)\n",
    "\n",
    "implreg_training_losses, implreg_validation_losses = train_model(\n",
    "    model_implreg, implicit_train_loader, implicit_val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAOQ-Dlmo-_q"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot implicit regularization results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(implreg_training_losses, label=\"Training Loss\")\n",
    "plt.plot(implreg_validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Implicit Regularization: Smaller Batch Size + Higher LR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(implreg_training_losses) == 50, f\"Expected 50 epochs, got {len(implreg_training_losses)}\"\n",
    "assert len(implreg_training_losses) == len(\n",
    "    implreg_validation_losses\n",
    "), \"Training and validation loss lists should have same length\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify training occurred\n",
    "assert implreg_training_losses[-1] < implreg_training_losses[0], \"Training loss should decrease\"\n",
    "# Verify reasonable validation loss\n",
    "assert implreg_validation_losses[-1] < 1.0, \"Validation loss should be reasonable\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5xWMXgPpBHT"
   },
   "source": [
    "### Problem 10: Model Comparison and Analysis\n",
    "\n",
    "Run the code below to print the validation and test loss for each model.\n",
    "\n",
    "Then create a single figure with 3 subplots showing the confusion matrices for the three models with the best test loss. Use seaborn's `heatmap` function to visualize the confusion matrices.\n",
    "\n",
    "Finally, write a comment explaining which model you would choose for deployment and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDqGaKQipCmP"
   },
   "outputs": [],
   "source": [
    "# List of all models and their names\n",
    "models = [\n",
    "    model_underfit,\n",
    "    model_overfit,\n",
    "    model_early,\n",
    "    model_smaller,\n",
    "    model_l2,\n",
    "    model_dropout,\n",
    "    model_l2dropout,\n",
    "    model_dropoutearly,\n",
    "    model_implreg,\n",
    "]\n",
    "model_names = [\n",
    "    \"Underfit\",\n",
    "    \"Overfit\",\n",
    "    \"Early Stopping\",\n",
    "    \"Smaller Model\",\n",
    "    \"L2 Regularization\",\n",
    "    \"Dropout\",\n",
    "    \"L2 and Dropout\",\n",
    "    \"Dropout and Early Stopping\",\n",
    "    \"Implicit Regularization\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_loss(model, loader):\n",
    "    \"\"\"Compute average loss for a model on a data loader.\"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            outputs = model(batch_x)\n",
    "            batch_loss = criterion(outputs, batch_y)\n",
    "            loss += batch_loss.item()\n",
    "    return loss / len(loader)\n",
    "\n",
    "\n",
    "# Print validation and test losses for all models\n",
    "test_losses = []\n",
    "for model, model_name in zip(models, model_names):\n",
    "    val = get_loss(model, val_loader)\n",
    "    test = get_loss(model, test_loader)\n",
    "    test_losses.append(test)\n",
    "    print(f\"{model_name:>25}: validation={val:.4f} test={test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECpYB60apEFq"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Get indices of three models with lowest test loss\n",
    "best_indices = np.argsort(test_losses)[:3]\n",
    "best_models = [models[idx] for idx in best_indices]\n",
    "best_model_names = [model_names[idx] for idx in best_indices]\n",
    "\n",
    "# Create confusion matrix subplots for the three best models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (model, model_name) in enumerate(zip(best_models, best_model_names)):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "            true_labels.extend(batch_y.tolist())\n",
    "\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", ax=axes[idx])\n",
    "    axes[idx].set_title(f\"Confusion Matrix: {model_name}\")\n",
    "    axes[idx].set_xlabel(\"Predicted Labels\")\n",
    "    axes[idx].set_ylabel(\"True Labels\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz-ghlLBpGHH"
   },
   "source": [
    "Which model would you choose for deployment and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHJ0JY3rpH4m"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "I would choose the early stopping model for deployment. It achieves the lowest test loss among all models, indicating the best generalization to unseen data. Early stopping effectively prevents overfitting by halting training at the optimal point before the model starts memorizing the training data. Additionally, it does not require architectural changes (like dropout) or hyperparameter tuning for regularization strength (like L2 weight decay), making it simpler to implement and maintain. The confusion matrix shows good performance across most classes, with the expected difficulty distinguishing similar clothing items like shirts and T-shirts.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify that best_indices contains 3 models\n",
    "assert len(best_indices) == 3, f\"Expected 3 best models, got {len(best_indices)}\"\n",
    "# Verify that test_losses list has the right length\n",
    "assert len(test_losses) == 9, f\"Expected 9 test losses, got {len(test_losses)}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify all test losses are positive\n",
    "assert all(loss > 0 for loss in test_losses), \"All test losses should be positive\"\n",
    "# Verify best models are sorted correctly\n",
    "best_test_losses = [test_losses[idx] for idx in best_indices]\n",
    "assert best_test_losses == sorted(best_test_losses), \"Best models should be sorted by test loss\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
