{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 503, Group Work 3: ROC Curves and Logistic Regression\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Introduction to Logistic Regression\n",
    "\n",
    "In this lab, we are going to play with some logistic regression models. The data we will be using is synthetic and manually created by sampling from a multivariate Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 500\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, 0.75], [0.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([0, 2], [[1, 0.75], [0.75, 1]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack(\n",
    "    (np.zeros(num_observations, dtype=int), np.ones(num_observations, dtype=int))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let us visualize what the generated data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(\n",
    "    simulated_separableish_features[:, 0],\n",
    "    simulated_separableish_features[:, 1],\n",
    "    c=simulated_labels,\n",
    "    alpha=0.4,\n",
    "    s=5,\n",
    ")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Simulated Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We now split the data into training and testing sets. We use a 70%/30% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    simulated_separableish_features, simulated_labels, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now import logistic regression model from sklearn and train the model from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log = sklearn.linear_model.LogisticRegression()\n",
    "model_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Logistic regression has trained successfully. We now use our fitted model to predict the probability of the binary outcomes in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model_log.predict_proba(X_test)\n",
    "\n",
    "# Show logistic regression's estimated probability mass function along with\n",
    "# the true value of y_i for each test point\n",
    "viz_table = pd.DataFrame(\n",
    "    data=np.c_[y_prob, y_test],\n",
    "    columns=[r\"$\\hat p(0|x_i)$\", r\"$\\hat p(1|x_i)$\", r\"$y_i$\"],\n",
    ")\n",
    "viz_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute p(y_i | x_i) for each sample\n",
    "pygx = y_prob[np.r_[0 : len(y_test)], y_test]\n",
    "viz_table[r\"$\\hat p(y_i|x_i)$\"] = pygx\n",
    "viz_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average log likelihood\n",
    "np.mean(np.log(pygx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn provides a convenience function for computing the estimated cross-entropy\n",
    "# i.e., the negative log likelihood (NLL), which is just the negative of what\n",
    "# we computed above\n",
    "sklearn.metrics.log_loss(y_test, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Hard Classifiers and Thresholding\n",
    "\n",
    "Now let's make a *hard classifier*, something that makes a single guess about what the response might be, given any input. This is contrasted with the soft classifier we already have, that estimated the conditional probability of each response given any input.\n",
    "\n",
    "We'll start by making a hard classifier by thresholding our estimate of the conditional pmf.\n",
    "\n",
    "$$\\hat y(x) = \\begin{cases} 1 & \\mathrm{if}\\ \\hat p(1|x)>t \\\\ 0 & \\mathrm{otherwise}\\end{cases}$$\n",
    "\n",
    "We'll start with $t=0.6$, which is pretty arbitrary. You can also play around with different values and see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.6\n",
    "y_pred = np.where(y_prob[:, 1] > thresh, 1, 0)\n",
    "pd.DataFrame({\"phat(1|x_i)\": y_prob[:, 1], \"yhat(x_i)\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create a confusion matrix using Pandas\n",
    "cm_pd = pd.crosstab(y_test, y_pred, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "cm_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas can also create row summaries and column summaries\n",
    "cm_pd = pd.crosstab(y_test, y_pred, rownames=[\"Actual\"], colnames=[\"Predicted\"], margins=True)\n",
    "cm_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some metrics that are special for binary classification, where we have\n",
    "# a \"negative\" and \"positive\" class\n",
    "# Here \"positive\" is class 1 and \"negative\" is class 0\n",
    "\n",
    "FP = cnf_matrix[0, 1]  # it is \"negative\" (class 0) but we predict +1\n",
    "FN = cnf_matrix[1, 0]\n",
    "TP = cnf_matrix[1, 1]\n",
    "TN = cnf_matrix[0, 0]\n",
    "\n",
    "FP = float(FP)\n",
    "FN = float(FN)\n",
    "TP = float(TP)\n",
    "TN = float(TN)\n",
    "\n",
    "metrics_series = pd.Series(\n",
    "    {\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        \"TPR\": TP / (TP + FN),\n",
    "        # Specificity or true negative rate\n",
    "        \"TNR\": TN / (TN + FP),\n",
    "        # Precision or positive predictive value\n",
    "        \"PPV\": TP / (TP + FP),\n",
    "        # Negative predictive value\n",
    "        \"NPV\": TN / (TN + FN),\n",
    "        # Fall out or false positive rate\n",
    "        \"FPR\": FP / (FP + TN),\n",
    "        # False negative rate\n",
    "        \"FNR\": FN / (TP + FN),\n",
    "        # False discovery rate\n",
    "        \"FDR\": FP / (TP + FP),\n",
    "    }\n",
    ")\n",
    "metrics_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's make a curve, evaluating the precision and recall for many different hard classifiers based on many different thresholds. sklearn makes this easy for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, thresh = sklearn.metrics.precision_recall_curve(y_test, y_prob[:, 1])\n",
    "\n",
    "plt.plot(rec, prec, label=\"Precision and recall for many hard classifiers based on thresholds\")\n",
    "\n",
    "plt.plot(\n",
    "    [metrics_series.TPR],\n",
    "    [metrics_series.PPV],\n",
    "    \"o\",\n",
    "    label=\"Precision and recall for the hard classifier we chose above\",\n",
    ")\n",
    "\n",
    "plt.legend(bbox_to_anchor=[1, 1])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Group Work: Implementing ROC from Scratch\n",
    "\n",
    "In the lab, we demonstrated how to use sklearn's library to plot the precision-recall curve. For group work, we will implement ROC from scratch. The motivation is two-fold:\n",
    "\n",
    "1. You better understand how some of the libraries implement the functions you end up using.\n",
    "2. You build up the muscle memory for knowing what AUC and ROC actually are instead of just a number.\n",
    "\n",
    "We will use the NHANES dataset for this exercise. The data files are located in `data/NHANES/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the NHANES datasets\n",
    "bmx_df = pd.read_sas(\"data/NHANES/BMX_L.xpt\")\n",
    "demo_df = pd.read_sas(\"data/NHANES/DEMO_L.xpt\")\n",
    "hdl_df = pd.read_sas(\"data/NHANES/HDL_L.xpt\")\n",
    "\n",
    "print(\"First few rows of BMX_L.xpt (Body Measures):\")\n",
    "print(bmx_df.head())\n",
    "print(\"\\nFirst few rows of DEMO_L.xpt (Demographics):\")\n",
    "print(demo_df.head())\n",
    "print(\"\\nFirst few rows of HDL_L.xpt (HDL Cholesterol):\")\n",
    "print(hdl_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Load the NHANES Datasets\n",
    "\n",
    "Load the three NHANES datasets (HDL, BMX, and DEMO) into dataframes. The data files are located in `data/NHANES/`.\n",
    "\n",
    "Store the loaded dataframes in variables named `hdl`, `bmx`, and `demo`.\n",
    "\n",
    "**Hint:** Use `pd.read_sas()` to load SAS .xpt files. See the [Pandas read_sas documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_sas.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "hdl = pd.read_sas(\"data/NHANES/HDL_L.xpt\")\n",
    "bmx = pd.read_sas(\"data/NHANES/BMX_L.xpt\")\n",
    "demo = pd.read_sas(\"data/NHANES/DEMO_L.xpt\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"hdl\" in dir(), \"Variable 'hdl' not defined\"\n",
    "assert \"bmx\" in dir(), \"Variable 'bmx' not defined\"\n",
    "assert \"demo\" in dir(), \"Variable 'demo' not defined\"\n",
    "assert isinstance(hdl, pd.DataFrame), \"hdl should be a DataFrame\"\n",
    "assert isinstance(bmx, pd.DataFrame), \"bmx should be a DataFrame\"\n",
    "assert isinstance(demo, pd.DataFrame), \"demo should be a DataFrame\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"SEQN\" in hdl.columns, \"hdl should have SEQN column\"\n",
    "assert \"SEQN\" in bmx.columns, \"bmx should have SEQN column\"\n",
    "assert \"SEQN\" in demo.columns, \"demo should have SEQN column\"\n",
    "assert \"LBDHDD\" in hdl.columns, \"hdl should have LBDHDD column\"\n",
    "assert len(hdl) > 0, \"hdl should not be empty\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** Join the Datasets\n",
    "\n",
    "Join the three datasets together using their primary key (`SEQN`) with an inner join. Store the result in a variable named `df`.\n",
    "\n",
    "**Hint:** Use `pd.merge()` to join dataframes. You may need to call it twice to join all three datasets. See the [Pandas merge documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Inner join all three datasets on SEQN\n",
    "df = pd.merge(hdl, bmx, on=\"SEQN\", how=\"inner\")\n",
    "df = pd.merge(df, demo, on=\"SEQN\", how=\"inner\")\n",
    "# END SOLUTION\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"df\" in dir(), \"Variable 'df' not defined\"\n",
    "assert isinstance(df, pd.DataFrame), \"df should be a DataFrame\"\n",
    "assert \"SEQN\" in df.columns, \"df should have SEQN column\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"LBDHDD\" in df.columns, \"df should have LBDHDD column from hdl\"\n",
    "assert \"BMXBMI\" in df.columns, \"df should have BMXBMI column from bmx\"\n",
    "assert \"RIDAGEYR\" in df.columns, \"df should have RIDAGEYR column from demo\"\n",
    "assert len(df) > 5000, \"df should have a substantial number of rows after inner join\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Select Predictor Variables\n",
    "\n",
    "Pick a set of predictors to run a logistic regression to predict the level of HDL cholesterol.\n",
    "\n",
    "(a) In the markdown cell below, write the column name of each variable you select along with a short English description of what the variable represents.\n",
    "\n",
    "(b) In the code cell, filter your dataset to only contain the columns `SEQN`, your predictor variables, and the response variable `LBDHDD`. Rename `LBDHDD` to `HDL` for clarity. Store the result in a variable named `my_df`. You may also rename the other columns to more readable names.\n",
    "\n",
    "**Required predictors:** You must include at least:\n",
    "- `BMXBMI` (Body Mass Index)\n",
    "- `RIDAGEYR` (Age in years at screening)\n",
    "- `INDFMPIR` (Ratio of family income to poverty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**Selected Variables:**\n",
    "- `LBDHDD`: HDL cholesterol level (mg/dL) - the response variable\n",
    "- `BMXBMI`: Body Mass Index - a measure of body fat based on height and weight\n",
    "- `RIDAGEYR`: Age in years at screening - participant's age when examined\n",
    "- `INDFMPIR`: Ratio of family income to poverty threshold - a socioeconomic indicator\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Select the required columns and rename them for clarity\n",
    "my_df = df[[\"SEQN\", \"LBDHDD\", \"BMXBMI\", \"RIDAGEYR\", \"INDFMPIR\"]].copy()\n",
    "my_df = my_df.rename(\n",
    "    columns={\n",
    "        \"LBDHDD\": \"HDL\",\n",
    "        \"BMXBMI\": \"BMI\",\n",
    "        \"RIDAGEYR\": \"ScreeningAge\",\n",
    "        \"INDFMPIR\": \"RatioToPoverty\",\n",
    "    }\n",
    ")\n",
    "# END SOLUTION\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"my_df\" in dir(), \"Variable 'my_df' not defined\"\n",
    "assert isinstance(my_df, pd.DataFrame), \"my_df should be a DataFrame\"\n",
    "assert len(my_df.columns) >= 4, \"my_df should have at least 4 columns (SEQN + HDL + predictors)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert \"SEQN\" in my_df.columns, \"my_df should have SEQN column\"\n",
    "assert len(my_df) > 0, \"my_df should not be empty\"\n",
    "assert \"HDL\" in my_df.columns, \"my_df should have HDL column (renamed from LBDHDD)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Handle Missing Values\n",
    "\n",
    "Drop all rows with missing values from `my_df` since they do not help us much for modeling.\n",
    "\n",
    "**Important:** If you remove more than 30% of all rows in the dataset, your set of variables is likely removing too much of the data. Try to find a different combination of variables if this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = len(my_df)\n",
    "# BEGIN SOLUTION\n",
    "my_df = my_df.dropna()\n",
    "# END SOLUTION\n",
    "print(f\"Retained {len(my_df) / original_size:.1%} of the original data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert my_df.isna().sum().sum() == 0, \"my_df should have no missing values after dropna\"\n",
    "assert len(my_df) / original_size > 0.7, \"Should retain at least 70% of the data\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(my_df) > 5000, \"my_df should have a substantial number of rows\"\n",
    "assert my_df.shape[1] >= 4, \"my_df should have at least 4 columns\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Create Binary HDL Indicator\n",
    "\n",
    "High-density lipoprotein (HDL) cholesterol is often called the \"good\" cholesterol because higher levels are generally associated with a lower risk of heart disease.\n",
    "\n",
    "An HDL of 60 mg/dL or higher is often viewed as protective against heart disease. This is typically the level you'd like to aim for, if possible.\n",
    "\n",
    "Convert your HDL variable to a binary indicator that is `True` when HDL is **at least** 60 mg/dL and `False` otherwise. Store this in a column named `HDL_healthy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Create binary indicator for healthy HDL (>= 60 mg/dL)\n",
    "my_df[\"HDL_healthy\"] = my_df[\"HDL\"] >= 60.0\n",
    "# END SOLUTION\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"HDL_healthy\" in my_df.columns, \"my_df should have 'HDL_healthy' column\"\n",
    "assert my_df[\"HDL_healthy\"].dtype == bool, \"HDL_healthy should be boolean type\"\n",
    "assert my_df[\"HDL_healthy\"].any(), \"Some samples should have HDL_healthy=True\"\n",
    "assert not my_df[\"HDL_healthy\"].all(), \"Some samples should have HDL_healthy=False\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check that the binary variable is correctly computed\n",
    "expected_healthy = my_df[\"HDL\"] >= 60.0\n",
    "assert (my_df[\"HDL_healthy\"] == expected_healthy).all(), \"HDL_healthy should be True when HDL >= 60\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Split Data into Training and Test Sets\n",
    "\n",
    "Separate the data into training and testing splits. Use an 80%/20% split with `random_state=8`.\n",
    "\n",
    "Store the results in variables named `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "**Hint:** Use `sklearn.model_selection.train_test_split()`. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Identify predictor columns (exclude SEQN, HDL, and HDL_healthy)\n",
    "predictor_cols = [col for col in my_df.columns if col not in [\"SEQN\", \"HDL\", \"HDL_healthy\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    my_df[predictor_cols],\n",
    "    my_df[\"HDL_healthy\"],\n",
    "    test_size=0.2,\n",
    "    random_state=8,\n",
    ")\n",
    "# END SOLUTION\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"X_train\" in dir(), \"Variable 'X_train' not defined\"\n",
    "assert \"X_test\" in dir(), \"Variable 'X_test' not defined\"\n",
    "assert \"y_train\" in dir(), \"Variable 'y_train' not defined\"\n",
    "assert \"y_test\" in dir(), \"Variable 'y_test' not defined\"\n",
    "assert len(X_train) > len(X_test), \"Training set should be larger than test set\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "total_size = len(X_train) + len(X_test)\n",
    "test_ratio = len(X_test) / total_size\n",
    "assert 0.15 < test_ratio < 0.25, f\"Test set should be ~20% of total, got {test_ratio:.1%}\"\n",
    "assert len(X_train) == len(y_train), \"X_train and y_train should have same length\"\n",
    "assert len(X_test) == len(y_test), \"X_test and y_test should have same length\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Train a Logistic Regression Model\n",
    "\n",
    "Train a logistic regression model on the training data. Store the trained model in a variable named `model`.\n",
    "\n",
    "**Hint:** Use `sklearn.linear_model.LogisticRegression()`. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"model\" in dir(), \"Variable 'model' not defined\"\n",
    "assert hasattr(model, \"predict\"), \"model should have a predict method\"\n",
    "assert hasattr(model, \"predict_proba\"), \"model should have a predict_proba method\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(model, \"coef_\"), \"model should be fitted (have coef_ attribute)\"\n",
    "assert model.coef_.shape[1] == X_train.shape[1], \"model should have correct number of coefficients\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 8:** Generate Predictions\n",
    "\n",
    "Generate both the predicted probabilities and the default binary predictions for the test set.\n",
    "\n",
    "Store the predicted probabilities in a variable named `predicted_probs` and the binary predictions in a variable named `predicted_hdl_healthy`.\n",
    "\n",
    "**Hint:** Use `model.predict_proba()` for probabilities and `model.predict()` for binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "predicted_probs = model.predict_proba(X_test)\n",
    "predicted_hdl_healthy = model.predict(X_test)\n",
    "# END SOLUTION\n",
    "print(f\"Average predicted probability: {predicted_probs.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"predicted_probs\" in dir(), \"Variable 'predicted_probs' not defined\"\n",
    "assert \"predicted_hdl_healthy\" in dir(), \"Variable 'predicted_hdl_healthy' not defined\"\n",
    "assert predicted_probs.shape[0] == len(X_test), \"predicted_probs should have same length as X_test\"\n",
    "assert predicted_probs.shape[1] == 2, \"predicted_probs should have 2 columns (one per class)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(predicted_hdl_healthy) == len(\n",
    "    X_test\n",
    "), \"predicted_hdl_healthy should have same length as X_test\"\n",
    "assert np.allclose(\n",
    "    predicted_probs.sum(axis=1), 1.0\n",
    "), \"Probabilities should sum to 1 for each sample\"\n",
    "assert predicted_hdl_healthy.dtype in (\n",
    "    bool,\n",
    "    np.bool_,\n",
    "), \"Binary predictions should be boolean type\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 9:** Evaluate with sklearn's ROC\n",
    "\n",
    "Use sklearn's `roc_curve` and `auc` functions to evaluate the logistic regression model. This will serve as a sanity check that our own implementation (in the next problem) is close to accurate.\n",
    "\n",
    "Store the results in variables named `fpr`, `tpr`, `thresholds`, and `roc_auc`.\n",
    "\n",
    "**Hint:** See the [roc_curve documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and [auc documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Compute ROC curve using sklearn\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predicted_probs[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# END SOLUTION\n",
    "\n",
    "print(f\"AUC: {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"fpr\" in dir(), \"Variable 'fpr' not defined\"\n",
    "assert \"tpr\" in dir(), \"Variable 'tpr' not defined\"\n",
    "assert \"roc_auc\" in dir(), \"Variable 'roc_auc' not defined\"\n",
    "assert 0 <= roc_auc <= 1, \"AUC should be between 0 and 1\"\n",
    "assert roc_auc > 0.5, \"AUC should be better than random (> 0.5)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(fpr) == len(tpr), \"fpr and tpr should have same length\"\n",
    "assert fpr[0] == 0, \"FPR should start at 0\"\n",
    "assert tpr[0] == 0, \"TPR should start at 0\"\n",
    "assert fpr[-1] == 1, \"FPR should end at 1\"\n",
    "assert tpr[-1] == 1, \"TPR should end at 1\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 10:** Implement ROC from Scratch\n",
    "\n",
    "Implement your own ROC function and evaluate your logistic regression on it. You may **not** wrap functions that already compute ROC (e.g., `sklearn.metrics.roc_curve`). You may use `sklearn.metrics.confusion_matrix` to compute the confusion matrix for each threshold.\n",
    "\n",
    "Your function should:\n",
    "1. Take `y_true` (true labels) and `predicted_probs_positive` (predicted probabilities for the positive class) as inputs\n",
    "2. Use at least 1000 thresholds evenly spaced between 0 and 1\n",
    "3. For each threshold, compute the FPR and TPR from the confusion matrix\n",
    "4. Return lists of FPR values, TPR values, and thresholds\n",
    "\n",
    "**Recall:**\n",
    "- TPR (True Positive Rate) = TP / (TP + FN)\n",
    "- FPR (False Positive Rate) = FP / (FP + TN)\n",
    "\n",
    "Store your results in `my_fprs`, `my_tprs`, and `my_thresholds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def compute_roc(y_true, predicted_probs_positive):\n",
    "    \"\"\"\n",
    "    Compute the ROC curve from scratch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        The true labels of the test set.\n",
    "    predicted_probs_positive : array-like\n",
    "        The predicted probabilities of the positive class for the test set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fpr_list : list\n",
    "        A list of false positive rates for each threshold.\n",
    "    tpr_list : list\n",
    "        A list of true positive rates for each threshold.\n",
    "    thresholds : ndarray\n",
    "        The thresholds used to calculate the rates.\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Create 1000 evenly spaced thresholds from 0 to 1\n",
    "    thresholds = np.linspace(0, 1, 1000)\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Create binary predictions based on threshold\n",
    "        y_pred = predicted_probs_positive > threshold\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculate FPR and TPR\n",
    "        current_fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        current_tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        fpr_list.append(current_fpr)\n",
    "        tpr_list.append(current_tpr)\n",
    "\n",
    "    return fpr_list, tpr_list, thresholds\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "my_fprs, my_tprs, my_thresholds = compute_roc(y_test, predicted_probs[:, 1])\n",
    "\n",
    "# Plot the ROC curve\n",
    "my_roc_auc = auc(my_fprs, my_tprs)\n",
    "print(f\"AUC (custom implementation): {my_roc_auc:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(my_fprs, my_tprs, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {my_roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) - Custom Implementation\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"my_fprs\" in dir(), \"Variable 'my_fprs' not defined\"\n",
    "assert \"my_tprs\" in dir(), \"Variable 'my_tprs' not defined\"\n",
    "assert \"my_thresholds\" in dir(), \"Variable 'my_thresholds' not defined\"\n",
    "assert len(my_thresholds) >= 1000, \"Should use at least 1000 thresholds\"\n",
    "assert (\n",
    "    len(my_fprs) == len(my_tprs) == len(my_thresholds)\n",
    "), \"FPR, TPR, and thresholds should have same length\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check that the custom AUC is close to sklearn's AUC\n",
    "custom_auc = auc(my_fprs, my_tprs)\n",
    "assert (\n",
    "    abs(custom_auc - roc_auc) < 0.05\n",
    "), f\"Custom AUC ({custom_auc:.3f}) should be close to sklearn AUC ({roc_auc:.3f})\"\n",
    "# Check that FPR and TPR are in valid range\n",
    "assert all(0 <= f <= 1 for f in my_fprs), \"All FPR values should be between 0 and 1\"\n",
    "assert all(0 <= t <= 1 for t in my_tprs), \"All TPR values should be between 0 and 1\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 11:** Find the Optimal Threshold\n",
    "\n",
    "A common approach for selecting a final threshold is to find the point on the ROC curve that is closest to the ideal point (0, 1), which represents perfect classification (FPR=0, TPR=1).\n",
    "\n",
    "Write code that finds this optimal threshold and the corresponding FPR and TPR values. Store them in variables named `optimal_threshold`, `optimal_fpr`, and `optimal_tpr`.\n",
    "\n",
    "Then, plot the ROC curve with a marker at the optimal point.\n",
    "\n",
    "**Hint:** The Euclidean distance from a point (fpr, tpr) to (0, 1) is: $\\sqrt{fpr^2 + (1 - tpr)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Convert lists to arrays for easier computation\n",
    "fpr_array = np.array(my_fprs)\n",
    "tpr_array = np.array(my_tprs)\n",
    "\n",
    "# Compute distance from each point to (0, 1)\n",
    "distances = np.sqrt(fpr_array**2 + (1 - tpr_array) ** 2)\n",
    "\n",
    "# Find the index of the minimum distance\n",
    "min_index = np.argmin(distances)\n",
    "\n",
    "# Extract optimal values\n",
    "optimal_threshold = my_thresholds[min_index]\n",
    "optimal_fpr = my_fprs[min_index]\n",
    "optimal_tpr = my_tprs[min_index]\n",
    "# END SOLUTION\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"Optimal FPR: {optimal_fpr:.4f}\")\n",
    "print(f\"Optimal TPR: {optimal_tpr:.4f}\")\n",
    "\n",
    "# Plot the ROC curve with the optimal point\n",
    "plt.figure()\n",
    "plt.plot(my_fprs, my_tprs, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {my_roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.plot(\n",
    "    optimal_fpr,\n",
    "    optimal_tpr,\n",
    "    \"ro\",\n",
    "    markersize=10,\n",
    "    label=f\"Optimal point (t={optimal_threshold:.3f})\",\n",
    ")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve with Optimal Threshold\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert \"optimal_threshold\" in dir(), \"Variable 'optimal_threshold' not defined\"\n",
    "assert \"optimal_fpr\" in dir(), \"Variable 'optimal_fpr' not defined\"\n",
    "assert \"optimal_tpr\" in dir(), \"Variable 'optimal_tpr' not defined\"\n",
    "assert 0 <= optimal_threshold <= 1, \"Optimal threshold should be between 0 and 1\"\n",
    "assert 0 <= optimal_fpr <= 1, \"Optimal FPR should be between 0 and 1\"\n",
    "assert 0 <= optimal_tpr <= 1, \"Optimal TPR should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check that the optimal point is actually the closest to (0, 1)\n",
    "optimal_distance = np.sqrt(optimal_fpr**2 + (1 - optimal_tpr) ** 2)\n",
    "for i in range(len(my_fprs)):\n",
    "    dist = np.sqrt(my_fprs[i] ** 2 + (1 - my_tprs[i]) ** 2)\n",
    "    assert (\n",
    "        dist >= optimal_distance - 1e-10\n",
    "    ), \"Found a point closer to (0,1) than the 'optimal' point\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
