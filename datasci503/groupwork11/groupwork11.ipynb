{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADqNXD0RkttC"
   },
   "source": [
    "# DATASCI 503, Group Work 11: Building and Tuning Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wAlCaLikph7"
   },
   "source": [
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!** Upon completion, one member of the team should submit their team's work through Canvas **as html**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743786006156,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "qWKbpMSBOZd4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIMDfcbVzzgk"
   },
   "source": [
    "# PLEASE USE RANDOM STATE 42 FOR THIS ASSIGNMENT\n",
    "\n",
    "# **NOTE**: This assignment requires training multiple neural networks for several epochs. Please ensure you provide yourself with ample time to run the full notebook from start to finish and generate all outputs before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tt1uAONQSVZn"
   },
   "source": [
    "### Problem 1a: Activation Functions\n",
    "\n",
    "In 1-2 sentences, explain what an activation function is and why it is useful for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8m1lseWS1mR"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "An activation function introduces non-linearity into a neural network by transforming the output of each neuron. Without activation functions, a neural network would only be able to learn linear relationships regardless of its depth, since composing linear functions still produces a linear function.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3nhl4KoS5NC"
   },
   "source": [
    "### Problem 1b: List Activation Functions\n",
    "\n",
    "List 5 different activation functions and explain why each might be useful in neural networks. Use the following format: Name, Formula: Use Case.\n",
    "\n",
    "1. Tanh, $\\tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x})$: The tanh function is a smooth, differentiable everywhere function bounded between -1, 1.\n",
    "2. List item\n",
    "3. List item\n",
    "4. List item\n",
    "5. List item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "1. Tanh, $\\tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x})$: The tanh function is a smooth, differentiable everywhere function bounded between -1, 1.\n",
    "2. ReLU, $\\text{ReLU}(x) = \\max(0, x)$: Fast to compute and helps mitigate the vanishing gradient problem; widely used in hidden layers.\n",
    "3. Sigmoid, $\\sigma(x) = 1 / (1 + e^{-x})$: Outputs values between 0 and 1, making it useful for binary classification output layers.\n",
    "4. Leaky ReLU, $\\text{LeakyReLU}(x) = \\max(\\alpha x, x)$ where $\\alpha$ is small (e.g., 0.01): Addresses the \"dying ReLU\" problem by allowing small negative gradients.\n",
    "5. Softmax, $\\text{softmax}(x_i) = e^{x_i} / \\sum_j e^{x_j}$: Converts a vector of logits into a probability distribution, useful for multi-class classification output layers.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frn4-4Z5kvLV"
   },
   "source": [
    "### Problem 1c: Understanding Loss Functions\n",
    "\n",
    "Write down the names and formulas of 2 loss functions (ideally the most widely used) for neural networks. One should be for classification and one should be for regression. Include one sentence explaining intuitively what quantities the model is trying to bring close together and/or why the function is useful to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg8rrnrOlJAl"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**CLASSIFICATION**: Cross-Entropy Loss, $L = -\\sum_{i} y_i \\log(\\hat{y}_i)$. This loss measures the difference between the predicted probability distribution and the true distribution, encouraging the model to assign high probability to the correct class.\n",
    "\n",
    "**REGRESSION**: Mean Squared Error (MSE), $L = \\frac{1}{n}\\sum_{i}(y_i - \\hat{y}_i)^2$. This loss measures the average squared difference between predicted and actual values, penalizing larger errors more heavily and encouraging predictions close to the true values.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr-Jt-acXX2Y"
   },
   "source": [
    "### Problem 2: Gradient Descent with Autograd\n",
    "\n",
    "Consider the simple regression problem given $X=(x_1,\\dots,x_n)$ as a vector of length $n$ and targets $Y=(y_1,\\dots,y_n)$. Use gradient descent to minimize the Mean Squared Error loss:\n",
    "\n",
    "$$f(w) = \\frac{1}{n}\\sum_i (y_i - w_0 - w_1 x_i)^2$$\n",
    "\n",
    "Write a function `mse_gd` that takes as input:\n",
    "- `X`: Input feature tensor\n",
    "- `y`: Target tensor  \n",
    "- `eta`: Step size (learning rate), default 0.1\n",
    "- `max_iter`: Maximum number of iterations, default 1000\n",
    "\n",
    "The function should return the learned weights `w` as a tensor of shape `(2,)` where `w[0]` is the intercept and `w[1]` is the slope.\n",
    "\n",
    "**Note:** This is similar to the problem from last week's groupwork, but now use PyTorch with autograd instead of NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1743786006168,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "gL7CQdPkWzz-"
   },
   "outputs": [],
   "source": [
    "def mse_gd(features, targets, eta=1e-1, max_iter=1000):\n",
    "    # BEGIN SOLUTION\n",
    "    # Randomly initialize weights from standard normal distribution\n",
    "    # weights[0] is intercept, weights[1] is slope\n",
    "    weights = torch.randn(2, requires_grad=True)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Compute MSE loss: mean of squared residuals\n",
    "        loss = torch.mean(torch.square(targets - weights[0] - weights[1] * features))\n",
    "\n",
    "        # Compute gradients via backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient descent step without tracking gradients\n",
    "        with torch.no_grad():\n",
    "            weights -= eta * weights.grad\n",
    "\n",
    "        # Reset gradients for next iteration\n",
    "        weights.grad.zero_()\n",
    "\n",
    "    return weights\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(10)\n",
    "X_test_gd = torch.rand(60)\n",
    "y_test_gd = 2.0 - 1.4 * X_test_gd + torch.randn(60) * 0.3\n",
    "\n",
    "weights_result = mse_gd(X_test_gd, y_test_gd)\n",
    "weights_np = weights_result.detach().numpy()\n",
    "\n",
    "assert weights_result.shape == (2,), f\"Expected shape (2,), got {weights_result.shape}\"\n",
    "assert abs(weights_np[0] - 2.0) < 0.3, f\"Intercept should be close to 2.0, got {weights_np[0]:.4f}\"\n",
    "assert abs(weights_np[1] - (-1.4)) < 0.3, f\"Slope should be close to -1.4, got {weights_np[1]:.4f}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "torch.manual_seed(42)\n",
    "X_hidden = torch.rand(100)\n",
    "y_hidden = 3.0 + 2.0 * X_hidden + torch.randn(100) * 0.1\n",
    "weights_hidden = mse_gd(X_hidden, y_hidden)\n",
    "w_hidden_np = weights_hidden.detach().numpy()\n",
    "assert abs(w_hidden_np[0] - 3.0) < 0.2, \"Intercept estimation failed on hidden test\"\n",
    "assert abs(w_hidden_np[1] - 2.0) < 0.2, \"Slope estimation failed on hidden test\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yBo_-lodh3c"
   },
   "source": [
    "Run the next cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1743786006694,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "wwSfVvg7apyz",
    "outputId": "11e9297a-983c-4592-8eba-ec84ccdd038b"
   },
   "outputs": [],
   "source": [
    "# NO NEED TO EDIT THIS CELL\n",
    "torch.manual_seed(10)\n",
    "X = torch.rand(60)\n",
    "y = 2.0 - 1.4 * X + torch.randn(60) * 0.3\n",
    "\n",
    "w = mse_gd(X, y)\n",
    "w = w.detach().numpy()  # need to detach tensors that require grad before calling numpy\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot([0, 1], [2, 0.6], color=\"red\", linestyle=\"dashed\")  # true line in red\n",
    "plt.plot([0, 1], [w[0], w[0] + w[1]], color=\"blue\", linestyle=\"dashed\")  # estimated line in blue\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzzUwTxuV_eY"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "**Tips and Tricks for Training Your NNs**\n",
    "\n",
    "*   **Start Small**: Models wiith fewer trainable parameters have less of an opportunity to overfit.\n",
    "*   **Try a Wide Variety of Settings**: There are so many activation functions, widths, depths, network types, and regularization techniques at your disposal that sometimes you need to explore around with all combinations to get a sense of what works and what doesn't.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58Kc7oSjvfDq"
   },
   "source": [
    "### Problem 3: Working with DataLoaders\n",
    "\n",
    "To work effectively with PyTorch, we use the `DataLoader` class, which provides:\n",
    "\n",
    "1. Automatic Batching\n",
    "2. Sample Shuffling\n",
    "3. Parallel Data Loading\n",
    "4. Parallel Data Transformations\n",
    "\n",
    "Using the Higgs dataset loaded below, perform the following operations:\n",
    "\n",
    "1. Split the data into train and test splits (80/20) using stratification.\n",
    "2. Create validation data from the train split to create train and validation splits (80/20).\n",
    "3. Standard scale the training set and use the **same** scaling for both the validation and test sets.\n",
    "4. Convert each split into a `TensorDataset`.\n",
    "5. Create a `DataLoader` object for each split with an appropriate batch size (use a power of 2, such as 512).\n",
    "\n",
    "**Hint:** You should use a single `StandardScaler` object, fitting only on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743786006694,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "9qxLA_-_fP4w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1743786008587,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "Qhbu6oPEwp7d",
    "outputId": "7bf6728f-af46-4d74-e630-daf91c9a53e6"
   },
   "outputs": [],
   "source": [
    "# NO NEED TO CHANGE THIS CELL\n",
    "\n",
    "NUM_SAMPLES = 50_000\n",
    "\n",
    "df = pd.read_csv(\"data/higgs.csv\", index_col=\"EventId\")\n",
    "df = df.sample(n=NUM_SAMPLES, random_state=42)\n",
    "\n",
    "# First column is the binary target (0 or 1), rest are features\n",
    "df.columns = [f\"feature_{i}\" for i in range(1, df.shape[1])] + [\"is_higgs_signal\"]\n",
    "\n",
    "df[\"is_higgs_signal\"] = (df[\"is_higgs_signal\"] == \"s\").astype(int)\n",
    "\n",
    "# Split into features and target\n",
    "X = df.drop(\"is_higgs_signal\", axis=1)\n",
    "y = df[\"is_higgs_signal\"]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1743786008647,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "o9T7uTvrymhy",
    "outputId": "78dc99a6-03f6-4258-ba63-4d7253fceeab"
   },
   "outputs": [],
   "source": [
    "# NO NEED TO CHANGE THIS CELL\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1743786008662,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "ObsYomQgyn0V",
    "outputId": "98faedc0-593c-40e6-f46f-c9b968823beb"
   },
   "outputs": [],
   "source": [
    "# NO NEED TO CHANGE THIS CELL\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1743786008933,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "97cAVJLpvesJ"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Split data into training and testing sets (80/20) with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
    ")\n",
    "\n",
    "# Split training data into train and validation sets (80/20) with stratification\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, shuffle=True, stratify=y_train\n",
    ")\n",
    "\n",
    "# Scale the data using a single scaler fit on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors (features as float32, labels as long for CrossEntropyLoss)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_validation = torch.tensor(X_validation, dtype=torch.float32)\n",
    "y_validation = torch.tensor(y_validation.values, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset for each split\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "validation_dataset = torch.utils.data.TensorDataset(X_validation, y_validation)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders with batch size of 512 (power of 2 for GPU efficiency)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=True)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify data split sizes:\n",
    "# 50000 * 0.8 * 0.8 = 32000 train, 50000 * 0.8 * 0.2 = 8000 val, 50000 * 0.2 = 10000 test\n",
    "assert len(train_dataset) == 32000, f\"Expected 32000 training samples, got {len(train_dataset)}\"\n",
    "assert (\n",
    "    len(validation_dataset) == 8000\n",
    "), f\"Expected 8000 validation samples, got {len(validation_dataset)}\"\n",
    "assert len(test_dataset) == 10000, f\"Expected 10000 test samples, got {len(test_dataset)}\"\n",
    "\n",
    "# Verify tensor types\n",
    "assert X_train.dtype == torch.float32, \"X_train should be float32\"\n",
    "assert y_train.dtype == torch.long, \"y_train should be long\"\n",
    "\n",
    "# Verify DataLoader batch size\n",
    "assert train_loader.batch_size == 512, f\"Expected batch size 512, got {train_loader.batch_size}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify scaling was applied (training data should have mean close to 0)\n",
    "assert abs(X_train.mean().item()) < 0.1, \"Training data should be approximately zero-centered\"\n",
    "# Verify feature dimensions are preserved\n",
    "assert X_train.shape[1] == 31, \"Should have 31 features\"\n",
    "# Verify labels are binary\n",
    "assert set(y_train.unique().tolist()) == {0, 1}, \"Labels should be binary (0, 1)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1743786008948,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "aBlYH-WBtBp5",
    "outputId": "8ed24759-2957-4225-b473-fc4b8fd80295"
   },
   "outputs": [],
   "source": [
    "len(validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arO0VAOtttgv"
   },
   "source": [
    "### Problem 4: Training a Model\n",
    "\n",
    "We have provided a commented `train_model` function for you. Review the function to understand each argument and step.\n",
    "\n",
    "Then, create a small neural network with one hidden layer of width 32 and train it for 100 epochs using the provided training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1743786008988,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "xRCGwU3YH5sT"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=0):\n",
    "    \"\"\"\n",
    "    model (torch.nn.Module): The neural network\n",
    "    train_loader (torch.utils.data.DataLoader): Training data\n",
    "    val_loader (torch.utils.data.DataLoader): Validation data\n",
    "    criterion (torch.nn Loss function): The loss function\n",
    "    optimizer (torch.optim): The optimizer used to make gradient steps\n",
    "    num_epochs (int): The number of training epochs to perform\n",
    "    patience (int): Number of epochs for early stopping. If 0, early stopping is not used.\n",
    "    \"\"\"\n",
    "\n",
    "    # intiate a way to keep track of training and validation losses\n",
    "    validation_losses = []\n",
    "    training_losses = []\n",
    "\n",
    "    min_validation_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # for each epoch...\n",
    "    for epoch in range(num_epochs):\n",
    "        # set the model to train mode\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # new iteration so we set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            # get outputs from the model (forward pass)\n",
    "            outputs = model(batch_X)\n",
    "            # get the loss using the outputs and the truth\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            # compute gradients of the loss with respect to model parameters (backward pass)\n",
    "            loss.backward()\n",
    "            # take a step with optimizer to update model parameters using computed gradients\n",
    "            optimizer.step()\n",
    "            # add to the epoch's runnning training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # get the mean training loss\n",
    "        train_loss /= len(train_loader)\n",
    "        # append training loss observation to train lost list\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        # we want to evaluate model losses on validation set so we set model\n",
    "        # to eval mode here\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        # we also don't want to do backpropagation while not training so we\n",
    "        # turn off gradient\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                # val forward pass\n",
    "                outputs = model(batch_X)\n",
    "                # val loss function\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                # add to total running validation loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # get the mean training loss\n",
    "        val_loss /= len(val_loader)\n",
    "        if patience:\n",
    "            if val_loss < min_validation_loss:\n",
    "                min_validation_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        # append training loss observation to train lost list\n",
    "        validation_losses.append(val_loss)\n",
    "        # once in a while...\n",
    "        if epoch % 10 == 0:\n",
    "            # Print epoch progress, training loss, and validation loss\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    return training_losses, validation_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63759,
     "status": "ok",
     "timestamp": 1743786072755,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "VA2FhRjWp9BF",
    "outputId": "e5a7c8c2-e819-4f8c-a6c9-621e2c9d54e1"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Create a simple neural network with one hidden layer of width 32\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 2),\n",
    ")\n",
    "\n",
    "# Use CrossEntropyLoss for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use Adam optimizer with learning rate 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train for 100 epochs\n",
    "training_losses, validation_losses, trained_model = train_model(\n",
    "    model, train_loader, validation_loader, criterion, optimizer, num_epochs=100\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(model, nn.Module), \"model should be an nn.Module\"\n",
    "assert isinstance(criterion, nn.Module), \"criterion should be a loss function\"\n",
    "assert isinstance(optimizer, optim.Optimizer), \"optimizer should be an Optimizer\"\n",
    "assert len(training_losses) > 0, \"training_losses should have entries after training\"\n",
    "assert len(validation_losses) > 0, \"validation_losses should have entries after training\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses) == 100, \"Should train for 100 epochs\"\n",
    "assert all(loss >= 0 for loss in training_losses), \"Training losses should be non-negative\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1743786072926,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "IBaRDpJh_l8q",
    "outputId": "6ea24788-2cba-4d50-9273-085857036f0e"
   },
   "outputs": [],
   "source": [
    "# CHECK HOW THE MODEL DID\n",
    "# NO CHANGES NECESSARY HERE\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIITUFmJuCkp"
   },
   "source": [
    "## Part 5: Comparing Model Architectures\n",
    "\n",
    "We will observe how the loss trajectories evolve as we increase the size of the models. As we increase the model's size, a divergence between the training and the validation loss will appear. In very large models, validation loss will increase despite the falling training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6JDv12scLTI"
   },
   "source": [
    "### Problem 5a: Model Building Function\n",
    "\n",
    "Write a function called `build_model` that takes in the following arguments:\n",
    "\n",
    "1. `n_hidden` (int): The number of hidden layers to use.\n",
    "2. `n_neurons` (int): The width of each hidden layer.\n",
    "3. `activation` (str): A string indicating the activation function to use (e.g., 'relu', 'tanh', 'sigmoid').\n",
    "4. `input_dim` (int): The input dimension of your data.\n",
    "5. `output_dim` (int): The output dimension of your data.\n",
    "6. `include_dropout` (bool or list): Whether to include dropout layers. If a list, use the dropout rates per layer.\n",
    "\n",
    "The function should return a neural network (`nn.Sequential`) with these properties, ready to be trained with your `train_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1743786072929,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "_XOOCbhlcPf1"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    n_hidden=1, n_neurons=30, activation=\"relu\", input_dim=0, output_dim=0, include_dropout=False\n",
    "):\n",
    "    # BEGIN SOLUTION\n",
    "    # Map activation names to PyTorch activation classes\n",
    "    activation_map = {\n",
    "        \"relu\": nn.ReLU,\n",
    "        \"tanh\": nn.Tanh,\n",
    "        \"sigmoid\": nn.Sigmoid,\n",
    "        \"leaky_relu\": nn.LeakyReLU,\n",
    "    }\n",
    "\n",
    "    # Get activation function class, default to ReLU if not found\n",
    "    activation_fn = activation_map.get(activation.lower(), nn.ReLU)\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # Build hidden layers\n",
    "    for layer_idx in range(n_hidden):\n",
    "        # First layer takes input_dim, subsequent layers take n_neurons\n",
    "        in_features = input_dim if layer_idx == 0 else n_neurons\n",
    "        layers.append(nn.Linear(in_features, n_neurons))\n",
    "        layers.append(activation_fn())\n",
    "\n",
    "        # Add dropout if specified\n",
    "        if include_dropout:\n",
    "            layers.append(nn.Dropout(include_dropout[layer_idx]))\n",
    "\n",
    "    # Add output layer\n",
    "    layers.append(nn.Linear(n_neurons, output_dim))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_model = build_model(n_hidden=2, n_neurons=16, activation=\"relu\", input_dim=10, output_dim=2)\n",
    "\n",
    "# Check that model is nn.Sequential\n",
    "assert isinstance(test_model, nn.Sequential), \"Model should be an nn.Sequential\"\n",
    "\n",
    "# Check number of layers (2 hidden layers * 2 (linear + activation) + 1 output = 5)\n",
    "assert len(test_model) == 5, f\"Expected 5 layers, got {len(test_model)}\"\n",
    "\n",
    "# Check input/output dimensions\n",
    "test_input = torch.randn(4, 10)\n",
    "test_output = test_model(test_input)\n",
    "assert test_output.shape == (4, 2), f\"Expected output shape (4, 2), got {test_output.shape}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with dropout\n",
    "dropout_model = build_model(\n",
    "    n_hidden=3,\n",
    "    n_neurons=32,\n",
    "    activation=\"tanh\",\n",
    "    input_dim=20,\n",
    "    output_dim=5,\n",
    "    include_dropout=[0.2, 0.3, 0.4],\n",
    ")\n",
    "assert (\n",
    "    len(dropout_model) == 10\n",
    "), \"Model with dropout should have 10 layers (3*(linear+activation+dropout) + output)\"\n",
    "\n",
    "# Verify activation type\n",
    "assert isinstance(dropout_model[1], nn.Tanh), \"Should use Tanh activation when specified\"\n",
    "\n",
    "# Test forward pass with dropout model\n",
    "dropout_input = torch.randn(8, 20)\n",
    "dropout_output = dropout_model(dropout_input)\n",
    "assert dropout_output.shape == (8, 5), \"Dropout model output shape incorrect\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-zThl4rGvsA"
   },
   "source": [
    "### Problem 5b: Evaluating Neural Network Architectures\n",
    "\n",
    "Using your `build_model` method, try out at least 2 depths, 2 widths, and 2 activations (8 total models). Record the final validation loss of each model using a dictionary. Print out the test loss of the best performing model, and in the markdown cell below, describe which model performed best and report its test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481031,
     "status": "ok",
     "timestamp": 1743786553961,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "V3AoLQoUcMd-",
    "outputId": "3c74d917-65ea-49c1-a8ae-264f80b87017"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Architecture parameters to test\n",
    "num_layers_list = [3, 5]\n",
    "num_neurons_list = [16, 32]\n",
    "activations_list = [\"relu\", \"tanh\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dictionary to store performance results\n",
    "model_performance = {}\n",
    "lowest_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "# Try all combinations of architectures\n",
    "for n_hidden in num_layers_list:\n",
    "    for n_neurons in num_neurons_list:\n",
    "        for activation in activations_list:\n",
    "            print(f\"Training: {n_hidden} layers, {n_neurons} neurons, {activation} activation\")\n",
    "\n",
    "            model = build_model(\n",
    "                n_hidden, n_neurons, activation, input_dim=X_train.shape[1], output_dim=2\n",
    "            ).to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            train_losses, val_losses, trained_model = train_model(\n",
    "                model, train_loader, validation_loader, criterion, optimizer, num_epochs=100\n",
    "            )\n",
    "\n",
    "            # Record performance\n",
    "            model_performance[(n_hidden, n_neurons, activation)] = val_losses[-1]\n",
    "            if val_losses[-1] < lowest_val_loss:\n",
    "                lowest_val_loss = val_losses[-1]\n",
    "                best_model = model\n",
    "\n",
    "            print(f\"Final Val Loss: {val_losses[-1]:.6f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1743786553990,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "lyATeuixa_yz",
    "outputId": "87e6ced0-983e-464b-abf1-059d69454dec"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Find best model configuration\n",
    "best_config = min(model_performance, key=model_performance.get)\n",
    "print(f\"Best model configuration: {best_config}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss = criterion(best_model(X_test), y_test)\n",
    "print(f\"Test loss: {test_loss.item():.6f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElGsyDUvHp2M"
   },
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "My best performing model was a neural network with 5 hidden layers, 16 neurons per layer, and ReLU activation. It achieved a test loss of approximately 0.0058.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASdv7nsgEFhx"
   },
   "source": [
    "## Part 6: Adding Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rHoVWcswFLa"
   },
   "source": [
    "### Problem 6a: Weight Decay\n",
    "\n",
    "Pick a deep model of your choosing (using `build_model`) and train it with an optimizer that includes weight decay (L2 regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRxWepNawbBK"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "You may be familiar with Occam's Razor principle: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given 2 networks that achieve a similar performance, go with the simpler one! Less risk of overfitting!\n",
    "\n",
    "* [L1 regularization](https://developers.google.com/machine-learning/glossary/#L1_regularization), where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights). L1 regularization produces sparse models because a decrease of 0.1 -> 0 is as penalty reducing as 10.1 -> 10.0 .\n",
    "\n",
    "* [L2 regularization](https://developers.google.com/machine-learning/glossary/#L2_regularization), where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the squared \"L2 norm\" of the weights). L2 regularization is also called **weight decay** in the context of neural networks.\n",
    "\n",
    "Here, we apply L2 regularization using the `weight_decay` feature of PyTorch.\n",
    "Feel free to read the [article](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and adjust the `weight_decay` parameter to remove overfitting in the medium size model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Pick a deep model of your choosing (using `build_model`) and train it with an optimizer that has weight_decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(model, nn.Module), \"model should be an nn.Module\"\n",
    "assert isinstance(optimizer, optim.Adam), \"optimizer should be Adam\"\n",
    "assert optimizer.defaults.get(\"weight_decay\", 0) > 0, \"optimizer should have weight_decay > 0\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(model, \"forward\"), \"model should have forward method\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1743786553997,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "HFGmcwduwVyQ"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Build a deep model\n",
    "model = build_model(\n",
    "    n_hidden=5, n_neurons=64, activation=\"relu\", input_dim=X_train.shape[1], output_dim=2\n",
    ")\n",
    "\n",
    "# Use CrossEntropyLoss and Adam optimizer with weight decay\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220256,
     "status": "ok",
     "timestamp": 1743786774254,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "AqLEdb7HomOk",
    "outputId": "136b1ed5-318d-490f-894e-9f66546cf5c1"
   },
   "outputs": [],
   "source": [
    "training_losses, validation_losses, trained_model = train_model(\n",
    "    model, train_loader, validation_loader, criterion, optimizer, num_epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1743786774619,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "VefUaGa_pEn4",
    "outputId": "6719be44-6964-40af-b462-28dcee2fa31d"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(model, nn.Module), \"model should be an nn.Module\"\n",
    "# Check that model contains dropout layers\n",
    "has_dropout = any(isinstance(layer, nn.Dropout) for layer in model.modules())\n",
    "assert has_dropout, \"model should contain Dropout layers\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "dropout_layers = [layer for layer in model.modules() if isinstance(layer, nn.Dropout)]\n",
    "assert len(dropout_layers) == n_hidden, f\"Should have {n_hidden} dropout layers\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmnBNOOVxiG8"
   },
   "source": [
    "### Problem 6b: Dropout\n",
    "\n",
    "Your `build_model` function already supports dropout via the `include_dropout` parameter. Use it to build a model with dropout layers, specifying dropout rates that increase with depth (lower near input, higher in deeper layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743786774624,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "OFEYvtrHxSWS"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Define dropout rates that increase with depth\n",
    "n_hidden = 5\n",
    "dropout_rates = [0.2, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Build model with dropout\n",
    "model = build_model(\n",
    "    n_hidden=n_hidden,\n",
    "    n_neurons=64,\n",
    "    activation=\"relu\",\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=2,\n",
    "    include_dropout=dropout_rates,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165363,
     "status": "ok",
     "timestamp": 1743786939988,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "SPZqwVchx5xp",
    "outputId": "8dc69e42-f166-43a5-fabd-01f8d7febec8"
   },
   "outputs": [],
   "source": [
    "training_losses, validation_losses, trained_model = train_model(\n",
    "    model, train_loader, validation_loader, criterion, optimizer, num_epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(model, nn.Module), \"model should be an nn.Module\"\n",
    "assert isinstance(criterion, nn.Module), \"criterion should be a loss function\"\n",
    "assert isinstance(optimizer, optim.Optimizer), \"optimizer should be an Optimizer\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(model, \"forward\"), \"model should have forward method\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1743786940290,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "AOOeEeC_ppAC",
    "outputId": "55c44c7e-4b92-47a0-dd67-3649e5f513e7"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTC06k_OYe2S"
   },
   "source": [
    "### Problem 6c: Early Stopping\n",
    "\n",
    "The `train_model` function already includes early stopping via the `patience` parameter. Demonstrate early stopping by training a model with `patience=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743786940293,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "Wv9dGs9FW9Z-"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Build a model without dropout or weight decay to demonstrate early stopping\n",
    "model = build_model(\n",
    "    n_hidden=5, n_neurons=64, activation=\"relu\", input_dim=X_train.shape[1], output_dim=2\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(model, nn.Module), \"model should be an nn.Module\"\n",
    "assert isinstance(optimizer, optim.Adam), \"optimizer should be Adam\"\n",
    "# At least one regularization technique should be used\n",
    "uses_weight_decay = optimizer.defaults.get(\"weight_decay\", 0) > 0\n",
    "has_dropout = any(isinstance(layer, nn.Dropout) for layer in model.modules())\n",
    "assert uses_weight_decay or has_dropout, \"Should use at least one regularization technique\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(model, \"forward\"), \"model should have forward method\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16594,
     "status": "ok",
     "timestamp": 1743786956886,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "XG86r0P1XWjf",
    "outputId": "3d707bca-a417-4187-aff1-e202a8128174"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Train with early stopping (patience=5)\n",
    "training_losses, validation_losses, trained_model = train_model(\n",
    "    model, train_loader, validation_loader, criterion, optimizer, num_epochs=200, patience=5\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1743786957049,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "J7teQgwKZ0ax",
    "outputId": "4b1a8b25-7355-457f-f578-8381e10a25d6"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7qMg_7Nwy5t"
   },
   "source": [
    "### Problem 6d: Putting It All Together\n",
    "\n",
    "Choose any combination of at least 2 regularization strategies (weight decay, dropout, early stopping) and train a model for up to 500 epochs. The model should stop early if using early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743786957052,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "7zfs_qQIw1cz"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Combine weight decay and early stopping\n",
    "model = build_model(\n",
    "    n_hidden=5, n_neurons=64, activation=\"relu\", input_dim=X_train.shape[1], output_dim=2\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52720,
     "status": "ok",
     "timestamp": 1743787009773,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "ihJNDs3Lph7J",
    "outputId": "32a18609-11a4-4c5e-94fd-28dc9a3d3eaf"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Train with early stopping and weight decay\n",
    "training_losses, validation_losses, trained_model = train_model(\n",
    "    model, train_loader, validation_loader, criterion, optimizer, num_epochs=500, patience=10\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(model, nn.Module), \"model should be an nn.Module\"\n",
    "assert isinstance(optimizer, optim.Adam), \"optimizer should be Adam\"\n",
    "# Training should have used early stopping\n",
    "assert len(training_losses) < 500, \"Training should have stopped early with patience\"\n",
    "# At least one regularization technique should be used\n",
    "uses_weight_decay = optimizer.defaults.get(\"weight_decay\", 0) > 0\n",
    "has_dropout = any(isinstance(layer, nn.Dropout) for layer in model.modules())\n",
    "assert uses_weight_decay or has_dropout, \"Should use at least one regularization technique\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(model, \"forward\"), \"model should have forward method\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1743787010086,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "qDqBBxfI0Yd8",
    "outputId": "c7f4ac17-4021-4588-9caf-014df27984f4"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743787010091,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "iEREwt81Z3qr",
    "outputId": "d2f84d1f-a0c1-447b-d080-aef3689d431e"
   },
   "outputs": [],
   "source": [
    "validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1743787010094,
     "user": {
      "displayName": "Roman Kouznetsov",
      "userId": "17272545075189313587"
     },
     "user_tz": 240
    },
    "id": "rfKdeKUPENhJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
