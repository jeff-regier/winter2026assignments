{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 503, Homework 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is one of the most fundamental methods for binary classification. Unlike linear regression, which predicts continuous values, logistic regression models the probability that an observation belongs to a particular class using the logistic function. This assignment covers the mathematical foundations of logistic regression—including odds, log-odds, and the softmax generalization to multiple classes—along with practical skills in training classifiers and evaluating their performance using metrics like ROC curves and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Deriving the Odds from Logistic Regression\n",
    "\n",
    "Logistic regression models the probability as:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1+ e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "Show that the odds $\\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "$$\\require{cancel}$$  \n",
    "\n",
    "From the logistic regression formula:\n",
    "\n",
    "$$\n",
    "p(x)  = \\frac{e^{\\beta_0 + \\beta_1 X}}{1+ e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "1-p(x) = 1-\\frac{e^{\\beta_0 + \\beta_1 X}}{1+ e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1+ e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "\\frac{p(x)}{1-p(x)} = \\frac{e^{\\beta_0 + \\beta_1 X}/\\cancel{(1+ e^{\\beta_0 + \\beta_1 X}})}{1/\\cancel{(1+ e^{\\beta_0 + \\beta_1 X})}} = e^{\\beta_0 + \\beta_1 X}\n",
    "$$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2:** The Curse of Dimensionality in KNN\n",
    "\n",
    "Consider the K-nearest neighbors (KNN) algorithm where we use 10% of the available observations for each prediction. The features $X_1, X_2, \\ldots, X_p$ are independently and uniformly distributed on $[0,1]$.\n",
    "\n",
    "(a) For $p = 1$, what fraction of available observations will be used to make each prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "For $p = 1$, $X \\sim \\text{Uniform}[0,1]$. The fraction of the available observations used to make each prediction is always **10%**.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For $p = 2$, what fraction of available observations will be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "For $p = 2$, the fraction is $0.1 \\times 0.1 = 0.01$, or **1%**.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) For $p = 100$, what fraction will be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "For $p = 100$, the fraction is $0.1^{100}$, which is essentially **0%**.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) What is the drawback of KNN when $p$ is large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The drawback of KNN with large $p$ is that as $p$ increases, the fraction of available observations used for prediction approaches 0. KNN's simple idea that similar inputs lead to similar outputs is not powerful enough for high-dimensional problems.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) If we want to use 10% of the observations (by volume), what side length is needed for a hypercube in $p$ dimensions? Calculate for $p = 1, 2, 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Since Volume $= s^p = 0.1$, we have $s = 0.1^{1/p}$.\n",
    "- For $p = 1$: $s = 0.1$\n",
    "- For $p = 2$: $s \\approx 0.316$\n",
    "- For $p = 100$: $s \\approx 0.977$ (nearly the full range)\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3:** Odds and Probability Conversion\n",
    "\n",
    "(a) If the odds of an event are 0.37, what is the probability of the event?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Given: Odds $= \\frac{p}{1-p} = 0.37$. Solving: $p = \\frac{0.37}{1.37} \\approx 0.27$\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) If the probability of an event is 0.16, what are the odds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Given: $p = 0.16$. Odds $= \\frac{0.16}{0.84} \\approx 0.19$\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4:** Softmax vs Binary Logistic Regression\n",
    "\n",
    "Consider classifying fruit as either orange or apple based on a single predictor $X$.\n",
    "\n",
    "I fit a binary logistic regression model:\n",
    "$$\\log\\left(\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)}\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X$$\n",
    "\n",
    "with $\\hat{\\beta}_0 = 2$ and $\\hat{\\beta}_1 = -1$.\n",
    "\n",
    "My friend fits a softmax regression model:\n",
    "$$p(\\text{orange}|x) = \\frac{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x)}{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x) + \\exp(\\hat{\\alpha}_{\\text{apple},0} + \\hat{\\alpha}_{\\text{apple},1} x)}$$\n",
    "\n",
    "with $\\hat{\\alpha}_{\\text{orange},0} = 1.2$, $\\hat{\\alpha}_{\\text{orange},1} = -2$, $\\hat{\\alpha}_{\\text{apple},0} = 3$, $\\hat{\\alpha}_{\\text{apple},1} = 0.6$.\n",
    "\n",
    "(a) In my model, what is $\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "From the log-odds formulation: $\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)} = e^{\\beta_0 + \\beta_1 X}$\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) In my friend's model, derive an expression for $\\log\\left(\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Taking the ratio of probabilities and simplifying:\n",
    "$$\\log \\left( \\frac{p(\\text{orange} | x)}{p(\\text{apple}|x)} \\right) = (\\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0}) + (\\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1}) X$$\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Show the relationship between $\\hat{\\beta}_0, \\hat{\\beta}_1$ and my friend's $\\hat{\\alpha}$ coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "The relationship is: $\\hat{\\beta}_0 = \\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0}$ and $\\hat{\\beta}_1 = \\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1}$. With the given values, the friend's model gives different predictions since $1.2 - 3 = -1.8 \\neq 2$.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5:** Predicting Academic Success\n",
    "\n",
    "Suppose we are predicting whether a student gets an A in a class based on hours studied ($X_1$) and GPA ($X_2$). The logistic regression model is:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = -4 + 0.05 X_1 + X_2$$\n",
    "\n",
    "(a) A student with a GPA of 3.5 studies 5 hours per week. What is the probability they get an A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Log-odds $= -4 + 0.05(5) + 3.5 = -0.25$, so $p = \\frac{e^{-0.25}}{1+e^{-0.25}} \\approx 0.438$ or **43.8%**.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What are the odds this student gets an A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "Odds $= e^{-0.25} \\approx 0.779$\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) How many hours would this student need to study to have a 50% chance of getting an A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "For $p = 0.5$, log-odds $= 0$. Solving $0 = -4 + 0.05 X_1 + 3.5$ gives $X_1 = 10$ hours.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Problems: College Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_train = pd.read_csv(\"./data/college_train.csv\")\n",
    "college_test = pd.read_csv(\"./data/college_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = college_train.drop([\"Private\", \"Name\"], axis=1)\n",
    "Y_train = np.where(college_train[\"Private\"] == \"Yes\", 1, 0)\n",
    "X_test = college_test.drop([\"Private\", \"Name\"], axis=1)\n",
    "Y_test = np.where(college_test[\"Private\"] == \"Yes\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 6:** Training a Logistic Regression Classifier\n",
    "\n",
    "Train a logistic regression model to classify colleges as Private or Public using the provided training data.\n",
    "\n",
    "Store the trained model in a variable called `model_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "model_log = sklearn.linear_model.LogisticRegression(max_iter=1000)\n",
    "model_log.fit(X_train, Y_train)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert model_log is not None, \"model_log should be defined\"\n",
    "assert hasattr(model_log, \"predict\"), \"model_log should have a predict method\"\n",
    "assert hasattr(model_log, \"predict_proba\"), \"model_log should have a predict_proba method\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(model_log, \"coef_\"), \"Model should be fitted (have coef_ attribute)\"\n",
    "num_features = X_train.shape[1]\n",
    "assert model_log.coef_.shape[1] == num_features, \"Model should have correct coefficients\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 7:** Computing Negative Log-Likelihood\n",
    "\n",
    "Compute the mean negative log-likelihood (NLL) on the test set. Store the predicted probabilities in a variable called `y_pred_prob` and the mean NLL in a variable called `mean_nll`.\n",
    "\n",
    "**Hint:** You can use `model.predict_proba()` to get probabilities and `sklearn.metrics.log_loss()` to compute NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "y_pred_prob = model_log.predict_proba(X_test)\n",
    "mean_nll = sklearn.metrics.log_loss(Y_test, y_pred_prob)\n",
    "print(f\"Mean NLL: {mean_nll:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert y_pred_prob is not None, \"y_pred_prob should be defined\"\n",
    "assert y_pred_prob.shape == (len(Y_test), 2), \"y_pred_prob should have shape (n_samples, 2)\"\n",
    "assert mean_nll is not None, \"mean_nll should be defined\"\n",
    "assert 0 < mean_nll < 1, \"mean_nll should be between 0 and 1 for a reasonable model\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert np.allclose(y_pred_prob.sum(axis=1), 1.0), \"Probabilities should sum to 1\"\n",
    "assert mean_nll < 0.5, \"Model should achieve NLL less than 0.5\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 8:** Classification Metrics at Threshold 0.5\n",
    "\n",
    "Using a threshold of 0.5, compute the following classification metrics:\n",
    "- True Positive Rate (TPR, also called Recall or Sensitivity)\n",
    "- False Positive Rate (FPR)\n",
    "- True Negative Rate (TNR, also called Specificity)\n",
    "- False Negative Rate (FNR)\n",
    "\n",
    "Store these in variables `tpr`, `fpr`, `tnr`, and `fnr` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "thresh = 0.5\n",
    "y_pred = np.where(y_pred_prob[:, 1] > thresh, 1, 0)\n",
    "\n",
    "# Calculate using confusion matrix\n",
    "cnf_matrix = sklearn.metrics.confusion_matrix(Y_test, y_pred)\n",
    "fp_count = cnf_matrix[0, 1].astype(float)\n",
    "fn_count = cnf_matrix[1, 0].astype(float)\n",
    "tp_count = cnf_matrix[1, 1].astype(float)\n",
    "tn_count = cnf_matrix[0, 0].astype(float)\n",
    "\n",
    "tpr = tp_count / (tp_count + fn_count)\n",
    "fpr = fp_count / (fp_count + tn_count)\n",
    "tnr = tn_count / (tn_count + fp_count)\n",
    "fnr = fn_count / (tp_count + fn_count)\n",
    "\n",
    "print(f\"TPR: {tpr:.4f}\")\n",
    "print(f\"FPR: {fpr:.4f}\")\n",
    "print(f\"TNR: {tnr:.4f}\")\n",
    "print(f\"FNR: {fnr:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert 0 <= tpr <= 1, \"TPR should be between 0 and 1\"\n",
    "assert 0 <= fpr <= 1, \"FPR should be between 0 and 1\"\n",
    "assert 0 <= tnr <= 1, \"TNR should be between 0 and 1\"\n",
    "assert 0 <= fnr <= 1, \"FNR should be between 0 and 1\"\n",
    "assert np.isclose(tpr + fnr, 1.0), \"TPR + FNR should equal 1\"\n",
    "assert np.isclose(fpr + tnr, 1.0), \"FPR + TNR should equal 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert tpr > 0.9, \"TPR should be greater than 0.9 for this model\"\n",
    "assert fpr < 0.15, \"FPR should be less than 0.15 for this model\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 9:** Classification Metrics at Threshold 0.9\n",
    "\n",
    "Repeat the classification metrics computation using a threshold of 0.9. Store the results in variables called `tpr_high`, `fpr_high`, `tnr_high`, and `fnr_high`.\n",
    "\n",
    "Compare these to the metrics at threshold 0.5. What happens to TPR and FPR when we increase the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "thresh_high = 0.9\n",
    "y_pred_high = np.where(y_pred_prob[:, 1] > thresh_high, 1, 0)\n",
    "\n",
    "tpr_high = sklearn.metrics.recall_score(Y_test, y_pred_high)\n",
    "fpr_high = 1 - sklearn.metrics.recall_score(1 - Y_test, 1 - y_pred_high)\n",
    "tnr_high = sklearn.metrics.recall_score(1 - Y_test, 1 - y_pred_high)\n",
    "fnr_high = 1 - sklearn.metrics.recall_score(Y_test, y_pred_high)\n",
    "\n",
    "print(f\"TPR at 0.9: {tpr_high:.4f}\")\n",
    "print(f\"FPR at 0.9: {fpr_high:.4f}\")\n",
    "print(f\"TNR at 0.9: {tnr_high:.4f}\")\n",
    "print(f\"FNR at 0.9: {fnr_high:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert 0 <= tpr_high <= 1, \"TPR should be between 0 and 1\"\n",
    "assert 0 <= fpr_high <= 1, \"FPR should be between 0 and 1\"\n",
    "assert np.isclose(tpr_high + fnr_high, 1.0), \"TPR + FNR should equal 1\"\n",
    "assert np.isclose(fpr_high + tnr_high, 1.0), \"FPR + TNR should equal 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert tpr_high < tpr, \"TPR should decrease when threshold increases\"\n",
    "assert fpr_high < fpr, \"FPR should decrease when threshold increases\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your results above, explain what happens to TPR and FPR when we increase the threshold from 0.5 to 0.9.\n",
    "\n",
    "> BEGIN SOLUTION\n",
    "\n",
    "When we increase the threshold from 0.5 to 0.9, **both TPR and FPR decrease**. This is because the model becomes more conservative about predicting the positive class:\n",
    "\n",
    "- **TPR decreases**: With a higher threshold, fewer instances are predicted as positive, so we miss more actual positives (more false negatives), reducing the true positive rate.\n",
    "- **FPR decreases**: Similarly, we also make fewer false positive predictions since the model requires stronger evidence to predict positive.\n",
    "\n",
    "This illustrates the fundamental tradeoff in classification: a higher threshold reduces false positives but at the cost of missing true positives. The choice of threshold depends on the relative costs of false positives vs. false negatives in the application.\n",
    "\n",
    "> END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 10:** ROC Curve\n",
    "\n",
    "Plot the ROC (Receiver Operating Characteristic) curve by varying the threshold from 0 to 1. Store the lists of FPR and TPR values across thresholds in variables called `fpr_list` and `tpr_list`.\n",
    "\n",
    "**Hint:** Create thresholds using `np.arange(0, 1.001, 0.01)` and compute TPR/FPR at each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "thresholds = np.arange(0, 1.001, 0.01)\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = np.where(y_pred_prob[:, 1] > thresh, 1, 0)\n",
    "    current_tpr = sklearn.metrics.recall_score(Y_test, y_pred_thresh, zero_division=0)\n",
    "    current_fpr = 1 - sklearn.metrics.recall_score(1 - Y_test, 1 - y_pred_thresh, zero_division=0)\n",
    "    tpr_list.append(current_tpr)\n",
    "    fpr_list.append(current_fpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_list, tpr_list)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random classifier\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(fpr_list) > 0, \"fpr_list should not be empty\"\n",
    "assert len(tpr_list) > 0, \"tpr_list should not be empty\"\n",
    "assert len(fpr_list) == len(tpr_list), \"fpr_list and tpr_list should have same length\"\n",
    "assert all(0 <= x <= 1 for x in fpr_list), \"All FPR values should be between 0 and 1\"\n",
    "assert all(0 <= x <= 1 for x in tpr_list), \"All TPR values should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(fpr_list) == 101, \"Should have 101 threshold values\"\n",
    "assert fpr_list[0] == 1.0 and tpr_list[0] == 1.0, \"At threshold 0, should predict all positive\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 11:** AUC Score\n",
    "\n",
    "Compute the Area Under the ROC Curve (AUC) score. Store the result in a variable called `auc_score`.\n",
    "\n",
    "**Hint:** Use `sklearn.metrics.roc_auc_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "auc_score = sklearn.metrics.roc_auc_score(Y_test, y_pred_prob[:, 1])\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert auc_score is not None, \"auc_score should be defined\"\n",
    "assert 0 <= auc_score <= 1, \"AUC should be between 0 and 1\"\n",
    "assert auc_score > 0.5, \"AUC should be greater than 0.5 (better than random)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert auc_score > 0.95, \"AUC should be greater than 0.95 for this model\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
