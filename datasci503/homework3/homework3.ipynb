{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 503, Homework 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is one of the most fundamental methods for binary classification. Unlike linear regression, which predicts continuous values, logistic regression models the probability that an observation belongs to a particular class using the logistic function. This assignment covers the mathematical foundations of logistic regression—including odds, log-odds, and the softmax generalization to multiple classes—along with practical skills in training classifiers and evaluating their performance using metrics like ROC curves and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Deriving the Odds from Logistic Regression\n",
    "\n",
    "Logistic regression models the probability as:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1+ e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "Show that the odds $\\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "$$\\require{cancel}$$  \n",
    "\n",
    "From the logistic regression formula:\n",
    "\n",
    "$$\n",
    "p(x)  = \\frac{e^{\\beta_0 + \\beta_1 X}}{1+ e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "1-p(x) = 1-\\frac{e^{\\beta_0 + \\beta_1 X}}{1+ e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1+ e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "\\frac{p(x)}{1-p(x)} = \\frac{e^{\\beta_0 + \\beta_1 X}/\\cancel{(1+ e^{\\beta_0 + \\beta_1 X}})}{1/\\cancel{(1+ e^{\\beta_0 + \\beta_1 X})}} = e^{\\beta_0 + \\beta_1 X}\n",
    "$$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: The Curse of Dimensionality in KNN\n",
    "\n",
    "Consider the K-nearest neighbors (KNN) algorithm where we use 10% of the available observations for each prediction. The features $X_1, X_2, \\ldots, X_p$ are independently and uniformly distributed on $[0,1]$.\n",
    "\n",
    "(a) For $p = 1$, what fraction of available observations will be used to make each prediction?\n",
    "\n",
    "(b) For $p = 2$, what fraction of available observations will be used?\n",
    "\n",
    "(c) For $p = 100$, what fraction will be used?\n",
    "\n",
    "(d) What is the drawback of KNN when $p$ is large?\n",
    "\n",
    "(e) If we want to use 10% of the observations (by volume), what side length is needed for a hypercube in $p$ dimensions? Calculate for $p = 1, 2, 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**(a)** For $p = 1$, $X \\sim \\text{Uniform}[0,1]$.\n",
    "\n",
    "- For any test observation $X$ that falls in the range $[0.05, 0.95]$, the window of observations used for prediction is $[X-0.05, X+0.05]$, which covers 10% of the total range $[0,1]$.\n",
    "- For any test observation falling in the range $[0, 0.05]$, the window used is $[0, 0.1]$, which covers 10% of the total range.\n",
    "- For any test observation falling in the range $[0.95, 1.0]$, the window used is $[0.9, 1.0]$, which covers 10% of the total range.\n",
    "\n",
    "The fraction of the available observations used to make each prediction is thus always **10%**.\n",
    "\n",
    "**(b)** For $p = 2$, since 10% of the observations will fall within the desired range for $X_1$ and similarly for $X_2$, the fraction of observations that fall within the desired range for both $X_1$ and $X_2$ simultaneously is $0.1 \\times 0.1 = 0.01$, or **1%**.\n",
    "\n",
    "**(c)** For $p = 100$ features, the fraction of observations that fall within 10% of the range for all 100 features simultaneously, assuming independence across features, is $0.1^{100}$, which is essentially **0%**.\n",
    "\n",
    "**(d)** The drawback of KNN with large $p$ is that as $p$ increases, the fraction of available observations used for prediction approaches 0. KNN's simple idea is that similar inputs probably lead to similar outputs, but this idea is not usually going to be powerful enough to handle high-dimensional problems. As dimensionality increases, the probability of there being training observations \"like\" or \"near\" the test observation $X$ across all $p$ dimensions approaches zero.\n",
    "\n",
    "**(e)** Since Volume of hypercube $= (\\text{side length})^p = 0.1$, we have $s = 0.1^{1/p}$.\n",
    "\n",
    "- For $p = 1$, the side length $s = 0.1^{1/1} = 0.1$. A 1-dimensional hypercube (line segment) would have length 0.1 to encompass 10% of the observations.\n",
    "  \n",
    "- For $p = 2$, the side length $s = 0.1^{1/2} \\approx 0.316$. A 2-dimensional hypercube (square) would have sides of approximately 0.316 to encompass 10% of the observations.\n",
    "\n",
    "- For $p = 100$, the side length $s = 0.1^{1/100} \\approx 0.977$. Each side of a 100-dimensional hypercube needs to be nearly the full range to encompass 10% of the observations, showing that these 10% of training observations selected aren't really \"close\" at all.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Odds and Probability Conversion\n",
    "\n",
    "(a) If the odds of an event are 0.37, what is the probability of the event?\n",
    "\n",
    "(b) If the probability of an event is 0.16, what are the odds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**(a)** Given: Odds $= \\frac{p}{1-p} = 0.37$\n",
    "\n",
    "Solving for $p$:\n",
    "$$p = 0.37(1-p) = 0.37 - 0.37p$$\n",
    "$$p + 0.37p = 0.37$$\n",
    "$$1.37p = 0.37$$\n",
    "$$p = \\frac{0.37}{1.37} \\approx 0.27$$\n",
    "\n",
    "**(b)** Given: $p = 0.16$\n",
    "\n",
    "Odds $= \\frac{p}{1-p} = \\frac{0.16}{1-0.16} = \\frac{0.16}{0.84} \\approx 0.19$\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Softmax vs Binary Logistic Regression\n",
    "\n",
    "Consider classifying fruit as either orange or apple based on a single predictor $X$.\n",
    "\n",
    "I fit a binary logistic regression model:\n",
    "$$\\log\\left(\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)}\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X$$\n",
    "\n",
    "with $\\hat{\\beta}_0 = 2$ and $\\hat{\\beta}_1 = -1$.\n",
    "\n",
    "My friend fits a softmax regression model:\n",
    "$$p(\\text{orange}|x) = \\frac{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x)}{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x) + \\exp(\\hat{\\alpha}_{\\text{apple},0} + \\hat{\\alpha}_{\\text{apple},1} x)}$$\n",
    "\n",
    "with $\\hat{\\alpha}_{\\text{orange},0} = 1.2$, $\\hat{\\alpha}_{\\text{orange},1} = -2$, $\\hat{\\alpha}_{\\text{apple},0} = 3$, $\\hat{\\alpha}_{\\text{apple},1} = 0.6$.\n",
    "\n",
    "(a) In my model, what is $\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)}$?\n",
    "\n",
    "(b) In my friend's model, derive an expression for $\\log\\left(\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)}\\right)$.\n",
    "\n",
    "(c) Show the relationship between $\\hat{\\beta}_0, \\hat{\\beta}_1$ and my friend's $\\hat{\\alpha}$ coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**(a)** From the log-odds formulation:\n",
    "$$\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)} = e^{\\beta_0 + \\beta_1 X}$$\n",
    "\n",
    "**(b)** In my friend's model:\n",
    "\n",
    "$$\\tilde{p}(\\text{orange}|x) = \\frac{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x)}{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x) + \\exp(\\hat{\\alpha}_{\\text{apple},0} + \\hat{\\alpha}_{\\text{apple},1} x)}$$\n",
    "\n",
    "$$\\frac{p(\\text{orange}|x)}{p(\\text{apple}|x)} = \\frac{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} X)}{\\exp(\\hat{\\alpha}_{\\text{apple},0} + \\hat{\\alpha}_{\\text{apple},1} X)}$$\n",
    "\n",
    "Taking the log:\n",
    "$$\\log \\left( \\frac{p(\\text{orange} | x)}{p(\\text{apple}|x)} \\right) = (\\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0}) + (\\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1}) X$$\n",
    "\n",
    "**(c)** Comparing the two formulations:\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0}$$\n",
    "$$\\hat{\\beta}_1 = \\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1}$$\n",
    "\n",
    "Verifying with the given values:\n",
    "- $\\hat{\\beta}_0 = 1.2 - 3 = -1.8$ (should be 2, so the friend's model gives different predictions)\n",
    "- $\\hat{\\beta}_1 = -2 - 0.6 = -2.6$ (should be -1, so the models differ)\n",
    "\n",
    "Note: The two models are parameterized differently and would give the same predictions only if the relationship $\\hat{\\beta}_0 = \\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0}$ and $\\hat{\\beta}_1 = \\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1}$ holds.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Predicting Academic Success\n",
    "\n",
    "Suppose we are predicting whether a student gets an A in a class based on hours studied ($X_1$) and GPA ($X_2$). The logistic regression model is:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = -4 + 0.05 X_1 + X_2$$\n",
    "\n",
    "(a) A student with a GPA of 3.5 studies 5 hours per week. What is the probability they get an A?\n",
    "\n",
    "(b) What are the odds this student gets an A?\n",
    "\n",
    "(c) How many hours would this student need to study to have a 50% chance of getting an A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BEGIN SOLUTION\n",
    "\n",
    "**(a)** Compute the log-odds:\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = -4 + 0.05(5) + 3.5 = -4 + 0.25 + 3.5 = -0.25$$\n",
    "\n",
    "From Problem 1, we know $p(z) = \\frac{e^z}{1+e^z}$ for log-odds $z$.\n",
    "\n",
    "So $p = \\frac{e^{-0.25}}{1+e^{-0.25}} \\approx 0.438$.\n",
    "\n",
    "The probability of getting an A is approximately **43.8%**.\n",
    "\n",
    "**(b)** The odds are:\n",
    "$$\\text{Odds} = \\frac{p}{1-p} = e^{-0.25} \\approx 0.779$$\n",
    "\n",
    "**(c)** We want $p = 0.5$. That means the log-odds must be:\n",
    "$$\\log\\left(\\frac{0.5}{0.5}\\right) = \\log(1) = 0$$\n",
    "\n",
    "Assuming GPA = 3.5, we solve:\n",
    "$$0 = -4 + 0.05 X_1 + 3.5$$\n",
    "$$0.5 = 0.05 X_1$$\n",
    "$$X_1 = 10$$\n",
    "\n",
    "The student needs to study **10 hours per week** to have a 50% chance of getting an A.\n",
    "> END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the calculations for Problem 5\n",
    "z = -4 + 0.05 * 5 + 3.5\n",
    "probability = np.exp(z) / (1 + np.exp(z))\n",
    "odds = probability / (1 - probability)\n",
    "print(f\"Log-odds: {z}\")\n",
    "print(f\"Probability: {probability:.4f}\")\n",
    "print(f\"Odds: {odds:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Problems: College Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_train = pd.read_csv(\"./data/college_train.csv\")\n",
    "college_test = pd.read_csv(\"./data/college_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = college_train.drop([\"Private\", \"Name\"], axis=1)\n",
    "Y_train = np.where(college_train[\"Private\"] == \"Yes\", 1, 0)\n",
    "X_test = college_test.drop([\"Private\", \"Name\"], axis=1)\n",
    "Y_test = np.where(college_test[\"Private\"] == \"Yes\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Training a Logistic Regression Classifier\n",
    "\n",
    "Train a logistic regression model to classify colleges as Private or Public using the provided training data.\n",
    "\n",
    "Store the trained model in a variable called `model_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "model_log = sklearn.linear_model.LogisticRegression(max_iter=1000)\n",
    "model_log.fit(X_train, Y_train)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert model_log is not None, \"model_log should be defined\"\n",
    "assert hasattr(model_log, \"predict\"), \"model_log should have a predict method\"\n",
    "assert hasattr(model_log, \"predict_proba\"), \"model_log should have a predict_proba method\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert hasattr(model_log, \"coef_\"), \"Model should be fitted (have coef_ attribute)\"\n",
    "num_features = X_train.shape[1]\n",
    "assert model_log.coef_.shape[1] == num_features, \"Model should have correct coefficients\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: Computing Negative Log-Likelihood\n",
    "\n",
    "Compute the mean negative log-likelihood (NLL) on the test set.\n",
    "\n",
    "Store the predicted probabilities in `y_pred_prob` and the mean NLL in `mean_nll`.\n",
    "\n",
    "**Hint:** You can use `model.predict_proba()` to get probabilities and `sklearn.metrics.log_loss()` to compute NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "y_pred_prob = model_log.predict_proba(X_test)\n",
    "mean_nll = sklearn.metrics.log_loss(Y_test, y_pred_prob)\n",
    "print(f\"Mean NLL: {mean_nll:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert y_pred_prob is not None, \"y_pred_prob should be defined\"\n",
    "assert y_pred_prob.shape == (len(Y_test), 2), \"y_pred_prob should have shape (n_samples, 2)\"\n",
    "assert mean_nll is not None, \"mean_nll should be defined\"\n",
    "assert 0 < mean_nll < 1, \"mean_nll should be between 0 and 1 for a reasonable model\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert np.allclose(y_pred_prob.sum(axis=1), 1.0), \"Probabilities should sum to 1\"\n",
    "assert mean_nll < 0.5, \"Model should achieve NLL less than 0.5\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: Classification Metrics at Threshold 0.5\n",
    "\n",
    "Using a threshold of 0.5, compute the following classification metrics:\n",
    "- True Positive Rate (TPR, also called Recall or Sensitivity)\n",
    "- False Positive Rate (FPR)\n",
    "- True Negative Rate (TNR, also called Specificity)\n",
    "- False Negative Rate (FNR)\n",
    "\n",
    "Store these in variables `tpr`, `fpr`, `tnr`, and `fnr` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "thresh = 0.5\n",
    "y_pred = np.where(y_pred_prob[:, 1] > thresh, 1, 0)\n",
    "\n",
    "# Calculate using confusion matrix\n",
    "cnf_matrix = sklearn.metrics.confusion_matrix(Y_test, y_pred)\n",
    "fp_count = cnf_matrix[0, 1].astype(float)\n",
    "fn_count = cnf_matrix[1, 0].astype(float)\n",
    "tp_count = cnf_matrix[1, 1].astype(float)\n",
    "tn_count = cnf_matrix[0, 0].astype(float)\n",
    "\n",
    "tpr = tp_count / (tp_count + fn_count)\n",
    "fpr = fp_count / (fp_count + tn_count)\n",
    "tnr = tn_count / (tn_count + fp_count)\n",
    "fnr = fn_count / (tp_count + fn_count)\n",
    "\n",
    "print(f\"TPR: {tpr:.4f}\")\n",
    "print(f\"FPR: {fpr:.4f}\")\n",
    "print(f\"TNR: {tnr:.4f}\")\n",
    "print(f\"FNR: {fnr:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert 0 <= tpr <= 1, \"TPR should be between 0 and 1\"\n",
    "assert 0 <= fpr <= 1, \"FPR should be between 0 and 1\"\n",
    "assert 0 <= tnr <= 1, \"TNR should be between 0 and 1\"\n",
    "assert 0 <= fnr <= 1, \"FNR should be between 0 and 1\"\n",
    "assert np.isclose(tpr + fnr, 1.0), \"TPR + FNR should equal 1\"\n",
    "assert np.isclose(fpr + tnr, 1.0), \"FPR + TNR should equal 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert tpr > 0.9, \"TPR should be greater than 0.9 for this model\"\n",
    "assert fpr < 0.15, \"FPR should be less than 0.15 for this model\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Classification Metrics at Threshold 0.9\n",
    "\n",
    "Repeat the classification metrics computation using a threshold of 0.9.\n",
    "\n",
    "Store the results in `tpr_high`, `fpr_high`, `tnr_high`, and `fnr_high`.\n",
    "\n",
    "Compare these to the metrics at threshold 0.5. What happens to TPR and FPR when we increase the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "thresh_high = 0.9\n",
    "y_pred_high = np.where(y_pred_prob[:, 1] > thresh_high, 1, 0)\n",
    "\n",
    "tpr_high = sklearn.metrics.recall_score(Y_test, y_pred_high)\n",
    "fpr_high = 1 - sklearn.metrics.recall_score(1 - Y_test, 1 - y_pred_high)\n",
    "tnr_high = sklearn.metrics.recall_score(1 - Y_test, 1 - y_pred_high)\n",
    "fnr_high = 1 - sklearn.metrics.recall_score(Y_test, y_pred_high)\n",
    "\n",
    "print(f\"TPR at 0.9: {tpr_high:.4f}\")\n",
    "print(f\"FPR at 0.9: {fpr_high:.4f}\")\n",
    "print(f\"TNR at 0.9: {tnr_high:.4f}\")\n",
    "print(f\"FNR at 0.9: {fnr_high:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert 0 <= tpr_high <= 1, \"TPR should be between 0 and 1\"\n",
    "assert 0 <= fpr_high <= 1, \"FPR should be between 0 and 1\"\n",
    "assert np.isclose(tpr_high + fnr_high, 1.0), \"TPR + FNR should equal 1\"\n",
    "assert np.isclose(fpr_high + tnr_high, 1.0), \"FPR + TNR should equal 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert tpr_high < tpr, \"TPR should decrease when threshold increases\"\n",
    "assert fpr_high < fpr, \"FPR should decrease when threshold increases\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: ROC Curve\n",
    "\n",
    "Plot the ROC (Receiver Operating Characteristic) curve by varying the threshold from 0 to 1.\n",
    "\n",
    "Store the lists of FPR and TPR values across thresholds in `fpr_list` and `tpr_list`.\n",
    "\n",
    "**Hint:** Create thresholds using `np.arange(0, 1.001, 0.01)` and compute TPR/FPR at each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "thresholds = np.arange(0, 1.001, 0.01)\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = np.where(y_pred_prob[:, 1] > thresh, 1, 0)\n",
    "    current_tpr = sklearn.metrics.recall_score(Y_test, y_pred_thresh, zero_division=0)\n",
    "    current_fpr = 1 - sklearn.metrics.recall_score(1 - Y_test, 1 - y_pred_thresh, zero_division=0)\n",
    "    tpr_list.append(current_tpr)\n",
    "    fpr_list.append(current_fpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_list, tpr_list)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random classifier\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(fpr_list) > 0, \"fpr_list should not be empty\"\n",
    "assert len(tpr_list) > 0, \"tpr_list should not be empty\"\n",
    "assert len(fpr_list) == len(tpr_list), \"fpr_list and tpr_list should have same length\"\n",
    "assert all(0 <= x <= 1 for x in fpr_list), \"All FPR values should be between 0 and 1\"\n",
    "assert all(0 <= x <= 1 for x in tpr_list), \"All TPR values should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(fpr_list) == 101, \"Should have 101 threshold values\"\n",
    "assert fpr_list[0] == 1.0 or tpr_list[0] == 1.0, \"At threshold 0, should predict all positive\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: AUC Score\n",
    "\n",
    "Compute the Area Under the ROC Curve (AUC) score.\n",
    "\n",
    "Store the result in `auc_score`.\n",
    "\n",
    "**Hint:** Use `sklearn.metrics.roc_auc_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "auc_score = sklearn.metrics.roc_auc_score(Y_test, y_pred_prob[:, 1])\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert auc_score is not None, \"auc_score should be defined\"\n",
    "assert 0 <= auc_score <= 1, \"AUC should be between 0 and 1\"\n",
    "assert auc_score > 0.5, \"AUC should be greater than 0.5 (better than random)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert auc_score > 0.95, \"AUC should be greater than 0.95 for this model\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
