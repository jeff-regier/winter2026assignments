{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 315, Homework 7: Regularization for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this homework assignment, you will train a neural network to infer the number of galaxies in an image, as in group-work assignment 6. Now, however, the images will be noisier and you will need to use various types of regularization to achieve the target level of accuracy.\n",
    "\n",
    "To submit your work, please upload html output from executing this notebook to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "## Import relevant packages and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Here are steps to follow to load the data.\n",
    "\n",
    "  1. Download the following files from Canvas (`Files/homework/hw7 data`):\n",
    "      - [dataset_train_0.5_norm.pt](https://umich.instructure.com/courses/733177/files/39931923/download?download_frd=1)\n",
    "      - [dataset_test_0.5_norm.pt](https://umich.instructure.com/courses/733177/files/39931924/download?download_frd=1)\n",
    "  2. If you're using Google colab, go to the `Files` tab on the left\n",
    "  3. Create a directory named `data` (or name is something else and change the path below)\n",
    "  4. Upload the dataset files to this directory either by clicking the `Upload` button or dragging the files to the directory.\n",
    "\n",
    "Alternative, you may place the dataset in your Google drive for persistent storage, and connect to it with the following code:\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "Now the following code block should load the data. You may need to update the file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_images, train_val_counts = torch.load(\"data/dataset_train_0.5_norm.pt\", weights_only=True)\n",
    "test_images, test_counts = torch.load(\"data/dataset_test_0.5_norm.pt\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine split indices\n",
    "split_index = int(0.9 * len(train_val_images))\n",
    "\n",
    "# Split the data\n",
    "train_images = train_val_images[:split_index]\n",
    "train_counts = train_val_counts[:split_index]\n",
    "val_images = train_val_images[split_index:]\n",
    "val_counts = train_val_counts[split_index:]\n",
    "\n",
    "print(\"Training set size:\", len(train_images))\n",
    "print(\"Validation set size:\", len(val_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Let's display two random images from the train and test set and their corresponding number of galaxies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randint(len(train_images), (1,)).item()\n",
    "plt.imshow(train_images[i], cmap=\"gray\")\n",
    "plt.title(f\"Number of galaxies: {train_counts[i]}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randint(len(val_images), (1,)).item()\n",
    "plt.imshow(val_images[i], cmap=\"gray\")\n",
    "plt.title(f\"Number of galaxies: {val_counts[i]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Problem 1: Specifying the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We will be predicting the number of galaxies in the images, using simple feedforward neural networks.\n",
    "\n",
    "The input to the model will be the image, and the output will be the number of galaxies in the image.\n",
    "As the latter is a discrete variable, we will treat this as a classification problem, that is, the numbers will be treated as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the image dimension\n",
    "dim = 50\n",
    "# the number of classes (maximum number of galaxies is 6)\n",
    "n_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Fill out the below with a model architecture that you think will work well.\n",
    "\n",
    "We are still limiting ourselves to linear layers (no convolutions), but you can now also use regularization.\n",
    "\n",
    "One example of a regularization technique is [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html), which randomly sets a fraction of the input units to 0 at each update during training time, which helps prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(dim * dim, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, n_classes),\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(model, nn.Module), \"model should be an nn.Module\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "test_input = torch.randn(1, dim, dim)\n",
    "test_output = model(test_input)\n",
    "expected_shape = (1, n_classes)\n",
    "assert test_output.shape == expected_shape, f\"output shape should be {expected_shape}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Problem 2: Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The following function re-initializes the model parameters. It's useful if you are re-training a model after changing the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model_parameters(model):\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, \"reset_parameters\"):\n",
    "            module.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Now fill out the function below. Unlike in GroupWork 6, we will be passing an *optimizer* to the function.\n",
    "\n",
    "This allows us to experiment using optimizers (c.f. [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam)) with different values of the `weight_decay` parameter, which uses L2 regularization to penalize large weights.\n",
    "\n",
    "The function should return the training and testing losses so you can evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, num_epochs=10):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    reset_model_parameters(model)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for images, counts in train_dataloader:\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, counts.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, counts in val_dataloader:\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, counts.long())\n",
    "                val_loss += loss.detach().item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"epoch:{epoch} train:{train_loss:.4} val:{val_loss:.4}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "import inspect\n",
    "\n",
    "assert callable(train), \"train should be a function\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "sig = inspect.signature(train)\n",
    "assert len(sig.parameters) >= 4, \"train should have at least 4 parameters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Set up your optimizer with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-3  # SOLUTION\n",
    "weight_decay = 1e-5  # SOLUTION\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(optimizer, optim.Optimizer), \"optimizer should be an Optimizer\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(optimizer.param_groups) > 0, \"optimizer should have param groups\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Set up your data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # SOLUTION\n",
    "\n",
    "train_dataset = TensorDataset(train_images, train_counts)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_images, val_counts)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(batch_size, int) and batch_size > 0, \"batch_size must be positive\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(train_dataloader) > 0, \"train_dataloader should not be empty\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Train the model. Note: adjust the number of epochs if you think the model is not trained enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train(model, optimizer, train_dataloader, val_dataloader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Plot the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"validation\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Problem 3: Evaluating the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Run the code below and achieve at least **63% accuracy** on the test set. If your model is not performing quite well enough, go back to Problems 1 and 2 and try different model architectures and training hyperparameters, then retrain the model until you achieve the target test-set accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_train_counts = model(train_images)\n",
    "pred_val_counts = model(val_images)\n",
    "pred_test_counts = model(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "train_accuracy = (pred_train_counts.argmax(dim=1) == train_counts).float().mean().item()\n",
    "val_accuracy = (pred_val_counts.argmax(dim=1) == val_counts).float().mean().item()\n",
    "test_accuracy = (pred_test_counts.argmax(dim=1) == test_counts).float().mean().item()\n",
    "print(f\"Train accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Validation accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert test_accuracy >= 0.63, f\"Test accuracy >= 63% required, got {test_accuracy:.2%}\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert train_accuracy >= 0.5, f\"Train accuracy >= 50%, got {train_accuracy:.2%}\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
