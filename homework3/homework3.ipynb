{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 315, Homework 3: Gradient Descent and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit, please upload an HTML file to Canvas showing the results of running this notebook using the process described in Group Work Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gradient Descent for an Arbitrary Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Gradient Descent for a Function of One Variable\n",
    "\n",
    "Recall that gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the function's gradient.\n",
    "\n",
    "Namely, the procedure is as follows:\n",
    "1. Start with an initial value $w$.\n",
    "2. Update $w$ by moving in the direction of the negative gradient of the function at $w$: $$w \\leftarrow w - \\eta \\nabla f(w).$$\n",
    "Here, $\\eta$ is the *learning rate*; it controls the size of the step we're taking in the direction of the negative gradient.\n",
    "3. Repeat step 2 until the variable $w$ changes very little between iterations.\n",
    "\n",
    "Let's say we want to minimize $$f(w) = (w - 2)^2,$$\n",
    "that is, find the $w$ where $f(w)$ achieves the minimum value **using gradient descent**.\n",
    "\n",
    "Implement `problem1()` to perform gradient descent and return the final value of $w$. The function takes the following optional parameters:\n",
    "- `epsilon`: convergence threshold for the change in $w$ between iterations (default: 0.01)\n",
    "- `initial_w`: starting value for $w$ (default: 10)\n",
    "- `eta`: learning rate (default: 0.1)\n",
    "\n",
    "**Hint:** The gradient of $f(w) = (w - 2)^2$ is $f'(w) = 2(w - 2) = 2w - 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem1(epsilon=0.01, initial_w=10, eta=0.1):\n",
    "    \"\"\"Perform gradient descent to minimize f(w) = (w - 2)^2.\"\"\"\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    def gradient(w):\n",
    "        return 2 * w - 4\n",
    "\n",
    "    w = initial_w\n",
    "    diff = float(\"inf\")\n",
    "\n",
    "    while diff > epsilon:\n",
    "        old_w = w\n",
    "        w = w - eta * gradient(w)\n",
    "        diff = abs(w - old_w)\n",
    "\n",
    "    return w\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "result1 = problem1()\n",
    "assert abs(result1 - 2) < 0.1, f\"Expected w close to 2, got {result1}\"\n",
    "assert problem1(epsilon=0.001) is not None, \"Function should return a value\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    abs(problem1(epsilon=0.0001, initial_w=10, eta=0.1) - 2) < 0.01\n",
    "), \"Should converge closer to 2 with smaller epsilon\"\n",
    "assert (\n",
    "    abs(problem1(epsilon=0.01, initial_w=-5, eta=0.1) - 2) < 0.1\n",
    "), \"Should work with negative initial value\"\n",
    "assert (\n",
    "    abs(problem1(epsilon=0.01, initial_w=2, eta=0.1) - 2) < 0.1\n",
    "), \"Should work when starting at minimum\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Gradient Descent for a Function of Two Variables\n",
    "\n",
    "Same as above, but now we want to minimize the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function):\n",
    "$$f(w_1, w_2) = (1 - w_1)^2 + 100(w_2 - w_1^2)^2$$\n",
    "\n",
    "This function has a global minimum at $(w_1, w_2) = (1, 1)$.\n",
    "\n",
    "Implement `problem2()` to perform gradient descent and return the final values of $w_1$ and $w_2$.\n",
    "\n",
    "**Hint 1:** You can treat $w$ as a vector (using a NumPy array), but this is not necessary.\n",
    "\n",
    "**Hint 2:** If your algorithm is running into trouble, try adjusting the learning rate first. A smaller learning rate (e.g., 0.001 or smaller) may be needed.\n",
    "\n",
    "**Hint 3:** The gradient is:\n",
    "$$\\nabla f = \\begin{bmatrix} -2(1 - w_1) - 400 w_1 (w_2 - w_1^2) \\\\ 200(w_2 - w_1^2) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2(epsilon=0.0001, initial_w=None, eta=0.0015):\n",
    "    \"\"\"Perform gradient descent to minimize the Rosenbrock function.\"\"\"\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    if initial_w is None:\n",
    "        initial_w = torch.tensor([-1.2, 1.0])\n",
    "\n",
    "    def gradient(w):\n",
    "        dw1 = -2 * (1 - w[0]) - 400 * w[0] * (w[1] - w[0] ** 2)\n",
    "        dw2 = 200 * (w[1] - w[0] ** 2)\n",
    "        return torch.tensor([dw1, dw2])\n",
    "\n",
    "    w = initial_w.clone()\n",
    "    diff = float(\"inf\")\n",
    "\n",
    "    while diff > epsilon:\n",
    "        old_w = w.clone()\n",
    "        w = w - eta * gradient(w)\n",
    "        diff = torch.sum(torch.abs(w - old_w)).item()\n",
    "\n",
    "    return w[0].item(), w[1].item()\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "w1, w2 = problem2()\n",
    "assert w1 is not None and w2 is not None, \"Function should return two values\"\n",
    "assert abs(w1 - 1) < 0.5, f\"Expected w1 close to 1, got {w1}\"\n",
    "assert abs(w2 - 1) < 0.5, f\"Expected w2 close to 1, got {w2}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "w1_precise, w2_precise = problem2(epsilon=0.00001)\n",
    "assert (\n",
    "    abs(w1_precise - 1) < 0.2\n",
    "), f\"With smaller epsilon, w1 should be closer to 1, got {w1_precise}\"\n",
    "assert (\n",
    "    abs(w2_precise - 1) < 0.2\n",
    "), f\"With smaller epsilon, w2 should be closer to 1, got {w2_precise}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent for Linear Regression\n",
    "\n",
    "The previous functions just depended on some parameters $(w_1, w_2)$. Now we are interested in optimizing loss functions that depend on data $(X, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Simple Linear Regression\n",
    "\n",
    "We are interested in:\n",
    "1. \"Fitting\" a linear model:\n",
    "$$\\hat{y}_i = \\alpha + \\beta x_i$$\n",
    "\n",
    "2. By minimizing the squared error loss:\n",
    "$$L(\\alpha, \\beta) = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2$$\n",
    "\n",
    "(Note: The $\\frac{1}{2}$ factor is just a convention in Machine Learning that simplifies the gradient.)\n",
    "\n",
    "3. Using gradient descent:\n",
    "$$\\alpha_k = \\alpha_{k-1} - \\eta \\frac{\\partial L}{\\partial \\alpha}$$\n",
    "$$\\beta_k = \\beta_{k-1} - \\eta \\frac{\\partial L}{\\partial \\beta}$$\n",
    "\n",
    "The gradients are:\n",
    "$$\\frac{\\partial L}{\\partial \\alpha} = -\\frac{1}{n}\\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)$$\n",
    "$$\\frac{\\partial L}{\\partial \\beta} = -\\frac{1}{n}\\sum_{i=1}^n (y_i - \\alpha - \\beta x_i) x_i$$\n",
    "\n",
    "You can check the convergence of the gradient descent by checking if the changes made to $\\alpha_k$ and $\\beta_k$ are smaller than epsilon:\n",
    "$$|\\alpha_k - \\alpha_{k-1}| + |\\beta_k - \\beta_{k-1}| < \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem3(x, y, eta, initial_alpha, initial_beta, epsilon):\n",
    "    \"\"\"Perform gradient descent to fit a simple linear regression model.\"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    alpha = initial_alpha\n",
    "    beta = initial_beta\n",
    "    diff = float(\"inf\")\n",
    "\n",
    "    while diff > epsilon:\n",
    "        # Compute predictions\n",
    "        y_pred = alpha + beta * x\n",
    "        residuals = y - y_pred\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_alpha = -torch.mean(residuals)\n",
    "        grad_beta = -torch.mean(residuals * x)\n",
    "\n",
    "        # Update parameters\n",
    "        old_alpha, old_beta = alpha, beta\n",
    "        alpha = alpha - eta * grad_alpha\n",
    "        beta = beta - eta * grad_beta\n",
    "\n",
    "        # Check convergence\n",
    "        diff = abs(alpha - old_alpha) + abs(beta - old_beta)\n",
    "\n",
    "    return alpha.item() if hasattr(alpha, \"item\") else alpha, (\n",
    "        beta.item() if hasattr(beta, \"item\") else beta\n",
    "    )\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cells to test your solution.\n",
    "\n",
    "**Note:** The obtained coefficients do not need to be exactly the same as the sklearn solution, but should be getting closer if you keep the algorithm running longer (i.e., make the $\\epsilon$ smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data\n",
    "torch.manual_seed(42)\n",
    "x_data = torch.rand(100)\n",
    "true_alpha = torch.rand(1)\n",
    "true_beta = torch.rand(1)\n",
    "y_data = true_alpha + x_data * true_beta + torch.rand(100) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the solution using scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_data[:, None], y_data)\n",
    "print(f\"sklearn intercept: {lr.intercept_}, sklearn coefficient: {lr.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "eta, initial_alpha, initial_beta, epsilon = 0.1, 0, 0, 0.0001\n",
    "alpha_hat, beta_hat = problem3(x_data, y_data, eta, initial_alpha, initial_beta, epsilon)\n",
    "print(f\"Your solution: alpha = {alpha_hat}, beta = {beta_hat}\")\n",
    "\n",
    "assert alpha_hat is not None and beta_hat is not None, \"Function should return alpha and beta\"\n",
    "assert abs(alpha_hat - lr.intercept_) < 0.1, \"Alpha should be close to sklearn intercept\"\n",
    "assert abs(beta_hat - lr.coef_[0]) < 0.1, \"Beta should be close to sklearn coefficient\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "alpha_test, beta_test = problem3(x_data, y_data, 0.5, 0, 0, 0.00001)\n",
    "assert abs(alpha_test - lr.intercept_) < 0.05, \"With smaller epsilon, should be closer to sklearn\"\n",
    "assert abs(beta_test - lr.coef_[0]) < 0.05, \"With smaller epsilon, should be closer to sklearn\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Multivariate Linear Regression\n",
    "\n",
    "In the vector form, gradient descent is given by:\n",
    "$$w_k = w_{k-1} - \\eta \\nabla L,$$\n",
    "\n",
    "where $w$ is the weight vector of the model, $\\eta$ is the learning rate, $L$ is the loss function and $\\nabla L$ is the corresponding gradient.\n",
    "\n",
    "Consider now multiple/multivariate linear regression:\n",
    "$$L(w) = \\frac{1}{2n}\\sum_{i=1}^n (y_i - x_i^T w)^2 = \\frac{1}{2n}||Y - Xw||^2,$$\n",
    "\n",
    "where each $x_i$ is a row vector, or correspondingly, $X$ is a matrix. Likewise $y_i$ is the corresponding element of the vector $Y$.\n",
    "\n",
    "To simplify things, we have calculated the closed-form formula of the gradient for you:\n",
    "$$\\nabla L = \\frac{1}{n} X^T(Xw - Y)$$\n",
    "\n",
    "The convergence criterion is:\n",
    "$$||w_k - w_{k-1}||_{\\infty} < \\epsilon$$\n",
    "\n",
    "where $||\\cdot||_{\\infty}$ is the infinity norm (maximum absolute value). You can compute this with `torch.max(torch.abs(w - old_w))`.\n",
    "\n",
    "Unlike the previous question, please also keep track of the loss at each iteration and return the whole loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem4(features, y, eta, initial_w, epsilon):\n",
    "    \"\"\"Perform gradient descent for multivariate linear regression.\"\"\"\n",
    "    n = features.shape[0]\n",
    "\n",
    "    # To fit the intercept, we pad the features matrix with a column of ones\n",
    "    ones = torch.ones((n, 1))\n",
    "    features_augmented = torch.hstack((ones, features))\n",
    "    y_col = y.reshape((n, 1))\n",
    "\n",
    "    losses = []\n",
    "    w = initial_w.clone()\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    diff = float(\"inf\")\n",
    "\n",
    "    while diff > epsilon:\n",
    "        # Compute predictions\n",
    "        y_pred = features_augmented @ w\n",
    "\n",
    "        # Compute and store loss\n",
    "        loss = 0.5 * torch.mean((y_col - y_pred) ** 2)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Compute gradient\n",
    "        grad = (1 / n) * features_augmented.T @ (y_pred - y_col)\n",
    "\n",
    "        # Update weights\n",
    "        old_w = w.clone()\n",
    "        w = w - eta * grad\n",
    "\n",
    "        # Check convergence\n",
    "        diff = torch.max(torch.abs(w - old_w)).item()\n",
    "    # END SOLUTION\n",
    "\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the California Housing dataset to check our solution. This dataset contains housing prices and features from the 1990 California census.\n",
    "\n",
    "**Note:** The coefficients obtained with your gradient descent implementation might not quite coincide with sklearn's solution. The important thing is that your training loss keeps decreasing, and that the final loss is comparable to sklearn's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing_df = pd.read_csv(\"housing.csv\")\n",
    "\n",
    "# Select numeric features\n",
    "feature_cols = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "]\n",
    "X_calif = housing_df[feature_cols].dropna()\n",
    "# Scale to units of $100k\n",
    "y_calif = housing_df.loc[X_calif.index, \"median_house_value\"] / 100000\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_calif, y_calif, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features (important for gradient descent convergence)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "n_features = X_calif.shape[1]\n",
    "print(f\"California Housing: {len(y_calif)} samples, {n_features} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "num_features = n_features + 1  # +1 for intercept\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "w_gd, losses = problem4(\n",
    "    X_train_t,\n",
    "    y_train_t,\n",
    "    eta=0.01,\n",
    "    initial_w=torch.randn(num_features, 1),\n",
    "    epsilon=0.001,\n",
    ")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(torch.arange(1, len(losses) + 1), losses, color=\"red\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Iterations\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {losses[-1]}\")\n",
    "\n",
    "# Compute test loss\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "X_test_augmented = torch.hstack((torch.ones((X_test_t.shape[0], 1)), X_test_t))\n",
    "y_test_pred = X_test_augmented @ w_gd\n",
    "y_test_col = y_test_t.reshape((-1, 1))\n",
    "test_loss = 0.5 * torch.mean((y_test_col - y_test_pred) ** 2)\n",
    "print(f\"Test loss: {test_loss.item()}\")\n",
    "\n",
    "assert len(losses) > 0, \"Should return a non-empty loss history\"\n",
    "assert losses[-1] < losses[0], \"Loss should decrease during training\"\n",
    "assert losses[-1] < 0.5, f\"Final training loss should be reasonable, got {losses[-1]}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert all(\n",
    "    losses[i] >= losses[i + 1] - 1e-10 for i in range(len(losses) - 1)\n",
    "), \"Loss should be monotonically decreasing\"\n",
    "assert w_gd.shape == (num_features, 1), f\"Weight shape should be ({num_features}, 1)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Minibatch Gradient Descent\n",
    "\n",
    "In minibatch gradient descent, we apply gradient descent to a subset of the data (called a \"batch\") at each iteration. This is more efficient for large datasets and can help escape local minima.\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "**Inputs:** $X, Y, w_0, \\eta, \\epsilon, |B|$ (batch size)\n",
    "\n",
    "**Step 0:** Set $n$ = number of rows in $X$ and calculate the number of batches as $m = \\lceil n / |B| \\rceil$\n",
    "\n",
    "**Step 1:** Initialize $w_k = w_0$\n",
    "\n",
    "**Step 2:** Repeat until $||w_k - w_{k-1}||_{\\infty} < \\epsilon$:\n",
    "\n",
    "- **Step 2a:** Shuffle the data randomly and split into $m$ batches: $B_1, B_2, ..., B_m$\n",
    "\n",
    "- **Step 2b:** For each batch $j$ in $\\{1, 2, ..., m\\}$, update weights:\n",
    "  $$w_k = w_{k-1} - \\eta \\frac{1}{|B_j|} X_{B_j}^T(X_{B_j}w_{k-1} - Y_{B_j})$$\n",
    "\n",
    "- **Step 2c:** After processing all batches, calculate and store the loss for the **whole** dataset:\n",
    "  $$L_k = \\frac{1}{2n}||Y - Xw_k||^2$$\n",
    "\n",
    "**Output:** $w_k$, list of losses\n",
    "\n",
    "**Hint:** It might be helpful to use `torch.randperm()` to shuffle the data:\n",
    "\n",
    "```python\n",
    "# Shuffle indices\n",
    "perm = torch.randperm(n)\n",
    "X_shuffled = X[perm]\n",
    "y_shuffled = y[perm]\n",
    "\n",
    "# Select a minibatch\n",
    "X_mini = X_shuffled[start:end]\n",
    "y_mini = y_shuffled[start:end]\n",
    "```\n",
    "\n",
    "Note that $X$ and $y$ must **always be shuffled together** (think about why this is important!).\n",
    "\n",
    "If $|B|$ is not exactly divisible by the sample size, make the last minibatch of size $n \\mod |B|$ and adjust the update rule accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem5(features, y, eta, initial_w, epsilon, batch_size):\n",
    "    \"\"\"Perform minibatch gradient descent for multivariate linear regression.\"\"\"\n",
    "    n = features.shape[0]\n",
    "    ones = torch.ones((n, 1))\n",
    "    features_augmented = torch.hstack((ones, features))\n",
    "    y_col = y.reshape((n, 1))\n",
    "\n",
    "    w = initial_w.clone()\n",
    "    losses = []\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    diff = float(\"inf\")\n",
    "    num_batches = (n + batch_size - 1) // batch_size  # ceiling division\n",
    "\n",
    "    while diff > epsilon:\n",
    "        old_w = w.clone()\n",
    "\n",
    "        # Shuffle data using random permutation\n",
    "        perm = torch.randperm(n)\n",
    "        features_shuffled = features_augmented[perm]\n",
    "        y_shuffled = y_col[perm]\n",
    "\n",
    "        # Process each batch\n",
    "        for j in range(num_batches):\n",
    "            start_idx = j * batch_size\n",
    "            end_idx = min((j + 1) * batch_size, n)\n",
    "\n",
    "            features_mini = features_shuffled[start_idx:end_idx]\n",
    "            y_mini = y_shuffled[start_idx:end_idx]\n",
    "            batch_n = features_mini.shape[0]\n",
    "\n",
    "            # Compute gradient for this batch\n",
    "            grad = (1 / batch_n) * features_mini.T @ (features_mini @ w - y_mini)\n",
    "            w = w - eta * grad\n",
    "\n",
    "        # Compute loss on full dataset\n",
    "        y_pred = features_augmented @ w\n",
    "        loss = 0.5 * torch.mean((y_col - y_pred) ** 2)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Check convergence\n",
    "        diff = torch.max(torch.abs(w - old_w)).item()\n",
    "    # END SOLUTION\n",
    "\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the test code from the previous question, as we're again solving a multivariate linear regression problem, this time with a slightly different optimization algorithm. Same caveats apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "num_features = n_features + 1\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "w_minibatch, losses_minibatch = problem5(\n",
    "    X_train_t,\n",
    "    y_train_t,\n",
    "    eta=0.01,\n",
    "    initial_w=torch.randn(num_features, 1),\n",
    "    epsilon=0.001,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(torch.arange(1, len(losses_minibatch) + 1), losses_minibatch, color=\"red\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs (Minibatch GD)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {losses_minibatch[-1]}\")\n",
    "\n",
    "# Compute test loss\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "X_test_augmented = torch.hstack((torch.ones((X_test_t.shape[0], 1)), X_test_t))\n",
    "y_test_pred_mb = X_test_augmented @ w_minibatch\n",
    "y_test_col = y_test_t.reshape((-1, 1))\n",
    "test_loss_mb = 0.5 * torch.mean((y_test_col - y_test_pred_mb) ** 2)\n",
    "print(f\"Test loss: {test_loss_mb.item()}\")\n",
    "\n",
    "assert len(losses_minibatch) > 0, \"Should return a non-empty loss history\"\n",
    "assert (\n",
    "    losses_minibatch[-1] < 0.5\n",
    "), f\"Final training loss should be reasonable, got {losses_minibatch[-1]}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert w_minibatch.shape == (\n",
    "    num_features,\n",
    "    1,\n",
    "), f\"Weight shape should be ({num_features}, 1)\"\n",
    "# Test with different batch size\n",
    "torch.manual_seed(42)\n",
    "w_small_batch, _ = problem5(\n",
    "    X_train_t,\n",
    "    y_train_t,\n",
    "    eta=0.01,\n",
    "    initial_w=torch.randn(num_features, 1),\n",
    "    epsilon=0.001,\n",
    "    batch_size=64,\n",
    ")\n",
    "assert w_small_batch.shape == (num_features, 1), \"Should work with smaller batch size\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
