{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "a186cfd388f27dfa"
   },
   "source": [
    "# DATASCI 315, Group Work Assignment 2: Shallow Neural Networks\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!** Upon completion, one member of the team should submit their team's work through Canvas **as html**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "8af7dedab7f6ca74"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Part 1: Shallow Neural Networks\n",
    "\n",
    "In this section, we will use PyTorch to build a shallow neural network from scratch. **Important:** Do not use PyTorch's autograd (automatic differentiation) for this assignment. We will implement everything manually using only tensor operations.\n",
    "\n",
    "The ingredients we need include:\n",
    "1. Single layer\n",
    "2. Activation\n",
    "3. Loss\n",
    "\n",
    "Later we will use PyTorch's automatic differentiation (autograd) to train models using backpropagation. Here, our focus is on the network's forward pass.\n",
    "\n",
    "**Documentation reference:** [PyTorch Tensors](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### ReLU Activation Function\n",
    "\n",
    "The ReLU (Rectified Linear Unit) activation function is defined as:\n",
    "$$\\text{ReLU}(x) = \\max \\{0, x\\}$$\n",
    "\n",
    "This function will be used in our neural network implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(x, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "This is a regression problem. The goal is to find a function $f(\\cdot|\\Theta)$ parametrized by $\\Theta$ (weights and biases of the network) such that $y_i\\approx f(x_i|\\Theta)$ given observations $x_1,\\dots,x_n$ and corresponding outputs $y_1,\\dots,y_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for regression\n",
    "x_train = torch.tensor(\n",
    "    [\n",
    "        0.09291784,\n",
    "        0.46809093,\n",
    "        0.93089486,\n",
    "        0.67612654,\n",
    "        0.73441752,\n",
    "        0.86847339,\n",
    "        0.49873225,\n",
    "        0.51083168,\n",
    "        0.18343972,\n",
    "        0.99380898,\n",
    "        0.27840809,\n",
    "        0.38028817,\n",
    "        0.12055708,\n",
    "        0.56715537,\n",
    "        0.92005746,\n",
    "        0.77072270,\n",
    "        0.85278176,\n",
    "        0.05315950,\n",
    "        0.87168699,\n",
    "        0.58858043,\n",
    "    ]\n",
    ")\n",
    "y_train = torch.tensor(\n",
    "    [\n",
    "        -0.15934537,\n",
    "        0.18195445,\n",
    "        0.451270150,\n",
    "        0.13921448,\n",
    "        0.09366691,\n",
    "        0.30567674,\n",
    "        0.372291170,\n",
    "        0.40716968,\n",
    "        -0.08131792,\n",
    "        0.41187806,\n",
    "        0.36943738,\n",
    "        0.3994327,\n",
    "        0.019062570,\n",
    "        0.35820410,\n",
    "        0.452564960,\n",
    "        -0.0183121,\n",
    "        0.02957665,\n",
    "        -0.24354444,\n",
    "        0.148038840,\n",
    "        0.26824970,\n",
    "    ]\n",
    ")\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.xlabel(\"Input $x$\")\n",
    "plt.ylabel(\"Output $y$\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "A shallow network can be thought of as a single layer of hidden units, but our code will be versatile enough to cover much more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### A Linear Layer Implementation\n",
    "\n",
    "Let us use PyTorch to create a class `Linear` that models a single linear transformation. To initialize an instance of this class, you need to provide `in_features` and `out_features`. The attributes saved during initialization include `weights` and `bias` of appropriate sizes, initialized as uniform random numbers between -1 and 1.\n",
    "\n",
    "We can use the magic method `__call__` to apply an instantiated `Linear` object to a tensor of conformable dimensions, like a function. Calling it on an input tensor $X$ of shape `(n, d)` where `d` equals `in_features` should return $Y=X A^\\\\top + b$, where $A$ is the `weights` attribute and $b$ is the `bias`. Note that $Y$ has shape `(n, p)` with `p` equal to `out_features`.\n",
    "\n",
    "**Shapes:** `weights` (i.e., $A$) should have shape `(p, d)` and `bias` (i.e., $b$) should have shape `(p,)$.\n",
    "\n",
    "**Note:** This is similar to `torch.nn.Linear`, which we will use when we start working with PyTorch's neural network module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # initialize\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Initialize weights uniformly between -1 and 1\n",
    "        self.weights = torch.rand(out_features, in_features) * 2 - 1  # weights A\n",
    "        self.bias = torch.rand(out_features) * 2 - 1  # bias b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # check conformable input\n",
    "        assert len(x.shape) == 2, \"Input must be 2D\"\n",
    "        assert x.shape[1] == self.in_features, \"Input features must match\"\n",
    "        # return Y=X A^T + b\n",
    "        return x @ self.weights.T + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Linear(in_features=2, out_features=5)  # input has 2 features, output has 5 features\n",
    "X = torch.rand(100, 2)  # (n=100, d=2)\n",
    "y = m(X)  # (n=100, p=5)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "For each individual $x$, note that the linear layer performs the following:\n",
    "$$y_j = \\sum_{i=1}^{d} A_{ji}x_i + b_j, \\quad j=1,\\dots, p.$$\n",
    "Thus, $A_{ji}$ is the weight attached for the connection between the $i$th coordinate of input and the $j$th coordinate of output. Now, let us add an activation function to our network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Problem 1: Building a Simple Neural Network\n",
    "\n",
    "Build a class `SimpleNetwork` by filling in the parts in the provided code block below. Use the `Linear` class implemented above and the `relu` function. The network architecture is:\n",
    "\n",
    "$$\n",
    "\\\\text{input } x \n",
    "\\;\\\\xrightarrow{\\\\text{Linear}}\\;\n",
    "\\\\text{hidden layer (}n_{\\\\text{hidden}}\\\\text{ units)}\n",
    "\\;\\\\xrightarrow{\\\\text{ReLU}}\\;\n",
    "\\\\text{activated hidden layer}\n",
    "\\;\\\\xrightarrow{\\\\text{Linear}}\\;\n",
    "\\\\text{output } \\\\hat{y}\n",
    "$$\n",
    "\n",
    "This is a regression problem, so the output is a scalar.\n",
    "\n",
    "Take `n_hidden` as a parameter when initializing the class---this is the number of hidden units. Note that a single hidden layer requires two `Linear` transformations:\n",
    "1. `layer1`: maps from `in_features` to `n_hidden`\n",
    "2. `layer2`: maps from `n_hidden` to 1 (the output)\n",
    "\n",
    "Initialize a model from this class using `n_hidden=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "class SimpleNetwork:\n",
    "    def __init__(self, in_features, n_hidden=2):\n",
    "        # BEGIN SOLUTION\n",
    "        # Create two Linear layers:\n",
    "        # - layer1: maps input to hidden layer\n",
    "        # - layer2: maps hidden layer to output\n",
    "        self.layer1 = Linear(in_features, n_hidden)\n",
    "        self.layer2 = Linear(n_hidden, 1)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BEGIN SOLUTION\n",
    "        # Forward pass: Linear -> ReLU -> Linear\n",
    "        h = self.layer1(x)\n",
    "        h = relu(h)\n",
    "        return self.layer2(h)\n",
    "        # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "model = SimpleNetwork(in_features=1, n_hidden=3)  # initializing an instance of the class\n",
    "X = torch.rand(100, 1)\n",
    "y = model.forward(X)  # should be shape (n, 1)\n",
    "\n",
    "assert y.shape == (100, 1), f\"Output shape should be (100, 1), got {y.shape}\"\n",
    "assert hasattr(model, \"layer1\"), \"Model should have layer1\"\n",
    "assert hasattr(model, \"layer2\"), \"Model should have layer2\"\n",
    "assert model.layer1.in_features == 1, \"layer1 should have 1 input feature\"\n",
    "assert model.layer1.out_features == 3, \"layer1 should have 3 output features\"\n",
    "assert model.layer2.in_features == 3, \"layer2 should have 3 input features\"\n",
    "assert model.layer2.out_features == 1, \"layer2 should have 1 output feature\"\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "model2 = SimpleNetwork(in_features=4, n_hidden=5)\n",
    "X2 = torch.rand(50, 4)\n",
    "y2 = model2.forward(X2)\n",
    "assert y2.shape == (50, 1), f\"Output shape should be (50, 1), got {y2.shape}\"\n",
    "assert model2.layer1.in_features == 4, \"layer1 should have 4 input features\"\n",
    "assert model2.layer1.out_features == 5, \"layer1 should have 5 output features (n_hidden)\"\n",
    "assert model2.layer2.in_features == 5, \"layer2 should have 5 input features\"\n",
    "assert model2.layer2.out_features == 1, \"layer2 should have 1 output feature\"\n",
    "\n",
    "model3 = SimpleNetwork(in_features=2, n_hidden=10)\n",
    "X3 = torch.rand(25, 2)\n",
    "y3 = model3.forward(X3)\n",
    "assert y3.shape == (25, 1), f\"Output shape should be (25, 1), got {y3.shape}\"\n",
    "assert model3.layer1.out_features == 10, \"layer1 should have 10 output features (n_hidden)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Comparing with Figure 3.4 in the Textbook (Understanding Deep Learning), the parameters of our model are related to the $\\theta$'s and $\\phi$'s as follows:\n",
    "\n",
    "1. First `Linear` weights are $\\theta_{11}$, $\\theta_{21}$, and $\\theta_{31}$\n",
    "2. First `Linear` biases are $\\theta_{10}$, $\\theta_{20}$, and $\\theta_{30}$\n",
    "3. Second `Linear` weights are $\\phi_1$, $\\phi_2$, and $\\phi_3$\n",
    "4. Second `Linear` bias is $\\phi_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Problem 2: Setting Network Parameters\n",
    "\n",
    "(a) Update the parameters of your neural network to match the following values.\n",
    "\n",
    "**First linear layer** (`layer1`):\n",
    "- `weights` shape: $(3, 1)$ --- each row corresponds to one hidden unit\n",
    "- `bias` shape: $(3,)$\n",
    "\n",
    "| Hidden Unit | Weight ($\\theta_{k1}$) | Bias ($\\theta_{k0}$) |\n",
    "|-------------|------------------------|----------------------|\n",
    "| 1           | $-1$                   | $0.3$                |\n",
    "| 2           | $2$                    | $-1$                 |\n",
    "| 3           | $0.65$                 | $-0.5$               |\n",
    "\n",
    "So: `layer1.weights = [[-1], [2], [0.65]]` and `layer1.bias = [0.3, -1, -0.5]`\n",
    "\n",
    "**Second linear layer** (`layer2`):\n",
    "- `weights` shape: $(1, 3)$ --- connects hidden units to the single output\n",
    "- `bias` shape: $(1,)$\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| $\\phi_1$ (weight for hidden unit 1) | $2$ |\n",
    "| $\\phi_2$ (weight for hidden unit 2) | $-1$ |\n",
    "| $\\phi_3$ (weight for hidden unit 3) | $7$ |\n",
    "| $\\phi_0$ (bias) | $-0.3$ |\n",
    "\n",
    "So: `layer2.weights = [[2, -1, 7]]` and `layer2.bias = [-0.3]`\n",
    "\n",
    "(b) Plot the neural network function $f(\\cdot;\\Theta)$ from 0 to 1 along with the training data.\n",
    "\n",
    "**Hint:** Access parameters via `model.layer1.weights`, `model.layer1.bias`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Set the weights and biases to the specified values\n",
    "model.layer1.weights = torch.tensor([[-1.0], [2.0], [0.65]])\n",
    "model.layer1.bias = torch.tensor([0.3, -1.0, -0.5])\n",
    "model.layer2.weights = torch.tensor([[2.0, -1.0, 7.0]])\n",
    "model.layer2.bias = torch.tensor([-0.3])\n",
    "# END SOLUTION\n",
    "\n",
    "# Part (b): Plot the network function with training data\n",
    "x_plot = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y_plot = model.forward(x_plot)\n",
    "plt.plot(x_plot.tolist(), y_plot.detach().tolist(), label=\"Network $f(x; \\\\Theta)$\")\n",
    "plt.scatter(x_train.tolist(), y_train.tolist(), color=\"red\", label=\"Training data\")\n",
    "plt.xlabel(\"Input $x$\")\n",
    "plt.ylabel(\"Output $y$\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert torch.allclose(\n",
    "    model.layer1.weights, torch.tensor([[-1.0], [2.0], [0.65]])\n",
    "), \"layer1 weights incorrect\"\n",
    "assert torch.allclose(model.layer1.bias, torch.tensor([0.3, -1.0, -0.5])), \"layer1 bias incorrect\"\n",
    "assert torch.allclose(\n",
    "    model.layer2.weights, torch.tensor([[2.0, -1.0, 7.0]])\n",
    "), \"layer2 weights incorrect\"\n",
    "assert torch.allclose(model.layer2.bias, torch.tensor([-0.3])), \"layer2 bias incorrect\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert model.layer1.weights.shape == (3, 1), \"layer1 weights should have shape (3, 1)\"\n",
    "assert model.layer1.bias.shape == (3,), \"layer1 bias should have shape (3,)\"\n",
    "assert model.layer2.weights.shape == (1, 3), \"layer2 weights should have shape (1, 3)\"\n",
    "assert model.layer2.bias.shape == (1,), \"layer2 bias should have shape (1,)\"\n",
    "test_x = torch.tensor([[0.5]])\n",
    "test_output = model.forward(test_x)\n",
    "assert test_output.shape == (1, 1), \"Forward pass output should have shape (1, 1)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "The loss quantifies how bad our prediction $\\hat{y}$ is with respect to the target outcome $y$. For regression problems, the most commonly used loss is the least squares loss, defined as\n",
    "$$\\text{LeastSquares}(Y,\\hat{Y}) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2.$$\n",
    "\n",
    "Often, equivalently, one uses MSE (mean squared error):\n",
    "$$\\text{MSE}(Y,\\hat{Y}) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Problem 3: Loss Function\n",
    "\n",
    "(a) Implement the least squares loss as a function:\n",
    "$$\\text{LeastSquares}(Y, \\hat{Y}) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Hint:** Be careful with shapes! `y_train` has shape `(n,)`, while the output of the network has shape `(n, 1)`. You may need to flatten or reshape one of the tensors.\n",
    "\n",
    "(b) What is the value of this loss using the current model on the training data? (If implemented correctly, it should be approximately 9.385.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_loss(y_true, y_pred):\n",
    "    # BEGIN SOLUTION\n",
    "    # Flatten tensors to handle shape mismatch between (n,) and (n,1)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_true = y_true.flatten()\n",
    "    return torch.sum((y_true - y_pred) ** 2)\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "# Compute the loss on the training data\n",
    "# BEGIN SOLUTION\n",
    "# Need to reshape x_train to (n, 1) for the network\n",
    "y_pred = model.forward(x_train[:, None])\n",
    "l_sq_loss = least_square_loss(y_train, y_pred)\n",
    "# END SOLUTION\n",
    "\n",
    "\n",
    "print(f\"Least squares loss: {l_sq_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert torch.isclose(\n",
    "    least_square_loss(torch.tensor([1.0, 2.0, 3.0]), torch.tensor([1.0, 2.0, 3.0])),\n",
    "    torch.tensor(0.0),\n",
    "), \"Loss should be 0 for identical tensors\"\n",
    "assert torch.isclose(\n",
    "    least_square_loss(torch.tensor([0.0, 0.0]), torch.tensor([1.0, 1.0])),\n",
    "    torch.tensor(2.0),\n",
    "), \"Loss should be 2 for [0,0] vs [1,1]\"\n",
    "assert torch.isclose(\n",
    "    l_sq_loss, torch.tensor(9.385), atol=0.01\n",
    "), f\"Loss should be ~9.385, got {l_sq_loss:.3f}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert torch.isclose(\n",
    "    least_square_loss(torch.tensor([1.0, 2.0, 3.0, 4.0]), torch.tensor([2.0, 3.0, 4.0, 5.0])),\n",
    "    torch.tensor(4.0),\n",
    "), \"Loss should be 4 for tensors differing by 1 each (4 elements)\"\n",
    "assert torch.isclose(\n",
    "    least_square_loss(torch.tensor([0.0, 0.0, 0.0]), torch.tensor([3.0, 4.0, 0.0])),\n",
    "    torch.tensor(25.0),\n",
    "), \"Loss should be 25 for [0,0,0] vs [3,4,0]\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "What remains? ****Training the network****â€”that is, learning the weights and biases such that the loss is minimized. This is achieved through gradient descent and backpropagation, which we will not cover today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Fun Parameter Optimization Without Gradients\n",
    "\n",
    "Although we cannot use backpropagation yet, we can still perform a mini grid search. Let us use a model with `n_hidden=2`. This is a shallow network with 7 parameters (why?). Let $V$ be a uniformly spaced grid of 5 values between -1 and 1.\n",
    "\n",
    "**Plan:** For each combination of parameters from this grid, compute the loss and choose the model with the minimum loss.\n",
    "\n",
    "Note: Even with this small network and only 5 values to search over, this requires checking each of $5^7\\approx 78000$ possible combinations, so it might take a while (runs in $< 5$ seconds on Colab). Clearly, with larger networks or bigger grids this is **not feasible** at all!\n",
    "\n",
    "Make sure your `SimpleNetwork` object (Problem 1) is correctly implemented. Running the following cell block will perform this grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SimpleNetwork(in_features=1, n_hidden=2)  # create model\n",
    "\n",
    "V = torch.linspace(-1, 1, 5)  # create the grid\n",
    "\n",
    "min_loss = float(\"inf\")  # to keep track of loss\n",
    "\n",
    "for a1 in V:\n",
    "    for a2 in V:\n",
    "        for a3 in V:\n",
    "            for a4 in V:\n",
    "                for a5 in V:\n",
    "                    for a6 in V:\n",
    "                        for a7 in V:\n",
    "                            # assign the parameters\n",
    "                            model1.layer1.weights = torch.tensor([[a1], [a2]])\n",
    "                            model1.layer2.weights = torch.tensor([[a3, a4]])\n",
    "                            model1.layer1.bias = torch.tensor([a5, a6])\n",
    "                            model1.layer2.bias = torch.tensor([a7])\n",
    "\n",
    "                            # predict on training data and get loss\n",
    "                            y_predicted = model1.forward(x_train[:, None])\n",
    "                            loss = least_square_loss(y_train, y_predicted)\n",
    "\n",
    "                            # if loss improves, store required quantities\n",
    "                            if loss < min_loss:\n",
    "                                min_loss = loss.item()\n",
    "                                best_params = (a1, a2, a3, a4, a5, a6, a7)\n",
    "\n",
    "min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "If done correctly, the loss should be much better than that in Problem 2 (around 0.54). You can see what the current network function looks like by running the following block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2, a3, a4, a5, a6, a7 = best_params\n",
    "model1.layer1.weights = torch.tensor([[a1], [a2]])\n",
    "model1.layer2.weights = torch.tensor([[a3, a4]])\n",
    "model1.layer1.bias = torch.tensor([a5, a6])\n",
    "model1.layer2.bias = torch.tensor([a7])\n",
    "\n",
    "x = torch.linspace(0, 1, 100)\n",
    "y = model1.forward(x[:, None])\n",
    "plt.plot(x, y.detach())\n",
    "plt.scatter(x_train, y_train, color=\"red\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Using the tools we have built so far, it should be easy to construct a neural network for a classification problem. Here is a simulated dataset we will use as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_clf = torch.randn(100, 2)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    if x[1] > torch.sin(2 * x[0]):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "y_train_clf = torch.tensor([f(x) for x in x_train_clf])\n",
    "plt.scatter(x_train_clf[y_train_clf == 0, 0], x_train_clf[y_train_clf == 0, 1], label=\"class 0\")\n",
    "plt.scatter(x_train_clf[y_train_clf == 1, 0], x_train_clf[y_train_clf == 1, 1], label=\"class 1\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Binary Classification Problem\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Here is an implementation of the softmax function, which is used as the activation in the last layer for classification. The output has the same shape as the input, with each row being a probability vector. For a single $x$ of shape `(d,)`, we have\n",
    "\n",
    "$$\\text{softmax}(x) = \\left(\\frac{e^{x_1}}{\\sum_i e^{x_i}}, \\dots, \\frac{e^{x_d}}{\\sum_i e^{x_i}}\\right).$$\n",
    "\n",
    "Hence the output also has $d$ coordinates, each non-negative and summing to 1, so it can be interpreted as a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = torch.exp(x - x.max(dim=1, keepdim=True).values)  # For numerical stability\n",
    "    return exp_x / exp_x.sum(dim=1, keepdim=True)  # Proper broadcasting\n",
    "\n",
    "\n",
    "# example\n",
    "X = torch.tensor([[10, 1], [8, 3], [-1, 2.0], [3.0, 3.1]])\n",
    "Y = softmax(X)\n",
    "print(f\"Sums of each row: {Y.sum(dim=1)}\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Problem 4: Classification Network\n",
    "\n",
    "(a) Modify the `SimpleNetwork` class to build a neural network for the binary classification problem above:\n",
    "\n",
    "$$\\text{input } x \\rightarrow \\texttt{Linear} \\text{ with } n_{\\text{hidden}} \\rightarrow \\text{ReLU} \\rightarrow \\texttt{Linear} \\text{ with out\\_features}=2 \\rightarrow \\text{softmax} \\rightarrow \\text{output } (\\hat{y}_0, \\hat{y}_1)$$\n",
    "\n",
    "(b) Instantiate the network with `n_hidden=3`.\n",
    "\n",
    "(c) Compute the binary cross entropy loss (implementation provided) for the network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetworkClassifier:\n",
    "    def __init__(self, in_features, n_hidden):\n",
    "        # BEGIN SOLUTION\n",
    "        # Use our custom Linear class (not torch.nn.Linear)\n",
    "        self.layer1 = Linear(in_features, n_hidden)\n",
    "        self.layer2 = Linear(n_hidden, 2)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BEGIN SOLUTION\n",
    "        # Forward pass: Linear -> ReLU -> Linear -> Softmax\n",
    "        h = self.layer1(x)\n",
    "        h = relu(h)\n",
    "        logits = self.layer2(h)\n",
    "        return softmax(logits)\n",
    "        # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_clf = SimpleNetworkClassifier(in_features=2, n_hidden=3)\n",
    "x_test = torch.rand(10, 2)\n",
    "test_y = test_clf.forward(x_test)\n",
    "\n",
    "# Visible test assertions\n",
    "assert test_y.shape == (10, 2), f\"Output shape should be (10, 2), got {test_y.shape}\"\n",
    "assert hasattr(test_clf, \"layer1\"), \"Model should have layer1\"\n",
    "assert hasattr(test_clf, \"layer2\"), \"Model should have layer2\"\n",
    "assert test_clf.layer1.in_features == 2, \"layer1 should have 2 input features\"\n",
    "assert test_clf.layer1.out_features == 3, \"layer1 should have 3 output features (n_hidden)\"\n",
    "assert test_clf.layer2.in_features == 3, \"layer2 should have 3 input features\"\n",
    "assert test_clf.layer2.out_features == 2, \"layer2 should have 2 output features (for 2 classes)\"\n",
    "assert torch.allclose(\n",
    "    test_y.sum(dim=1), torch.ones(10)\n",
    "), \"Each row should sum to 1 (softmax output)\"\n",
    "assert torch.all(test_y >= 0) and torch.all(\n",
    "    test_y <= 1\n",
    "), \"All probabilities should be between 0 and 1\"\n",
    "\n",
    "# Create model for use in later cells\n",
    "model_clf = SimpleNetworkClassifier(in_features=2, n_hidden=3)\n",
    "y_predicted = model_clf.forward(x_train_clf)  # (n, 2)\n",
    "print(\"Here are the predicted probabilities for the first 5 data points:\")\n",
    "print(y_predicted[:5, :])\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "test_clf2 = SimpleNetworkClassifier(in_features=5, n_hidden=10)\n",
    "x_test2 = torch.rand(20, 5)\n",
    "test_y2 = test_clf2.forward(x_test2)\n",
    "assert test_y2.shape == (20, 2), f\"Output shape should be (20, 2), got {test_y2.shape}\"\n",
    "assert test_clf2.layer1.in_features == 5, \"layer1 should have 5 input features\"\n",
    "assert test_clf2.layer1.out_features == 10, \"layer1 should have 10 output features\"\n",
    "assert torch.allclose(test_y2.sum(dim=1), torch.ones(20)), \"Softmax outputs should sum to 1\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "A common choice of loss is the binary cross entropy loss, which is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # y_pred is P(y=1), shape (n,) -> one column of y_predicted\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)  # Clamp for stability\n",
    "    loss = -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy(y_train_clf.float(), y_predicted[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Congrats! You now can build neural networks for both regression and classification using fully connected layers with just PyTorch tensors. When we learn PyTorch's neural network module (`torch.nn`), building a model will seem very similar to this, but using the built-in torch versions of the layers, activations and loss. Finally, we will see how to actually train such models using automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "19830e143a8ccf95"
   },
   "source": [
    "## Part 2: Neural Networks in Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "d54bffa1788f1af7"
   },
   "source": [
    "Following the book, we'll take a look at how neural networks can be represented in matrix form.  We'll start with a simple network with one input, one output, and three hidden units.  We'll be using PyTorch tensors for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "23f6ee2df2a9a236"
   },
   "source": [
    "Define the Rectified Linear Unit (ReLU) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "ffbf3dcb2a062225"
   },
   "outputs": [],
   "source": [
    "def relu(preactivation):\n",
    "    return torch.clamp(preactivation, min=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "3cbb0e3240cc1a1f"
   },
   "source": [
    "Define a shallow neural network with, one input, one output, and three hidden units as a function.\n",
    "\n",
    "NOTE: Python allows passing another function (`activation_fn`) as an argument to a function without any special syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "fdb03e7e5ead7e2a"
   },
   "outputs": [],
   "source": [
    "def shallow_1_1_3(\n",
    "    x,\n",
    "    activation_fn,\n",
    "    phi_0,\n",
    "    phi_1,\n",
    "    phi_2,\n",
    "    phi_3,\n",
    "    theta_10,\n",
    "    theta_11,\n",
    "    theta_20,\n",
    "    theta_21,\n",
    "    theta_30,\n",
    "    theta_31,\n",
    "):\n",
    "    # Initial lines\n",
    "    pre_1 = theta_10 + theta_11 * x\n",
    "    pre_2 = theta_20 + theta_21 * x\n",
    "    pre_3 = theta_30 + theta_31 * x\n",
    "\n",
    "    # Activation functions\n",
    "    act_1 = activation_fn(pre_1)\n",
    "    act_2 = activation_fn(pre_2)\n",
    "    act_3 = activation_fn(pre_3)\n",
    "\n",
    "    # Weight activations\n",
    "    w_act_1 = phi_1 * act_1\n",
    "    w_act_2 = phi_2 * act_2\n",
    "    w_act_3 = phi_3 * act_3\n",
    "\n",
    "    # Combine weighted activation and add y offset\n",
    "    y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
    "\n",
    "    # Return everything we have calculated\n",
    "\n",
    "    return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "15eb78d4130cff2a"
   },
   "source": [
    "For plotting, we'll assume input in is range [-1,1] and output [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "a8675ce4373f062e"
   },
   "outputs": [],
   "source": [
    "def plot_neural(x, y):\n",
    "    _fig, ax = plt.subplots()\n",
    "    ax.plot(x.T, y.T)\n",
    "    ax.set_xlabel(\"Input\")\n",
    "    ax.set_ylabel(\"Output\")\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.set_aspect(1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "a310b9e98781f0d"
   },
   "source": [
    "Let's define a network.  We'll just consider the inputs and outputs over the range [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "id": "8b246864f3e05a4e"
   },
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the first neural network\n",
    "n1_theta_10 = 0.0\n",
    "n1_theta_11 = -1.0\n",
    "n1_theta_20 = 0\n",
    "n1_theta_21 = 1.0\n",
    "n1_theta_30 = -0.67\n",
    "n1_theta_31 = 1.0\n",
    "n1_phi_0 = 1.0\n",
    "n1_phi_1 = -2.0\n",
    "n1_phi_2 = -3.0\n",
    "n1_phi_3 = 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "9d6f1d0b4e6dc3e"
   },
   "outputs": [],
   "source": [
    "# Define a range of input values\n",
    "n1_in = torch.arange(-1, 1, 0.01).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "39bbb42542dc7ccc"
   },
   "outputs": [],
   "source": [
    "# We run the neural network for each of these input values\n",
    "n1_out, *_ = shallow_1_1_3(\n",
    "    n1_in,\n",
    "    relu,\n",
    "    n1_phi_0,\n",
    "    n1_phi_1,\n",
    "    n1_phi_2,\n",
    "    n1_phi_3,\n",
    "    n1_theta_10,\n",
    "    n1_theta_11,\n",
    "    n1_theta_20,\n",
    "    n1_theta_21,\n",
    "    n1_theta_30,\n",
    "    n1_theta_31,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "id": "afbd2263aa082907",
    "outputId": "7d2c0c5f-5d64-4a0e-98fd-2c3c668001a0"
   },
   "outputs": [],
   "source": [
    "# And then plot it\n",
    "plot_neural(n1_in, n1_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "41d07e398a7cb1d1"
   },
   "source": [
    "## Matrix form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "80ac2a9d4210953d"
   },
   "source": [
    "Now we'll define the same neural network, but this time, we will  use matrix form as in equation 4.15.  When you get this right, it will draw the same plot as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "77ccf6451020aede"
   },
   "source": [
    "### Problem 5: Matrix Form of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "960bafb1c406dde1"
   },
   "source": [
    "The problem neural network used above can also be defined in matrix form, namely:\n",
    "$$h_1 = \\text{ReLU}(\\beta_0 + \\Omega_0 x)$$\n",
    "and\n",
    "$$y = \\beta_1 + \\Omega_1 h_1$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fill in the values of the $\\beta$ and $\\Omega$ matrices with the `n1_theta`, `n1_phi`, etc. parameters from the that define the network in the previous example above.\n",
    "\n",
    "NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0].\n",
    "\n",
    "To get you started we've filled in a couple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "id": "768e05ae5b5cecfc"
   },
   "outputs": [],
   "source": [
    "beta_0 = torch.zeros((3, 1))\n",
    "Omega_0 = torch.zeros((3, 1))\n",
    "beta_1 = torch.zeros((1, 1))\n",
    "Omega_1 = torch.zeros((1, 3))\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Fill in beta_0 (biases for hidden layer) and Omega_0 (weights for input->hidden)\n",
    "beta_0[0, 0] = n1_theta_10\n",
    "Omega_0[0, 0] = n1_theta_11\n",
    "beta_0[1, 0] = n1_theta_20\n",
    "Omega_0[1, 0] = n1_theta_21\n",
    "beta_0[2, 0] = n1_theta_30\n",
    "Omega_0[2, 0] = n1_theta_31\n",
    "\n",
    "# Fill in beta_1 (bias for output) and Omega_1 (weights for hidden->output)\n",
    "beta_1[0, 0] = n1_phi_0\n",
    "Omega_1[0, 0] = n1_phi_1\n",
    "Omega_1[0, 1] = n1_phi_2\n",
    "Omega_1[0, 2] = n1_phi_3\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert beta_0.shape == (3, 1), f\"beta_0 should have shape (3, 1), got {beta_0.shape}\"\n",
    "assert Omega_0.shape == (3, 1), f\"Omega_0 should have shape (3, 1), got {Omega_0.shape}\"\n",
    "assert beta_1.shape == (1, 1), f\"beta_1 should have shape (1, 1), got {beta_1.shape}\"\n",
    "assert Omega_1.shape == (1, 3), f\"Omega_1 should have shape (1, 3), got {Omega_1.shape}\"\n",
    "assert torch.isclose(\n",
    "    beta_0[0, 0], torch.tensor(n1_theta_10)\n",
    "), \"beta_0[0,0] should equal n1_theta_10\"\n",
    "assert torch.isclose(\n",
    "    Omega_0[0, 0], torch.tensor(n1_theta_11)\n",
    "), \"Omega_0[0,0] should equal n1_theta_11\"\n",
    "assert torch.isclose(beta_1[0, 0], torch.tensor(n1_phi_0)), \"beta_1[0,0] should equal n1_phi_0\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert torch.isclose(\n",
    "    beta_0[1, 0], torch.tensor(n1_theta_20)\n",
    "), \"beta_0[1,0] should equal n1_theta_20\"\n",
    "assert torch.isclose(\n",
    "    beta_0[2, 0], torch.tensor(n1_theta_30)\n",
    "), \"beta_0[2,0] should equal n1_theta_30\"\n",
    "assert torch.isclose(\n",
    "    Omega_0[1, 0], torch.tensor(n1_theta_21)\n",
    "), \"Omega_0[1,0] should equal n1_theta_21\"\n",
    "assert torch.isclose(\n",
    "    Omega_0[2, 0], torch.tensor(n1_theta_31)\n",
    "), \"Omega_0[2,0] should equal n1_theta_31\"\n",
    "assert torch.isclose(Omega_1[0, 0], torch.tensor(n1_phi_1)), \"Omega_1[0,0] should equal n1_phi_1\"\n",
    "assert torch.isclose(Omega_1[0, 1], torch.tensor(n1_phi_2)), \"Omega_1[0,1] should equal n1_phi_2\"\n",
    "assert torch.isclose(Omega_1[0, 2], torch.tensor(n1_phi_3)), \"Omega_1[0,2] should equal n1_phi_3\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "7cb8b69ba742315a"
   },
   "source": [
    "The following cell will then run the network in matrix form; you can compare whether you obtain the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "id": "442e248bf665e951",
    "outputId": "155ae363-a313-45f9-e570-1ab95b54c1a9"
   },
   "outputs": [],
   "source": [
    "# Make sure that input data matrix has different inputs in its columns\n",
    "n_data = n1_in.numel()\n",
    "n_dim_in = 1\n",
    "n1_in_mat = n1_in.reshape(n_dim_in, n_data)\n",
    "\n",
    "# This runs the network for ALL of the inputs, x at once so we can draw graph\n",
    "h1 = relu(beta_0 + torch.matmul(Omega_0, n1_in_mat))\n",
    "n1_out = beta_1 + torch.matmul(Omega_1, h1)\n",
    "\n",
    "# Draw the network and check that it looks the same as the non-matrix case\n",
    "plot_neural(n1_in, n1_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "id": "de4a20059065ced1"
   },
   "source": [
    "### Composing networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "id": "f6f491108da96061"
   },
   "source": [
    "Now we'll feed the output of the first network into the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "id": "6748ca638bade9db"
   },
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the second neural network\n",
    "n2_theta_10 = -0.6\n",
    "n2_theta_11 = -1.0\n",
    "n2_theta_20 = 0.2\n",
    "n2_theta_21 = 1.0\n",
    "n2_theta_30 = -0.5\n",
    "n2_theta_31 = 1.0\n",
    "n2_phi_0 = 0.5\n",
    "n2_phi_1 = -1.0\n",
    "n2_phi_2 = -1.5\n",
    "n2_phi_3 = 2.0\n",
    "\n",
    "# Define a range of input values\n",
    "n2_in = torch.arange(-1, 1, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "id": "1705dca95686908b"
   },
   "source": [
    "We run the second neural network on the output of the first network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "id": "d6484213c655753b",
    "outputId": "ad5bf393-7a78-4b20-890d-4c3905a5228c"
   },
   "outputs": [],
   "source": [
    "n2_out, *_ = shallow_1_1_3(\n",
    "    n1_out,\n",
    "    relu,\n",
    "    n2_phi_0,\n",
    "    n2_phi_1,\n",
    "    n2_phi_2,\n",
    "    n2_phi_3,\n",
    "    n2_theta_10,\n",
    "    n2_theta_11,\n",
    "    n2_theta_20,\n",
    "    n2_theta_21,\n",
    "    n2_theta_30,\n",
    "    n2_theta_31,\n",
    ")\n",
    "# And then plot it\n",
    "plot_neural(n1_in, n2_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {
    "id": "4df148fa0bbe8aee"
   },
   "source": [
    "### 3-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {
    "id": "803e7c1fc1afebf9"
   },
   "source": [
    "Now let's make a deep network with 3 hidden layers.  It will have $D_i=4$ inputs, $D_1=5$ neurons  in the first layer, $D_2=2$ neurons in the second layer and $D_3=4$ neurons in the third layer, and $D_o = 1$ output.  Consult figure 4.6 and equations 4.15 for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "id": "e285e25f04f2a88c"
   },
   "outputs": [],
   "source": [
    "# define sizes\n",
    "D_i = 4\n",
    "D_1 = 5\n",
    "D_2 = 2\n",
    "D_3 = 4\n",
    "D_o = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "id": "7c5f5bbe698d09b5"
   },
   "source": [
    "### Problem 6: Deep Network Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "id": "213ed496d0742cb0"
   },
   "source": [
    "We'll choose the inputs and parameters of this network randomly using `torch.randn`.\n",
    "For example, we'll set the input using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "id": "8897412f0781373b"
   },
   "outputs": [],
   "source": [
    "n_data = 4\n",
    "x = torch.randn(D_i, n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "id": "4017606687bbcf32"
   },
   "source": [
    "Now, the task is for you to initialize the parameters with Gaussian noise as well. Pay attention to the correct dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "id": "c782ba8865f413e8"
   },
   "outputs": [],
   "source": [
    "n_data = 4\n",
    "x = torch.randn(D_i, n_data)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Initialize all bias vectors and weight matrices with random Gaussian values\n",
    "# Layer 0: input (D_i) -> hidden layer 1 (D_1)\n",
    "beta_0 = torch.randn(D_1, 1)\n",
    "Omega_0 = torch.randn(D_1, D_i)\n",
    "\n",
    "# Layer 1: hidden layer 1 (D_1) -> hidden layer 2 (D_2)\n",
    "beta_1 = torch.randn(D_2, 1)\n",
    "Omega_1 = torch.randn(D_2, D_1)\n",
    "\n",
    "# Layer 2: hidden layer 2 (D_2) -> hidden layer 3 (D_3)\n",
    "beta_2 = torch.randn(D_3, 1)\n",
    "Omega_2 = torch.randn(D_3, D_2)\n",
    "\n",
    "# Layer 3: hidden layer 3 (D_3) -> output (D_o)\n",
    "beta_3 = torch.randn(D_o, 1)\n",
    "Omega_3 = torch.randn(D_o, D_3)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert beta_0.shape == (\n",
    "    D_1,\n",
    "    1,\n",
    "), f\"beta_0 should have shape ({D_1}, 1), got {beta_0.shape}\"\n",
    "assert Omega_0.shape == (\n",
    "    D_1,\n",
    "    D_i,\n",
    "), f\"Omega_0 should have shape ({D_1}, {D_i}), got {Omega_0.shape}\"\n",
    "assert beta_1.shape == (\n",
    "    D_2,\n",
    "    1,\n",
    "), f\"beta_1 should have shape ({D_2}, 1), got {beta_1.shape}\"\n",
    "assert Omega_1.shape == (\n",
    "    D_2,\n",
    "    D_1,\n",
    "), f\"Omega_1 should have shape ({D_2}, {D_1}), got {Omega_1.shape}\"\n",
    "assert beta_2.shape == (\n",
    "    D_3,\n",
    "    1,\n",
    "), f\"beta_2 should have shape ({D_3}, 1), got {beta_2.shape}\"\n",
    "assert Omega_2.shape == (\n",
    "    D_3,\n",
    "    D_2,\n",
    "), f\"Omega_2 should have shape ({D_3}, {D_2}), got {Omega_2.shape}\"\n",
    "assert beta_3.shape == (\n",
    "    D_o,\n",
    "    1,\n",
    "), f\"beta_3 should have shape ({D_o}, 1), got {beta_3.shape}\"\n",
    "assert Omega_3.shape == (\n",
    "    D_o,\n",
    "    D_3,\n",
    "), f\"Omega_3 should have shape ({D_o}, {D_3}), got {Omega_3.shape}\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify matrices are actually filled with values (not zeros)\n",
    "assert not torch.allclose(\n",
    "    beta_0, torch.zeros_like(beta_0)\n",
    "), \"beta_0 should be initialized with random values, not zeros\"\n",
    "assert not torch.allclose(\n",
    "    Omega_0, torch.zeros_like(Omega_0)\n",
    "), \"Omega_0 should be initialized with random values, not zeros\"\n",
    "assert not torch.allclose(\n",
    "    beta_1, torch.zeros_like(beta_1)\n",
    "), \"beta_1 should be initialized with random values, not zeros\"\n",
    "assert not torch.allclose(\n",
    "    Omega_1, torch.zeros_like(Omega_1)\n",
    "), \"Omega_1 should be initialized with random values, not zeros\"\n",
    "assert not torch.allclose(\n",
    "    beta_2, torch.zeros_like(beta_2)\n",
    "), \"beta_2 should be initialized with random values, not zeros\"\n",
    "assert not torch.allclose(\n",
    "    Omega_2, torch.zeros_like(Omega_2)\n",
    "), \"Omega_2 should be initialized with random values, not zeros\"\n",
    "assert not torch.allclose(\n",
    "    beta_3, torch.zeros_like(beta_3)\n",
    "), \"beta_3 should be initialized with random values, not zeros\"\n",
    "assert not torch.allclose(\n",
    "    Omega_3, torch.zeros_like(Omega_3)\n",
    "), \"Omega_3 should be initialized with random values, not zeros\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {
    "id": "9b035e7094d72565"
   },
   "source": [
    "You can test your solution below: if you set the correct sizes, the following code will run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "id": "2f0abe03c977b36a",
    "outputId": "1cb27c3c-7725-4883-87da-a8b13acfb558"
   },
   "outputs": [],
   "source": [
    "h1 = relu(beta_0 + torch.matmul(Omega_0, x))\n",
    "h2 = relu(beta_1 + torch.matmul(Omega_1, h1))\n",
    "h3 = relu(beta_2 + torch.matmul(Omega_2, h2))\n",
    "y = beta_3 + torch.matmul(Omega_3, h3)\n",
    "\n",
    "if h1.shape[0] != D_1 or h1.shape[1] != n_data:\n",
    "    print(\"h1 is wrong shape\")\n",
    "if h2.shape[0] != D_2 or h1.shape[1] != n_data:\n",
    "    print(\"h2 is wrong shape\")\n",
    "if h3.shape[0] != D_3 or h1.shape[1] != n_data:\n",
    "    print(\"h3 is wrong shape\")\n",
    "if y.shape[0] != D_o or h1.shape[1] != n_data:\n",
    "    print(\"Output is wrong shape\")\n",
    "\n",
    "# Print the inputs and outputs\n",
    "print(\"Input data points\")\n",
    "print(x)\n",
    "print(\"Output data points\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {
    "id": "765b610606e9f4c1"
   },
   "source": [
    "## Part 3: Regression and Least Squares Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {
    "id": "c7598374be9c6d2b"
   },
   "source": [
    "We are going to be using a shallow neural network to perform regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "id": "62edf9e6c63e378a"
   },
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def relu(preactivation):\n",
    "    return torch.clamp(preactivation, min=0.0)\n",
    "\n",
    "\n",
    "# Define a shallow neural network\n",
    "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
    "    # Make sure that input data is (1 x n_data) array\n",
    "    n_data = x.numel()\n",
    "    x = x.reshape(1, n_data)\n",
    "\n",
    "    # This runs the network for ALL of the inputs, x at once so we can draw graph\n",
    "    h1 = relu(torch.matmul(beta_0, torch.ones((1, n_data))) + torch.matmul(omega_0, x))\n",
    "    return torch.matmul(beta_1, torch.ones((1, n_data))) + torch.matmul(omega_1, h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "id": "e794a6bd48dc93fe"
   },
   "outputs": [],
   "source": [
    "# Get parameters for model -- we can call this function to easily reset them\n",
    "def get_parameters():\n",
    "    # And we'll create a network that approximately fits it\n",
    "    beta_0 = torch.zeros((3, 1))  # formerly theta_x0\n",
    "    omega_0 = torch.zeros((3, 1))  # formerly theta_x1\n",
    "    beta_1 = torch.zeros((1, 1))  # formerly phi_0\n",
    "    omega_1 = torch.zeros((1, 3))  # formerly phi_x\n",
    "\n",
    "    beta_0[0, 0] = 0.3\n",
    "    beta_0[1, 0] = -1.0\n",
    "    beta_0[2, 0] = -0.5\n",
    "    omega_0[0, 0] = -1.0\n",
    "    omega_0[1, 0] = 1.8\n",
    "    omega_0[2, 0] = 0.65\n",
    "    beta_1[0, 0] = 0.1\n",
    "    omega_1[0, 0] = -2.0\n",
    "    omega_1[0, 1] = -1.0\n",
    "    omega_1[0, 2] = 7.0\n",
    "\n",
    "    return beta_0, omega_0, beta_1, omega_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "id": "49ea69910fa9a0b8"
   },
   "outputs": [],
   "source": [
    "# Utility function for plotting data\n",
    "def plot_univariate_regression(\n",
    "    x_model, y_model, x_data=None, y_data=None, sigma_model=None, title=None\n",
    "):\n",
    "    # Make sure model data are 1D arrays\n",
    "    x_model = x_model.squeeze()\n",
    "    y_model = y_model.squeeze()\n",
    "\n",
    "    _fig, ax = plt.subplots()\n",
    "    ax.plot(x_model, y_model)\n",
    "    if sigma_model is not None:\n",
    "        ax.fill_between(\n",
    "            x_model,\n",
    "            y_model - 2 * sigma_model,\n",
    "            y_model + 2 * sigma_model,\n",
    "            color=\"lightgray\",\n",
    "        )\n",
    "    ax.set_xlabel(r\"Input, $x$\")\n",
    "    ax.set_ylabel(r\"Output, $y$\")\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.set_aspect(0.5)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    if x_data is not None:\n",
    "        ax.plot(x_data, y_data, \"ko\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {
    "id": "8b44c8f399a07ca4"
   },
   "source": [
    "## Univariate regression\n",
    "\n",
    "We'll investigate a simple univariate regression situation with a single input $x$ and a single output $y$ as pictured in figures 5.4 and 5.5b in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {
    "id": "d3cd0af9aa144850",
    "outputId": "11ff9fa6-5320-4c28-abcd-42de425dea31"
   },
   "outputs": [],
   "source": [
    "# Let's create some 1D training data\n",
    "x_train = torch.tensor(\n",
    "    [\n",
    "        0.09291784,\n",
    "        0.46809093,\n",
    "        0.93089486,\n",
    "        0.67612654,\n",
    "        0.73441752,\n",
    "        0.86847339,\n",
    "        0.49873225,\n",
    "        0.51083168,\n",
    "        0.18343972,\n",
    "        0.99380898,\n",
    "        0.27840809,\n",
    "        0.38028817,\n",
    "        0.12055708,\n",
    "        0.56715537,\n",
    "        0.92005746,\n",
    "        0.77072270,\n",
    "        0.85278176,\n",
    "        0.05315950,\n",
    "        0.87168699,\n",
    "        0.58858043,\n",
    "    ]\n",
    ")\n",
    "y_train = torch.tensor(\n",
    "    [\n",
    "        -0.25934537,\n",
    "        0.18195445,\n",
    "        0.651270150,\n",
    "        0.13921448,\n",
    "        0.09366691,\n",
    "        0.30567674,\n",
    "        0.372291170,\n",
    "        0.20716968,\n",
    "        -0.08131792,\n",
    "        0.51187806,\n",
    "        0.16943738,\n",
    "        0.3994327,\n",
    "        0.019062570,\n",
    "        0.55820410,\n",
    "        0.452564960,\n",
    "        -0.1183121,\n",
    "        0.02957665,\n",
    "        -1.24354444,\n",
    "        0.248038840,\n",
    "        0.26824970,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get parameters for the model\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "sigma = 0.2\n",
    "\n",
    "# Define a range of input values\n",
    "x_model = torch.arange(0, 1, 0.01)\n",
    "# Run the model to get values to plot and plot it.\n",
    "y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {
    "id": "d9b403d7596c72"
   },
   "source": [
    "The blue line is the mean prediction of the model and the gray area represents plus/minus two standard deviations.  This model fits okay, but could be improved. Let's compute the loss.  We'll compute the  the least squares error, the likelihood, the negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {
    "id": "cf71da7959f81e62"
   },
   "source": [
    "### Problem 7: Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {
    "id": "3c46a56852a4a6b5"
   },
   "source": [
    "Implement a function that returns the probability under the normal distribution.\n",
    "$$\\operatorname{Pr}\\left(y \\mid \\mu, \\sigma^2\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left[-\\frac{(y-\\mu)^2}{2 \\sigma^2}\\right]$$\n",
    "\n",
    "You will need `torch.sqrt()` and `torch.exp()`, and `math.pi`.\n",
    "(Don't use a pre-built distribution -- that's cheating!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "id": "462e7953d8d7fc47"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def normal_distribution(y, mu, sigma):\n",
    "    # BEGIN SOLUTION\n",
    "    # Gaussian probability density function\n",
    "    return (\n",
    "        1\n",
    "        / (torch.sqrt(torch.tensor(2 * math.pi * sigma**2)))\n",
    "        * torch.exp(-((y - mu) ** 2) / (2 * sigma**2))\n",
    "    )\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "id": "1332c91efdc8dac2",
    "outputId": "82d64d72-d287-4a02-a6b4-800799c3b51e"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "print(\n",
    "    \"Correct answer = {:3.3f}, Your answer = {:3.3f}\".format(\n",
    "        0.119, normal_distribution(torch.tensor(1.0), torch.tensor(-1.0), 2.3).item()\n",
    "    )\n",
    ")\n",
    "\n",
    "assert torch.isclose(\n",
    "    normal_distribution(torch.tensor(1.0), torch.tensor(-1.0), 2.3),\n",
    "    torch.tensor(0.119),\n",
    "    atol=0.001,\n",
    "), \"normal_distribution(1, -1, 2.3) should be approximately 0.119\"\n",
    "assert torch.isclose(\n",
    "    normal_distribution(torch.tensor(0.0), torch.tensor(0.0), 1),\n",
    "    torch.tensor(1 / math.sqrt(2 * math.pi)),\n",
    "    atol=1e-6,\n",
    "), \"normal_distribution at mean with sigma=1 should be 1/sqrt(2*pi)\"\n",
    "assert normal_distribution(torch.tensor(0.0), torch.tensor(0.0), 1) > normal_distribution(\n",
    "    torch.tensor(1.0), torch.tensor(0.0), 1\n",
    "), \"Probability should be higher at the mean\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert torch.isclose(\n",
    "    normal_distribution(torch.tensor(0.0), torch.tensor(0.0), 1),\n",
    "    torch.tensor(0.3989422804014327),\n",
    "    atol=1e-6,\n",
    "), \"Normal distribution at mean with sigma=1 should be ~0.399\"\n",
    "assert torch.isclose(\n",
    "    normal_distribution(torch.tensor(2.0), torch.tensor(2.0), 0.5),\n",
    "    torch.tensor(0.7978845608028654),\n",
    "    atol=1e-6,\n",
    "), \"Normal distribution at mean with sigma=0.5 should be ~0.798\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {
    "id": "68a7bf17c455d5c6"
   },
   "source": [
    "### Problem 8: Gaussian Distribution Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {
    "id": "e94362d1f71564ef"
   },
   "source": [
    "Let's plot the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "id": "8e3ad297dafe8672",
    "outputId": "d35fa07c-e067-4210-ecb4-47e16b97f2a0"
   },
   "outputs": [],
   "source": [
    "y_gauss = torch.arange(-5, 5, 0.1)\n",
    "mu = 0\n",
    "sigma = 1.0\n",
    "gauss_prob = normal_distribution(y_gauss, mu, sigma)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_gauss, gauss_prob)\n",
    "ax.set_xlabel(r\"Input, $y$\")\n",
    "ax.set_ylabel(r\"Probability $Pr(y)$\")\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([0, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "id": "9cb802b901efc301"
   },
   "source": [
    "1. Predict what will happen if we change to mu=1 and leave sigma=1\n",
    "(you can  change the code above and see if you were correct)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {
    "id": "f28a156393bcd7f4"
   },
   "source": [
    "(Type your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "id": "7e3390cf97e06728"
   },
   "source": [
    "2. Predict what will happen if we leave mu = 0 and change sigma to 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {
    "id": "ab83e0c308794dad"
   },
   "source": [
    "(Type your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {
    "id": "17c8da9e943de148"
   },
   "source": [
    "3. Predict what will happen if we leave mu = 0 and change sigma to 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {
    "id": "9b0f9bb0c5c3b6b8"
   },
   "source": [
    "(Type your answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 8 verification: students should have answered the questions above\n",
    "# This cell verifies understanding of Gaussian distribution behavior\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Expected answers (for instructor reference):\n",
    "# 1. mu=1: The curve shifts right by 1 unit (peak moves from x=0 to x=1)\n",
    "# 2. sigma=2: The curve becomes wider/flatter (more spread out)\n",
    "# 3. sigma=0.5: The curve becomes narrower/taller (less spread out)\n",
    "problem_8_completed = True\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert problem_8_completed, \"Please complete the text answers above\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert isinstance(problem_8_completed, bool), \"problem_8_completed should be a boolean\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"Problem 8 complete - verify your text answers above match the expected behavior!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {
    "id": "1767aff14c18d149"
   },
   "source": [
    "## Likelihood, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "id": "f3921cf047170857"
   },
   "source": [
    "### Problem 9: Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {
    "id": "5e920f10d45b9e9d"
   },
   "source": [
    "Now let's compute the likelihood using this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {
    "id": "850140e605d60002"
   },
   "source": [
    "$$\n",
    "\\prod_{i=1}^I \\operatorname{Pr}\\left(\\mathbf{y}_i \\mid \\mathbf{f}\\left[\\mathbf{x}_i, \\phi\\right]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {
    "id": "8a8afcf1aa561b68"
   },
   "outputs": [],
   "source": [
    "# Return the likelihood of all of the data under the model\n",
    "def compute_likelihood(y_train, mu, sigma):\n",
    "    # BEGIN SOLUTION\n",
    "    # Product of individual Gaussian probabilities\n",
    "    return torch.prod(normal_distribution(y_train, mu, sigma))\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_likelihood = compute_likelihood(torch.tensor([0.0]), torch.tensor([0.0]), 1.0)\n",
    "expected_test = 1 / math.sqrt(2 * math.pi)\n",
    "\n",
    "assert torch.isclose(\n",
    "    test_likelihood, torch.tensor(expected_test), atol=1e-6\n",
    "), f\"Likelihood for single zero should be {expected_test}\"\n",
    "assert (\n",
    "    compute_likelihood(torch.tensor([0.0, 0.0]), torch.tensor([0.0, 0.0]), 1.0) < test_likelihood\n",
    "), \"Likelihood should decrease with more data points\"\n",
    "\n",
    "# Compute likelihood on training data\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "sigma = 0.2\n",
    "likelihood = compute_likelihood(y_train, mu_pred, sigma)\n",
    "print(\"Correct answer = {:9.9f}, Your answer = {:9.9f}\".format(0.000010624, likelihood.item()))\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "simple_likelihood = compute_likelihood(torch.tensor([0.0, 0.0]), torch.tensor([0.0, 0.0]), 1.0)\n",
    "expected_simple = (1 / math.sqrt(2 * math.pi)) ** 2\n",
    "assert torch.isclose(\n",
    "    simple_likelihood, torch.tensor(expected_simple), atol=1e-6\n",
    "), f\"Likelihood for zeros should be {expected_simple}, got {simple_likelihood}\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {
    "id": "21aa8a421225f0ba"
   },
   "source": [
    "You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well.  This is because it is the product of several probabilities, which are all quite small themselves.\n",
    "This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "id": "c868c5a9cca04f28"
   },
   "source": [
    "### Problem 10: Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {
    "id": "ce4f435fff2430dc"
   },
   "source": [
    "This is why we use negative log likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {
    "id": "dcbc71f88c3bbda7"
   },
   "source": [
    "$$\n",
    "-\\sum_{i=1}^I \\log \\left[\\operatorname{Pr}\\left(\\mathbf{y}_i \\mid \\mathbf{f}\\left[\\mathbf{x}_i, \\phi\\right]\\right)\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {
    "id": "7d7457ba53810b0b"
   },
   "outputs": [],
   "source": [
    "# Return the negative log likelihood of the data under the model\n",
    "def compute_negative_log_likelihood(y_train, mu, sigma):\n",
    "    # BEGIN SOLUTION\n",
    "    # Sum of negative log probabilities\n",
    "    return -torch.sum(torch.log(normal_distribution(y_train, mu, sigma)))\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "# Visible test assertions\n",
    "test_nll_single = compute_negative_log_likelihood(torch.tensor([0.0]), torch.tensor([0.0]), 1.0)\n",
    "expected_nll_single = -math.log(1 / math.sqrt(2 * math.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_nll_single = compute_negative_log_likelihood(torch.tensor([0.0]), torch.tensor([0.0]), 1.0)\n",
    "expected_nll_single = -math.log(1 / math.sqrt(2 * math.pi))\n",
    "\n",
    "assert torch.isclose(\n",
    "    test_nll_single, torch.tensor(expected_nll_single), atol=1e-6\n",
    "), f\"NLL for single zero should be {expected_nll_single}\"\n",
    "test_nll_double = compute_negative_log_likelihood(\n",
    "    torch.tensor([0.0, 0.0]), torch.tensor([0.0, 0.0]), 1.0\n",
    ")\n",
    "assert torch.isclose(\n",
    "    test_nll_double, torch.tensor(2 * expected_nll_single), atol=1e-6\n",
    "), \"NLL should be additive\"\n",
    "\n",
    "# Compute NLL on training data\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "sigma = 0.2\n",
    "nll = compute_negative_log_likelihood(y_train, mu_pred, sigma)\n",
    "print(\"Correct answer = {:9.9f}, Your answer = {:9.9f}\".format(11.452419564, nll.item()))\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "simple_nll = compute_negative_log_likelihood(\n",
    "    torch.tensor([0.0, 0.0]), torch.tensor([0.0, 0.0]), 1.0\n",
    ")\n",
    "expected_simple_nll = -2 * math.log(1 / math.sqrt(2 * math.pi))\n",
    "assert torch.isclose(\n",
    "    simple_nll, torch.tensor(expected_simple_nll), atol=1e-6\n",
    "), f\"NLL for zeros with sigma=1 should be {expected_simple_nll}, got {simple_nll}\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {
    "id": "da4c9179cbf39c0e"
   },
   "source": [
    "### Problem 11: Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {
    "id": "f203c6b8bdce2666"
   },
   "source": [
    "For good measure, let's compute the sum of squares as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {
    "id": "5c4e4a5791c8431e"
   },
   "source": [
    "$$\n",
    "\\sum_{i=1}^I\\left(y_i-\\mathrm{f}\\left[\\mathbf{x}_i, \\phi\\right]\\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {
    "id": "48c1d4e5c7455735"
   },
   "outputs": [],
   "source": [
    "# Return squared distance between observed and predicted values\n",
    "def compute_sum_of_squares(y_train, y_pred):\n",
    "    # BEGIN SOLUTION\n",
    "    # Sum of squared differences\n",
    "    return torch.sum((y_train - y_pred) ** 2)\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "# Visible test assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert torch.isclose(\n",
    "    compute_sum_of_squares(torch.tensor([1.0, 2.0]), torch.tensor([1.0, 2.0])),\n",
    "    torch.tensor(0.0),\n",
    "), \"SOS should be 0 for identical arrays\"\n",
    "assert torch.isclose(\n",
    "    compute_sum_of_squares(torch.tensor([0.0, 0.0]), torch.tensor([1.0, 0.0])),\n",
    "    torch.tensor(1.0),\n",
    "), \"SOS for [0,0] vs [1,0] should be 1\"\n",
    "assert torch.isclose(\n",
    "    compute_sum_of_squares(torch.tensor([0.0, 0.0]), torch.tensor([3.0, 4.0])),\n",
    "    torch.tensor(25.0),\n",
    "), \"SOS for [0,0] vs [3,4] should be 25 (3^2+4^2)\"\n",
    "\n",
    "# Compute SOS on training data\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "y_pred = mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "sum_of_squares = compute_sum_of_squares(y_train, y_pred)\n",
    "print(\"Correct answer = {:9.9f}, Your answer = {:9.9f}\".format(2.020992572, sum_of_squares.item()))\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "simple_sos = compute_sum_of_squares(torch.tensor([1.0, 2.0, 3.0]), torch.tensor([2.0, 3.0, 4.0]))\n",
    "assert torch.isclose(\n",
    "    simple_sos, torch.tensor(3.0), atol=1e-6\n",
    "), f\"Sum of squares for [1,2,3] vs [2,3,4] should be 3.0, got {simple_sos}\"\n",
    "simple_sos2 = compute_sum_of_squares(\n",
    "    torch.tensor([0.0, 0.0, 0.0, 0.0]), torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    ")\n",
    "assert torch.isclose(\n",
    "    simple_sos2, torch.tensor(30.0), atol=1e-6\n",
    "), f\"Sum of squares for [0,0,0,0] vs [1,2,3,4] should be 30.0, got {simple_sos2}\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
