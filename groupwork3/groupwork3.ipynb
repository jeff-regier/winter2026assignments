{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HHpTrsPr9Qk"
   },
   "source": [
    "# DATASCI 315, Group Work 3: Logistic Regression and Maximum Likelihood\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!** Upon completion, one member of the team should submit their team's work through Canvas **as html**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5EfFv_z1vbo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCplwtJR129o"
   },
   "source": [
    "In binary classification problems, responses are typically modeled as Bernoulli random variables. Recall that the Bernoulli distribution has the following probability mass function:\n",
    "$$\n",
    "P(y;p) = p^{y} \\cdot (1-p)^{1-y},\n",
    "$$\n",
    "where $y \\in \\{0, 1\\}$ and $p \\in [0, 1]$ is the rate parameter.\n",
    "You can easily verify that $P(y=1;p)=p$ and $P(y=0;p)=1-p$; the probability of observing event $y=1$ is $p$ and $y=0$ is $1-p$.\n",
    "\n",
    "## Problem 1: Bernoulli Distribution\n",
    "\n",
    "Write a function that returns the Bernoulli probability for a given response $y$ and rate $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q17QAUZJ28_a"
   },
   "outputs": [],
   "source": [
    "# Return probability under Bernoulli distribution for observed class y\n",
    "def bernoulli_distribution(y, prob):\n",
    "    # BEGIN SOLUTION\n",
    "    # Apply the Bernoulli PMF formula: P(y;p) = p^y * (1-p)^(1-y)\n",
    "    return prob**y * (1 - prob) ** (1 - y)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1738882073204,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "mmQpo4Y_3TsB",
    "outputId": "5043b621-7eae-46a0-b479-774b62a32d5f"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test cases for bernoulli_distribution\n",
    "assert abs(bernoulli_distribution(0, 0.2) - 0.8) < 1e-9, \"P(y=0; p=0.2) should be 0.8\"\n",
    "assert abs(bernoulli_distribution(1, 0.2) - 0.2) < 1e-9, \"P(y=1; p=0.2) should be 0.2\"\n",
    "assert abs(bernoulli_distribution(1, 0.7) - 0.7) < 1e-9, \"P(y=1; p=0.7) should be 0.7\"\n",
    "assert abs(bernoulli_distribution(0, 0.7) - 0.3) < 1e-9, \"P(y=0; p=0.7) should be 0.3\"\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert True  # placeholder hidden test\n",
    "# END HIDDEN TESTS\n",
    "\n",
    "print(\"All bernoulli_distribution tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB0CAs_h3jrM"
   },
   "source": [
    "## Problem 2: Likelihood\n",
    "\n",
    "The previous problem shows how to compute the probability of a single event.\n",
    "The likelihood of the data is the product of all individual probabilities:\n",
    "$$\n",
    "\\mathrm{Likelihood} = \\prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}\n",
    "$$\n",
    "\n",
    "Write a function that computes the likelihood using the given data.\n",
    "You should use broadcasting to compute the likelihood.\n",
    "For loops will not be accepted as a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVqcKjJ13dMo"
   },
   "outputs": [],
   "source": [
    "# Return the likelihood of all of the data under the model\n",
    "def compute_likelihood(y_train, prob):\n",
    "    # TODO: Compute the likelihood of the data\n",
    "    # You should use torch.prod() and your bernoulli_distribution function above\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute individual probabilities using Bernoulli PMF, then take product\n",
    "    individual_probs = bernoulli_distribution(y_train, prob)\n",
    "    return torch.prod(individual_probs)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1738882080740,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "V2KubQ6jPqG9",
    "outputId": "7b4c9055-cefe-4910-ba23-5db0d495418e"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test cases for compute_likelihood\n",
    "prob_test = torch.tensor(\n",
    "    [\n",
    "        0.09291784,\n",
    "        0.46809093,\n",
    "        0.93089486,\n",
    "        0.67612654,\n",
    "        0.73441752,\n",
    "        0.86847339,\n",
    "        0.49873225,\n",
    "        0.51083168,\n",
    "        0.18343972,\n",
    "        0.99380898,\n",
    "        0.27840809,\n",
    "        0.38028817,\n",
    "        0.12055708,\n",
    "        0.56715537,\n",
    "        0.92005746,\n",
    "        0.77072270,\n",
    "        0.85278176,\n",
    "        0.05315950,\n",
    "        0.87168699,\n",
    "        0.58858043,\n",
    "    ]\n",
    ")\n",
    "y_train_test = torch.tensor(\n",
    "    [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], dtype=torch.float32\n",
    ")\n",
    "likelihood = compute_likelihood(y_train_test, prob_test)\n",
    "\n",
    "assert abs(likelihood - 0.000069919) < 1e-9, f\"Expected ~0.000069919, got {likelihood}\"\n",
    "assert compute_likelihood(torch.tensor([1.0]), torch.tensor([0.5])) == 0.5, \"Single obs failed\"\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert True  # placeholder hidden test\n",
    "# END HIDDEN TESTS\n",
    "\n",
    "print(\"All compute_likelihood tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4f72Uel4fA3"
   },
   "source": [
    "## Problem 3: Negative Log-Likelihood\n",
    "\n",
    "Likelihood is conceptually important, but is difficult to store inside a computer since it quickly approaches zero as the number of data points grows. Therefore, it is wise to compute its log-transformed version instead of the original quantity:\n",
    "$$\n",
    "\\mathrm{nLL} = -\\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n",
    "$$\n",
    "This is also called the cross-entropy loss.\n",
    "\n",
    "Write a function that computes the negative log-likelihood using the following data.\n",
    "You should not simply surround the answer from the previous question with a log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0_J3sMp4-bi"
   },
   "outputs": [],
   "source": [
    "# Return the negative log likelihood of the data under the model\n",
    "def compute_negative_log_likelihood(y_train, prob):\n",
    "    # TODO: Compute the negative log-likelihood directly\n",
    "    # (don't use the likelihood function above)\n",
    "    # You will need torch.sum(), torch.log()\n",
    "    # BEGIN SOLUTION\n",
    "    # Apply the NLL formula: -sum[y*log(p) + (1-y)*log(1-p)]\n",
    "    return -torch.sum(y_train * torch.log(prob) + (1 - y_train) * torch.log(1 - prob))\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1738883919331,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "QeBiGvrSP9Vj",
    "outputId": "e973d498-edcf-499e-8389-e488dedebc99"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test cases for compute_negative_log_likelihood\n",
    "prob_test = torch.tensor(\n",
    "    [\n",
    "        0.09291784,\n",
    "        0.46809093,\n",
    "        0.93089486,\n",
    "        0.67612654,\n",
    "        0.73441752,\n",
    "        0.86847339,\n",
    "        0.49873225,\n",
    "        0.51083168,\n",
    "        0.18343972,\n",
    "        0.99380898,\n",
    "        0.27840809,\n",
    "        0.38028817,\n",
    "        0.12055708,\n",
    "        0.56715537,\n",
    "        0.92005746,\n",
    "        0.77072270,\n",
    "        0.85278176,\n",
    "        0.05315950,\n",
    "        0.87168699,\n",
    "        0.58858043,\n",
    "    ]\n",
    ")\n",
    "y_train_test = torch.tensor(\n",
    "    [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Test with repeated data (100x) to check numerical stability\n",
    "prob_repeated = prob_test.repeat(100)\n",
    "y_repeated = y_train_test.repeat(100)\n",
    "\n",
    "nll = compute_negative_log_likelihood(y_repeated, prob_repeated)\n",
    "assert abs(nll - 956.8168950) < 1e-4, f\"Expected ~956.8168950, got {nll}\"\n",
    "\n",
    "# Test with simple case\n",
    "nll_simple = compute_negative_log_likelihood(torch.tensor([1.0, 0.0]), torch.tensor([0.8, 0.3]))\n",
    "expected_simple = -torch.log(torch.tensor(0.8)) - torch.log(torch.tensor(0.7))\n",
    "assert abs(nll_simple - expected_simple) < 1e-6, \"Simple test failed\"\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert True  # placeholder hidden test\n",
    "# END HIDDEN TESTS\n",
    "\n",
    "print(\"All compute_negative_log_likelihood tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A6GwKrFtgyW"
   },
   "source": [
    "## Introduction to Logistic Regression\n",
    "\n",
    "Now we move onto logistic regression.\n",
    "In the previous problems, we immediately had access to $p_i$ for each observation.\n",
    "In real-world examples, we don't have direct access to $p_i$, but have to compute it from a set of characteristics in vectors $x_i$ ($i=1,\\ldots,n$).\n",
    "$p_i$ is given as a function of $x_i$ such that\n",
    "$$p_i = f(x_i)$$\n",
    "Logistic regression assumes that $f$ is a linear function of $x_i$.\n",
    "\n",
    "Let's look at the Wisconsin Breast Cancer dataset where there are 569 samples and 30 variables. We aim to classify tumors as malignant (1) or benign (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1738883924933,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "-LXVmEwc3ZmX",
    "outputId": "591708c5-3bcd-4a68-e6a4-73b990e24410"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data_cancer = load_breast_cancer()\n",
    "X = data_cancer.data\n",
    "y = data_cancer.target\n",
    "print(y)\n",
    "print(data_cancer.feature_names)\n",
    "print(data_cancer.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DUSKlYn5M8d"
   },
   "source": [
    "We will split this data set into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZTMKSf955z6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYsSg82Y6HRo"
   },
   "source": [
    "We can now fit a logistic regression model on this data. The solver \"sag\" refers to \"stochastic average gradient descent\", which is a variant of gradient descent algorithm that we will implement below. We specify 1000 to be the maximum number of iterations that our algorithm can run. The tolerance parameter of 0.001 is the stopping criterion similar to epsilon that we will use.\n",
    "\n",
    "The following code trains a logistic regression model.\n",
    "It learns the function $f$ that maps $x$ to $p$ as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sIJXZra7Ajr"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty=None, solver=\"sag\", tol=0.001, max_iter=1000, random_state=42).fit(\n",
    "    X_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYo0kdrt8zcx"
   },
   "source": [
    "We can now get hard label and soft label predictions for $X_{\\text{test}}$. Hard labels tell us exactly what type of tumor it is and soft labels give the probability that a tumor is malignant or benign.\n",
    "\n",
    "The following step computes the predicted probability $p_i = f(x_i)$ using the learned function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1738883932044,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "1yLVQc7h9JK6",
    "outputId": "95b608bb-5931-44f8-ab1c-e65823bfe0da"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKTrhgb9wrvd"
   },
   "source": [
    "In some application, just getting a prediction is not sufficient. We may want to assign some confidence score to our predictions. This can be done by looking at probability of each label for every single data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1738883933894,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "T4RMUnvfwtzt",
    "outputId": "4c259e6c-1da7-4724-dfb4-817401b9a158"
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "probs_y = clf.predict_proba(X_test)\n",
    "print(torch.round(torch.tensor(probs_y), decimals=2)[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRzKzNwgxJEm"
   },
   "source": [
    "As we can see, the model only has 54% confidence on the label it predicted. It is important to note that this is not a rigorous uncertainty quantification and there is no coverage guarantee of this uncertainty like our traditional statistical inference gives. Nevertheless, the probability here gives us a good heuristic on reliability of individual predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J4SzEnj9XER"
   },
   "source": [
    "We can now make a confusion matrix to evaluate the predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1738883935608,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "XyZengV79cvK",
    "outputId": "94ed7295-fc13-4e31-9a0c-417cb07a3a87"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# |true 0      | true 0.     |\n",
    "# |predicted 0 | predicted 1 |\n",
    "# ----------------------------\n",
    "# | true 1     | true 1.     |\n",
    "# |predicted 0 | predicted 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kyH4ZeNP9YO"
   },
   "source": [
    "## Problem 5: Sigmoid Function\n",
    "\n",
    "\\begin{align*}\n",
    "\\widehat{p}_i &= \\sigma(w^Tx_i) = \\frac{e^{w^Tx_i}}{1 + e^{w^Tx_i}} = \\frac{1}{1+ e^{-w^Tx_i}} \\\\\n",
    "\\widehat{P} &= \\begin{bmatrix} \\widehat{p}_1 \\\\\n",
    "\\widehat{p}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\widehat{p}_n\\end{bmatrix} \\\\\n",
    "X &= \\begin{bmatrix} x^T_1 \\\\\n",
    "x^T_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^T_n \\end{bmatrix}\n",
    "\\end{align*}\n",
    "where $x_i$ is an instance of $X$ with dimensions $p \\times 1$.\n",
    "\n",
    "Build a function that applies the sigmoid function to $X$ with dimensions $n \\times p$ with a given weight vector $w$ of shape $(p,)$ or $(p, 1)$, and returns $\\widehat{P}$ with dimensions $n \\times 1$. Assume the intercept vector is already included in $X$.\n",
    "\n",
    "$\\sigma(\\cdot)$ is the sigmoid function that you will implement.\n",
    "As before, you should not use for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FzGm8ss_XOY"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_cancer = load_breast_cancer()\n",
    "X_wis = data_cancer.data\n",
    "y_wis = data_cancer.target\n",
    "X_train_wis, X_test_wis, y_train_wis, y_test_wis = train_test_split(\n",
    "    X_wis, y_wis, test_size=0.3, random_state=42\n",
    ")\n",
    "sc_2 = StandardScaler()\n",
    "X_transform_wis = sc_2.fit_transform(X_train_wis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtlK7SWZQDGM"
   },
   "outputs": [],
   "source": [
    "def sigmoid(features, weights):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for logistic regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : Tensor of shape (n, p)\n",
    "        Feature matrix\n",
    "    weights : Tensor of shape (p,) or (p, 1)\n",
    "        Weight vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n, 1)\n",
    "        Predicted probabilities\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute z = X @ w, then apply sigmoid: 1 / (1 + exp(-z))\n",
    "    z = features @ weights\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1738883944805,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "4S9SxmraQPVz",
    "outputId": "882fe246-75c4-46a0-e56e-b2e372c034ed"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test cases for sigmoid function\n",
    "torch.manual_seed(42)\n",
    "X_transform_wis_t = torch.tensor(X_transform_wis, dtype=torch.float32)\n",
    "exp_w = torch.randn(X_transform_wis_t.shape[1], 1)\n",
    "sigmoid_output = sigmoid(X_transform_wis_t, exp_w)\n",
    "\n",
    "# Check shape\n",
    "expected_shape = (X_transform_wis_t.shape[0], 1)\n",
    "assert (\n",
    "    sigmoid_output.shape == expected_shape\n",
    "), f\"Expected shape {expected_shape}, got {sigmoid_output.shape}\"\n",
    "\n",
    "# Check values are in [0, 1]\n",
    "assert torch.all(sigmoid_output >= 0) and torch.all(\n",
    "    sigmoid_output <= 1\n",
    "), \"Sigmoid output must be in [0, 1]\"\n",
    "\n",
    "# Check specific values (note: different RNG will give different values)\n",
    "# Just verify the output is valid probabilities\n",
    "assert (\n",
    "    sigmoid_output.min() >= 0.0 and sigmoid_output.max() <= 1.0\n",
    "), \"Sigmoid values must be valid probabilities\"\n",
    "\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert True  # placeholder hidden test\n",
    "# END HIDDEN TESTS\n",
    "\n",
    "print(\"All sigmoid tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF_QcGeIQaoB"
   },
   "source": [
    "## Problem 6: Gradient Descent for Logistic Regression\n",
    "\n",
    "The loss function for Logistic Regression is:\n",
    "\\begin{align*}\n",
    "l_w(\\widehat{p}_i, y_i) &= \\left\\{\n",
    "  \\begin{array}{lr}\n",
    "        -\\log(\\sigma(w^T x_i)), & \\text{if } y_i = 1\\\\\n",
    "        -\\log(1 - \\sigma(w^T x_i)), & \\text{if } y_i = 0\n",
    "    \\end{array}\n",
    "  \\right\\} \\\\\n",
    "  &= -y_i\\log(\\sigma(w^T x_i)) - (1 - y_i)\\log(1 - \\sigma(w^T x_i))\\\\\n",
    "  L(w) &= \\frac{1}{n}\\sum_{i =1}^n l_w(\\widehat{p}_i, y_i)\n",
    "\\end{align*}\n",
    "\n",
    "The gradient of $L(w)$ is as follows:\n",
    "\\begin{align*}\\nabla_w L(w) &= \\frac{1}{n}\\sum_{i =1}^n \\left(\\widehat{p}_i - y_i\\right)x_i \\\\\n",
    "&= \\frac{1}{n} X^T\\left(\\widehat{P} - Y\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Write gradient descent for logistic regression which will output $\\widehat{w}$ and the training loss of the model using $\\widehat{w}$ with a given:\n",
    "- $X$ matrix with dimensions $n \\times p$\n",
    "- $Y$ vector with dimensions $n \\times 1$\n",
    "- $\\eta$ learning rate\n",
    "- $w_0$ initialization for $w$\n",
    "- $\\epsilon$ convergence condition\n",
    "\n",
    "This algorithm should also plot the losses across all iterations and return the final weights and loss.\n",
    "\n",
    "**Note:** Use vectorized operations (no explicit for loops over data points). The only loop allowed is the `while` loop for gradient descent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baCrHaFmSCPH"
   },
   "outputs": [],
   "source": [
    "def log_grad_descent(features, labels, eta, initial_w, epsilon):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for logistic regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : Tensor of shape (n, p)\n",
    "        Feature matrix (without intercept column)\n",
    "    labels : Tensor of shape (n,)\n",
    "        Target labels\n",
    "    eta : float\n",
    "        Learning rate\n",
    "    initial_w : Tensor of shape (p+1, 1)\n",
    "        Initial weights (including intercept)\n",
    "    epsilon : float\n",
    "        Convergence threshold for weight change\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : Tensor of shape (p+1, 1)\n",
    "        Learned weights\n",
    "    loss : float\n",
    "        Final training loss\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    n = features.shape[0]\n",
    "    # Add intercept column\n",
    "    features_aug = torch.hstack((torch.ones((n, 1)), features))\n",
    "    labels_col = labels.reshape((n, 1))\n",
    "\n",
    "    w = initial_w.clone()\n",
    "    losses = []\n",
    "\n",
    "    while True:\n",
    "        w_old = w.clone()\n",
    "\n",
    "        # Forward pass: compute predictions\n",
    "        p_hat = sigmoid(features_aug, w)\n",
    "\n",
    "        # Compute loss (cross-entropy)\n",
    "        loss = -torch.mean(labels_col * torch.log(p_hat) + (1 - labels_col) * torch.log(1 - p_hat))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Compute gradient and update weights\n",
    "        grad = features_aug.T @ (p_hat - labels_col) / n\n",
    "        w = w - eta * grad\n",
    "\n",
    "        # Check convergence\n",
    "        if torch.linalg.norm(w - w_old) < epsilon:\n",
    "            break\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss (Cross-Entropy)\")\n",
    "    plt.title(\"Gradient Descent Convergence\")\n",
    "    plt.grid(visible=True)\n",
    "    plt.show()\n",
    "\n",
    "    return w, loss.item()\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1738883963877,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "J8nmx_9JSKHl",
    "outputId": "e07bc32f-170d-479c-e84d-4b93a720703f"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test gradient descent implementation\n",
    "torch.manual_seed(42)\n",
    "X_transform_wis_t = torch.tensor(X_transform_wis, dtype=torch.float32)\n",
    "y_train_wis_t = torch.tensor(y_train_wis, dtype=torch.float32)\n",
    "new_p = X_transform_wis_t.shape[1] + 1\n",
    "w_graddescent, loss = log_grad_descent(\n",
    "    X_transform_wis_t,\n",
    "    y_train_wis_t,\n",
    "    eta=0.01,\n",
    "    initial_w=torch.randn(new_p, 1),\n",
    "    epsilon=0.001,\n",
    ")\n",
    "\n",
    "# Compute test loss\n",
    "X_test_scaled = torch.tensor(sc_2.transform(X_test_wis), dtype=torch.float32)\n",
    "X_test_aug = torch.hstack((torch.ones((X_test_wis.shape[0], 1)), X_test_scaled))\n",
    "pred_y_test = sigmoid(X_test_aug, w_graddescent)\n",
    "y_test_col = torch.tensor(y_test_wis, dtype=torch.float32).reshape((-1, 1))\n",
    "test_loss = -torch.mean(\n",
    "    y_test_col * torch.log(pred_y_test) + (1 - y_test_col) * torch.log(1 - pred_y_test)\n",
    ")\n",
    "\n",
    "print(f\"Training loss: {loss:.6f}\")\n",
    "print(f\"Test loss: {test_loss.item():.6f}\")\n",
    "print(f\"First 9 weight values (excluding intercept): {w_graddescent[1:10].flatten()}\")\n",
    "\n",
    "# Assertions (note: values may differ slightly due to different RNG)\n",
    "assert loss < 0.5, f\"Training loss should be < 0.5, got {loss}\"\n",
    "assert test_loss < 0.5, f\"Test loss should be < 0.5, got {test_loss}\"\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert True  # placeholder hidden test\n",
    "# END HIDDEN TESTS\n",
    "\n",
    "print(\"All gradient descent tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "12linueOnYx9y8NlhI_IMFzoAH8nOU4ky",
     "timestamp": 1738518872324
    },
    {
     "file_id": "1Klc-Mn-qpnePPFxDzwUWFN-9NFfGoNHt",
     "timestamp": 1738431799997
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
