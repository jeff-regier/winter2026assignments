{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HHpTrsPr9Qk"
   },
   "source": [
    "# DATASCI 315, Homework 4: Multiclass Logistic Regression\n",
    "\n",
    "**Submission instructions:** Upon completion, run the entire notebook, export as HTML, and upload to Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5EfFv_z1vbo"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWZ1hZJsSzq3"
   },
   "source": [
    "### Problem 1: One-Hot Encodings for Multiclass Data\n",
    "\n",
    "In Group Work 4, we explored logistic regression for binary classification. This homework extends logistic regression to more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4QCUKep_7wK"
   },
   "outputs": [],
   "source": [
    "# this part loads the iris dataset from sklearn and splits it\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_iris = load_iris()\n",
    "X_iris = torch.tensor(data_iris.data, dtype=torch.float32)\n",
    "y_iris = torch.tensor(data_iris.target, dtype=torch.long)\n",
    "\n",
    "# add the intercept/bias term to X\n",
    "n, p = X_iris.shape\n",
    "ones = torch.ones((n, 1))\n",
    "X_iris = torch.hstack((ones, X_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpaJPkLSSzzN"
   },
   "source": [
    "If we have a vector $y'$ with dimensions $n \\times 1$ that contains $m$ class labels stored in $l$ with dimensions $m \\times 1$, the one-hot encoding is an $n \\times m$ matrix $Y$ where $Y_{ij} = 1$ if $y'_i = l_j$ and $Y_{ij} = 0$ otherwise.\n",
    "\n",
    "For example, if\n",
    "$$\n",
    "y' = \\begin{bmatrix} \\textrm{Sahana} \\\\\n",
    "\\textrm{Eduardo} \\\\\n",
    "\\textrm{Jake} \\\\\n",
    "\\textrm{Eduardo} \\\\\n",
    "\\textrm{Jake} \\\\\n",
    "\\textrm{Jake} \\\\\n",
    "\\textrm{Eduardo}\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "l = \\begin{bmatrix} \\textrm{Sahana} \\\\\n",
    "\\textrm{Eduardo} \\\\\n",
    "\\textrm{Jake}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "then\n",
    "$$\n",
    "Y = \\begin{bmatrix} 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "0 & 0 & 1\\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "You can think of the first column as indicating whether $y'_i$ is Sahana, the second column as indicating whether $y'_i$ is Eduardo, and third column as indicating whether $y'_i$ is Jake.\n",
    "\n",
    "Write a function `one_hot_encoding(y_prime, l)` that converts class labels to a one-hot encoding:\n",
    "- **Input:** `y_prime` of shape $(n,)$ or $(n, 1)$, and `l` of shape $(m,)$ containing the unique labels\n",
    "- **Output:** `Y` of shape $(n, m)$\n",
    "\n",
    "**Requirements:**\n",
    "- Do not use `for` or `while` loops across observations (loops over the $m$ labels are acceptable since $m$ is typically small)\n",
    "- Use PyTorch vectorization for efficiency\n",
    "\n",
    "**Hint:** You may find [`torch.where`](https://pytorch.org/docs/stable/generated/torch.where.html) useful, or use `torch.nn.functional.one_hot` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Yox5qSgS9BY"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(y_prime, labels):\n",
    "    # BEGIN SOLUTION\n",
    "    # Create n x m matrix of zeros, set column j to 1 where y_prime matches labels[j]\n",
    "    one_hot = torch.zeros((len(y_prime), len(labels)))\n",
    "    for j, val in enumerate(labels):\n",
    "        one_hot[:, j] = (y_prime.ravel() == val).float()\n",
    "    return one_hot\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Jx863ONT1UF"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "labels = torch.arange(3)\n",
    "y_iris_check = y_iris.reshape((y_iris.shape[0], 1))[torch.randint(150, (10,))]\n",
    "one_hot_y = one_hot_encoding(y_iris_check, labels)\n",
    "assert one_hot_y.shape == (10, 3), f\"Expected shape (10, 3), got {one_hot_y.shape}\"\n",
    "assert torch.allclose(one_hot_y.sum(dim=1), torch.ones(10)), \"Each row should sum to 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with different random seed\n",
    "torch.manual_seed(123)\n",
    "y_test = torch.tensor([0, 1, 2, 0, 1])\n",
    "one_hot_test = one_hot_encoding(y_test, torch.arange(3))\n",
    "assert one_hot_test.shape == (5, 3), \"Shape mismatch for 5 samples\"\n",
    "assert torch.equal(one_hot_test[0], torch.tensor([1.0, 0.0, 0.0])), \"First row should be [1, 0, 0]\"\n",
    "assert torch.equal(one_hot_test[2], torch.tensor([0.0, 0.0, 1.0])), \"Third row should be [0, 0, 1]\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N24b4mrjWDN"
   },
   "source": [
    "### Problem 2: Making Predictions with Multiclass Logistic Regression\n",
    "\n",
    "A probabilistic classifier for multiclass data returns a length-$m$ vector of probabilities for each observation. Each entry of this vector is the probability of the observation belonging to the corresponding class. However, model outputs can take arbitrary values, so we need to transform these outputs so that they are nonnegative and sum to 1.\n",
    "\n",
    "The **softmax** function provides this transformation. Suppose that $z_i = (z_{i1}, \\ldots, z_{im})$ is the output of the model for data point $i$. The softmax function $\\sigma$ is defined as:\n",
    "$$\n",
    "\\sigma(z_i) = \\left[\n",
    "  \\frac{e^{z_{i1}}}{\\sum_{j=1}^m e^{z_{ij}}},\n",
    "  \\ldots,\n",
    "  \\frac{e^{z_{im}}}{\\sum_{j=1}^m e^{z_{ij}}}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "In multiclass logistic regression, the probability that an observation with features $x$ belongs to class $j$ is:\n",
    "$$\n",
    "p(y = j \\mid x) = \\sigma(Wx)_j,\n",
    "$$\n",
    "where $W$ is an $m \\times p$ matrix of parameters.\n",
    "\n",
    "Write a function `lr_predict(X, W)` that computes class probabilities:\n",
    "- **Input:** Design matrix $X$ of shape $(n, p)$ and parameter matrix $W$ of shape $(m, p)$\n",
    "- **Output:** Probability matrix $\\hat{P}$ of shape $(n, m)$ where $\\hat{P}_{ij}$ is the probability that observation $i$ belongs to class $j$\n",
    "\n",
    "**Requirements:** Use only NumPy functions (not SciPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_M6xA19CmGdk"
   },
   "outputs": [],
   "source": [
    "def lr_predict(features, weights):\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute logits Z = features @ weights.T, then apply softmax\n",
    "    numerator = torch.exp(features @ weights.T)\n",
    "    denominator = torch.sum(numerator, dim=1, keepdim=True)\n",
    "    return numerator / denominator\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wr97sG1zmN11"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "n, p = X_iris.shape\n",
    "m = labels.shape[0]\n",
    "torch.manual_seed(42)\n",
    "W = torch.randn(m, p)\n",
    "P_hat = lr_predict(X_iris, W)\n",
    "assert P_hat.shape == (n, m), f\"Expected shape ({n}, {m}), got {P_hat.shape}\"\n",
    "assert torch.allclose(torch.sum(P_hat, dim=1), torch.ones(n)), \"Probabilities should sum to 1\"\n",
    "assert torch.all(P_hat >= 0), \"Probabilities should be non-negative\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check probabilities are in valid range\n",
    "assert torch.all(P_hat <= 1), \"Probabilities should be at most 1\"\n",
    "# Test with identity weight matrix\n",
    "W_identity = torch.eye(m, p)\n",
    "P_identity = lr_predict(X_iris[:5], W_identity)\n",
    "assert P_identity.shape == (5, m), \"Shape mismatch with identity weights\"\n",
    "assert torch.allclose(P_identity.sum(dim=1), torch.ones(5)), \"Probabilities should sum to 1\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXPSaCuZgiCG"
   },
   "source": [
    "### Problem 3: Numerically Stable Prediction of Log Probabilities\n",
    "\n",
    "One can naively implement the loss function for multiclass logistic regression by taking the log of `P_hat` from `lr_predict`. However, this can be numerically unstable if `P_hat` contains entries that are nearly zero; applying a log to a variable that is not guaranteed to be strictly positive is inadvisable.\n",
    "\n",
    "To avoid this instability, we compute log probabilities directly using the **log-sum-exp** function:\n",
    "$$\n",
    "\\mathrm{logsumexp}(x_1, \\ldots, x_k) = \\log \\left[ \\sum_{j=1}^k \\exp(x_j) \\right].\n",
    "$$\n",
    "PyTorch provides `torch.logsumexp` which implements this in a numerically stable way.\n",
    "\n",
    "Observe that for log-softmax:\n",
    "$$\n",
    "\\log \\sigma(z_i)_j = z_{ij} - \\log\\left(\\sum_{k=1}^m \\exp(z_{ik})\\right) = z_{ij} - \\mathrm{logsumexp}(z_{i1}, \\ldots, z_{im}).\n",
    "$$\n",
    "\n",
    "Write a function `lr_predict_log(X, W)` that computes log probabilities:\n",
    "- **Input:** Design matrix $X$ of shape $(n, p)$ and parameter matrix $W$ of shape $(m, p)$\n",
    "- **Output:** Log-probability matrix of shape $(n, m)$\n",
    "\n",
    "**Hint:** Use `torch.logsumexp` with the `dim` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzJomVzmioSi"
   },
   "outputs": [],
   "source": [
    "def lr_predict_log(features, weights):\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute logits and subtract logsumexp for numerical stability\n",
    "    logits = features @ weights.T\n",
    "    return logits - torch.logsumexp(logits, dim=1, keepdim=True)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_uuMjAPpstY"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "P_hat_log = lr_predict_log(X_iris, W)\n",
    "assert P_hat_log.shape == (n, m), f\"Expected shape ({n}, {m}), got {P_hat_log.shape}\"\n",
    "assert torch.allclose(\n",
    "    torch.sum(torch.exp(P_hat_log), dim=1), torch.ones(n)\n",
    "), \"exp(log_probs) should sum to 1\"\n",
    "assert torch.all(P_hat_log <= 0), \"Log probabilities should be non-positive\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify consistency with lr_predict\n",
    "P_hat_from_log = torch.exp(P_hat_log)\n",
    "P_hat_direct = lr_predict(X_iris, W)\n",
    "assert torch.allclose(P_hat_from_log, P_hat_direct), \"exp(log_probs) should match probs\"\n",
    "# Test with extreme values to check numerical stability\n",
    "W_extreme = torch.tensor([[100.0, 0, 0, 0, 0], [0, 100.0, 0, 0, 0], [0, 0, 100.0, 0, 0]])\n",
    "P_extreme_log = lr_predict_log(X_iris[:3], W_extreme)\n",
    "assert not torch.any(torch.isnan(P_extreme_log)), \"Should handle extreme values without NaN\"\n",
    "assert not torch.any(torch.isinf(P_extreme_log)), \"Should handle extreme values without Inf\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-43a6KJgT-d0"
   },
   "source": [
    "### Problem 4: Gradient Descent for Multiclass Logistic Regression\n",
    "\n",
    "The **cross-entropy loss** for multiclass logistic regression is:\n",
    "$$\n",
    "L(W) = -\\sum_{i=1}^n \\sum_{j=1}^{m} Y_{ij} \\log \\hat{P}_{ij}\n",
    "$$\n",
    "where $\\hat{P}$ is an $n \\times m$ matrix containing the class probabilities (given the parameters $W$) and $Y$ is the one-hot encoding of the responses.\n",
    "\n",
    "The **gradient** of this loss with respect to $W$ is:\n",
    "$$\n",
    "\\nabla L(W) = (\\hat{P} - Y)^\\top X\n",
    "$$\n",
    "\n",
    "Implement gradient descent to find optimal weights $W$:\n",
    "- **Input:**\n",
    "  - $X$: design matrix of shape $(n, p)$\n",
    "  - $y$: labels of shape $(n,)$\n",
    "  - `eta`: learning rate\n",
    "  - `initial_W`: initial weights of shape $(m, p)$\n",
    "  - `epsilon`: convergence threshold\n",
    "- **Output:** A tuple `(W, loss)` containing the optimized weights and final training loss\n",
    "\n",
    "**Convergence criterion:** Stop when $\\|W_{\\text{new}} - W_{\\text{old}}\\|_2 < \\epsilon$.\n",
    "\n",
    "**Requirements:**\n",
    "- Use `lr_predict_log` for computing log probabilities (for numerical stability)\n",
    "- Use only NumPy (not scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biApDn1UULuC"
   },
   "outputs": [],
   "source": [
    "def lr_train(features, y, eta, initial_weights, epsilon):\n",
    "    # BEGIN SOLUTION\n",
    "    # Gradient descent: update weights until convergence\n",
    "    one_hot_y = one_hot_encoding(y, torch.arange(3))\n",
    "\n",
    "    weights = initial_weights.clone()\n",
    "    while True:\n",
    "        weights_old = weights.clone()\n",
    "        p_hat = lr_predict(features, weights)\n",
    "        grad = (p_hat - one_hot_y).T @ features\n",
    "        weights = weights - eta * grad\n",
    "        if torch.linalg.norm(weights - weights_old) < epsilon:\n",
    "            loss = -torch.sum(lr_predict_log(features, weights) * one_hot_y)\n",
    "            return weights, loss.item()\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkcxMTpZCLE0"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Split the data (using sklearn, then convert to torch)\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X_iris.numpy(), y_iris.numpy(), test_size=0.3, random_state=42\n",
    ")\n",
    "X_train_iris = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test_iris = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_train_iris = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_test_iris = torch.tensor(y_test_np, dtype=torch.long)\n",
    "\n",
    "w_init = torch.randn(m, p)\n",
    "eta = 1e-3\n",
    "epsilon = 0.001\n",
    "W_graddescent, training_loss = lr_train(X_train_iris, y_train_iris, eta, w_init, epsilon)\n",
    "assert training_loss < 15, f\"Training loss {training_loss} is too high\"\n",
    "assert training_loss > 5, f\"Training loss {training_loss} is suspiciously low\"\n",
    "\n",
    "pred_y_test = lr_predict_log(X_test_iris, W_graddescent)\n",
    "Y_test_onehot = one_hot_encoding(y_test_iris.reshape((y_test_iris.shape[0], 1)), torch.arange(m))\n",
    "test_loss = -torch.sum(pred_y_test * Y_test_onehot)\n",
    "assert test_loss < 5, f\"Test loss {test_loss} is too high\"\n",
    "assert test_loss > 1, f\"Test loss {test_loss} is suspiciously low\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check weight matrix shape\n",
    "assert W_graddescent.shape == (m, p), f\"Expected ({m}, {p}), got {W_graddescent.shape}\"\n",
    "# Check that predictions are valid probabilities\n",
    "P_test = lr_predict(X_test_iris, W_graddescent)\n",
    "assert torch.allclose(\n",
    "    P_test.sum(dim=1), torch.ones(P_test.shape[0])\n",
    "), \"Probabilities should sum to 1\"\n",
    "# Check accuracy is reasonable\n",
    "predictions = torch.argmax(P_test, dim=1)\n",
    "accuracy = torch.mean((predictions == y_test_iris).float())\n",
    "assert accuracy > 0.8, f\"Accuracy {accuracy} is too low for Iris dataset\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
