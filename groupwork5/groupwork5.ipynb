{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 315, Group Work 5: Galaxy Count Prediction with Neural Networks\n",
    "\n",
    "In this group work assignment, we will build models to predict the number of galaxies in images.\n",
    "\n",
    "This lab will likely involve revising models, as we want to construct a model that achieves good accuracy.\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. *During lab, feel free to flag down your GSI to ask questions at any point!* Upon completion, one member of the team should submit their team's work through Canvas as HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Download the following files:\n",
    "   - `dataset_train_0.25_norm.pt`\n",
    "   - `dataset_test_0.25_norm.pt`\n",
    "\n",
    "   From one of these locations:\n",
    "   - [Google Drive](https://drive.google.com/drive/folders/1DxOqrdON-_GBGSONRZfMcACyHUz_u8Dh?usp=sharing)\n",
    "   - [Canvas: dataset_train_0.25_norm.pt](https://umich.instructure.com/files/39858361/download?download_frd=1)\n",
    "   - [Canvas: dataset_test_0.25_norm.pt](https://umich.instructure.com/files/39858342/download?download_frd=1)\n",
    "\n",
    "2. Go to the `Files` tab on the left.\n",
    "3. Either create a `data` directory or change the path below.\n",
    "4. Upload the dataset files to the directory by clicking the `Upload` button or dragging the files to the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, n_train = torch.load(f\"data/dataset_train_{sigma}_norm.pt\", weights_only=False)\n",
    "test_images, n_test = torch.load(f\"data/dataset_test_{sigma}_norm.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Display the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's display two random images from the train and test sets along with their corresponding galaxy counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = torch.randint(0, len(train_images))\n",
    "plt.imshow(train_images[random_index], cmap=\"gray\")\n",
    "plt.title(f\"Number of galaxies: {n_train[random_index]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = torch.randint(0, len(test_images))\n",
    "plt.imshow(test_images[random_index], cmap=\"gray\")\n",
    "plt.title(f\"Number of galaxies: {n_test[random_index]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We will predict the number of galaxies in the images using simple feedforward neural networks.\n",
    "\n",
    "The input to the model will be the image, and the output will be the predicted number of galaxies. Since the galaxy count is a discrete variable, we will treat this as a classification problem where each count (0, 1, 2, ..., 6) is a separate class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimension (images are 50x50 pixels)\n",
    "image_dim = 50\n",
    "# Number of classes (galaxy counts range from 0 to 6)\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Problem 1: Define the Model Architecture\n",
    "\n",
    "Define a neural network model architecture that you think will work best for this classification task. For now, limit yourself to `nn.Linear` layers and activation functions (no regularization like dropout).\n",
    "\n",
    "**Requirements:**\n",
    "- Start with `nn.Flatten()` to convert the 2D image to a 1D vector\n",
    "- Use one or more `nn.Linear` layers with appropriate input/output dimensions\n",
    "- Include activation functions (e.g., `nn.ReLU()`) between linear layers\n",
    "- The final layer should output `num_classes` values (one per galaxy count)\n",
    "\n",
    "**Hint:** The input dimension after flattening is `image_dim * image_dim = 2500`. A good starting point is 1-3 hidden layers with 128-512 neurons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Two hidden layers with 512 neurons each, ReLU activations\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(image_dim * image_dim, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, num_classes),\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert hasattr(model, \"forward\"), \"Model must have a forward method\"\n",
    "assert isinstance(model, nn.Module), \"Model must be an nn.Module\"\n",
    "\n",
    "# Test that model produces correct output shape\n",
    "test_input = torch.randn(1, image_dim, image_dim)\n",
    "test_output = model(test_input)\n",
    "expected = (1, num_classes)\n",
    "assert test_output.shape == expected, f\"Expected {expected}, got {test_output.shape}\"\n",
    "\n",
    "print(r\"All model architecture tests passed\\!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify model has at least one hidden layer\n",
    "has_hidden = sum(1 for m in model.modules() if isinstance(m, nn.Linear)) >= 2\n",
    "assert has_hidden, \"Model should have at least one hidden layer\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We provide the following function that resets the model weights. This is useful if you want to retrain a model after changing the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model_parameters(model):\n",
    "    \"\"\"Reset all learnable parameters in the model to their initial values.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, \"reset_parameters\"):\n",
    "            module.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Problem 2: Implement the Training Loop\n",
    "\n",
    "Fill out the `train` function below to train your model.\n",
    "\n",
    "**Requirements:**\n",
    "1. Initialize an optimizer (e.g., `optim.Adam` or `optim.SGD`)\n",
    "2. For each epoch, iterate through the training data in batches:\n",
    "   - Compute the model's predictions\n",
    "   - Calculate the loss using `loss_fn`\n",
    "   - Backpropagate the gradients\n",
    "   - Update the model parameters\n",
    "   - Zero the gradients\n",
    "3. After each epoch, compute and store the training and test losses\n",
    "\n",
    "**Hint:** Remember to call `optimizer.zero_grad()` to clear gradients, `loss.backward()` to compute gradients, and `optimizer.step()` to update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    test_images,\n",
    "    test_labels,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001,\n",
    "):\n",
    "    \"\"\"Train the model and return training and test losses for each epoch.\"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Use cross-entropy loss for multi-class classification\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    reset_model_parameters(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for _epoch in range(num_epochs):\n",
    "        # Training loop: iterate through batches\n",
    "        for batch_images, batch_labels in train_dataloader:\n",
    "            predictions = model(batch_images)\n",
    "            loss = loss_fn(predictions, batch_labels.long())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate on full train and test sets after each epoch\n",
    "        with torch.no_grad():\n",
    "            train_predictions = model(train_images)\n",
    "            train_losses.append(loss_fn(train_predictions, train_labels).item())\n",
    "\n",
    "            test_predictions = model(test_images)\n",
    "            test_losses.append(loss_fn(test_predictions, test_labels).item())\n",
    "\n",
    "    return train_losses, test_losses\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert callable(train), \"train must be a callable function\"\n",
    "print(r\"Training function defined successfully\\!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert True  # Placeholder hidden test\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Next, pick a learning rate and batch size, then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5000\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_images, n_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_images, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "**Note:** You can also adjust the number of epochs if you think the model needs more training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    train_images,\n",
    "    n_train,\n",
    "    test_images,\n",
    "    n_test,\n",
    "    num_epochs=100,\n",
    "    learning_rate=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(test_losses, label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Test Loss Over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Goal: Achieve Target Accuracy\n",
    "\n",
    "**Goal:** Achieve at least 60% accuracy on the test set.\n",
    "\n",
    "If your model is not performing well enough, go back to Problems 1 and 2 to try different model architectures and training hyperparameters, then retrain the model.\n",
    "\n",
    "**Suggestions for improvement:**\n",
    "- Adjust the number and size of hidden layers\n",
    "- Try different learning rates (e.g., 1e-4, 1e-3, 1e-2)\n",
    "- Experiment with batch sizes (e.g., 32, 128, 1000)\n",
    "- Increase the number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_predictions = model(train_images)\n",
    "    test_predictions = model(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = (train_predictions.argmax(dim=1) == n_train).float().mean().item()\n",
    "test_accuracy = (test_predictions.argmax(dim=1) == n_test).float().mean().item()\n",
    "print(f\"Train accuracy: {train_accuracy:.2f}, Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert test_accuracy >= 0.60, (\n",
    "    f\"Test accuracy {test_accuracy:.2f} is below 60%. \"\n",
    "    \"Try adjusting your model architecture or hyperparameters.\"\n",
    ")\n",
    "expected_shape = (len(test_images), num_classes)\n",
    "assert (\n",
    "    test_predictions.shape == expected_shape\n",
    "), rf\"Output shape {test_predictions.shape} \\!= expected {expected_shape}\"\n",
    "print(r\"All tests passed\\!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert test_accuracy >= 0.55, \"Test accuracy should be at least 55%\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci-315",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
