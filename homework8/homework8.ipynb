{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 315, Homework 8: Fashion MNIST with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will apply deep learning to the Fashion MNIST dataset and explore techniques to address underfitting and overfitting.\n",
    "\n",
    "The Fashion MNIST dataset is a popular benchmark for machine learning and computer vision, often used as a drop-in replacement for the original MNIST dataset of handwritten digits. It consists of 70,000 grayscale images, each 28x28 pixels. The dataset is split into 60,000 training images and 10,000 test images. Each image depicts a clothing item or accessory from one of ten classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot.\n",
    "\n",
    "This dataset provides a more challenging classification task than the original MNIST digits, since the images are more complex and contain subtler differences between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch, via torchvision, makes it easy to load the Fashion MNIST dataset. Run the cell below to download the data and create training and validation dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to convert images to tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download the full training dataset\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Define the sizes for train/validation split\n",
    "train_size = int(0.7 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Download the test dataset\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Loss function to use throughout\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with the following function for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=50):\n",
    "    val_losses = []\n",
    "    training_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "            f\"Training Loss: {train_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    return training_losses, val_losses\n",
    "\n",
    "\n",
    "def plot_losses(training_losses, val_losses):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    plt.plot(training_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Underfitting\n",
    "\n",
    "Create a model that underfits the data for at least 50 epochs. Your model should demonstrate:\n",
    "- Training loss that drops steadily but remains above 1.0\n",
    "- Validation loss that remains similar to training loss (within 0.05) throughout training\n",
    "\n",
    "**Hint:** Use a very small hidden layer size (e.g., 2 neurons) to limit the model's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Use a very small hidden layer to limit model capacity and cause underfitting\n",
    "model_underfit = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2, 10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model_underfit.parameters(), lr=0.001)\n",
    "# END SOLUTION\n",
    "\n",
    "training_losses_1, val_losses_1 = train_model(\n",
    "    model_underfit, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "plot_losses(training_losses_1, val_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert training_losses_1[-1] > 1.0, \"Final training loss should be above 1.0 for underfitting\"\n",
    "assert (\n",
    "    abs(training_losses_1[-1] - val_losses_1[-1]) < 0.1\n",
    "), \"Training and validation loss should be similar for underfitting\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses_1) == 50, \"Should train for exactly 50 epochs\"\n",
    "assert training_losses_1[0] > training_losses_1[-1], \"Training loss should decrease\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Overfitting\n",
    "\n",
    "Create a larger model that overfits the data. Your model should demonstrate:\n",
    "- Training loss that drops steadily throughout training\n",
    "- Validation loss that drops at first but then begins increasing\n",
    "\n",
    "**Hint:** Use larger hidden layers (e.g., 128 neurons) to increase the model's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Use larger hidden layers to increase model capacity and cause overfitting\n",
    "model_overfit = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_overfit.parameters(), lr=0.001)\n",
    "# END SOLUTION\n",
    "\n",
    "training_losses_2, val_losses_2 = train_model(\n",
    "    model_overfit, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "plot_losses(training_losses_2, val_losses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert (\n",
    "    training_losses_2[-1] < val_losses_2[-1]\n",
    "), \"Training loss should be lower than validation loss for overfitting\"\n",
    "assert training_losses_2[-1] < 0.3, \"Training loss should be low for an overfit model\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses_2) == 50, \"Should train for exactly 50 epochs\"\n",
    "assert (\n",
    "    min(val_losses_2) < val_losses_2[-1]\n",
    "), \"Validation loss should increase after initial decrease\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Early Stopping\n",
    "\n",
    "Early stopping is a regularization technique that stops training when the validation loss begins to increase. It works by:\n",
    "1. Tracking the best validation loss seen so far\n",
    "2. Incrementing a counter each time the validation loss exceeds the best loss by more than a threshold (`min_delta`)\n",
    "3. Stopping training when this counter reaches a limit (`patience`)\n",
    "\n",
    "Modify the training function below to implement early stopping. The key changes you need to make are:\n",
    "- Update `best_val_loss` when validation improves by at least `min_delta`\n",
    "- Reset `patience_counter` when improvement occurs\n",
    "- Increment `patience_counter` when no improvement\n",
    "- Break out of the training loop when `patience_counter >= patience`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    num_epochs=100,\n",
    "    patience=5,\n",
    "    min_delta=0.01,\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    validation_losses = []\n",
    "    training_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        # Print training and validation loss\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "            f\"Training Loss: {train_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # BEGIN SOLUTION\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset the counter if there's an improvement\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "        # END SOLUTION\n",
    "\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the same architecture from Problem 2 using early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Use the same architecture as Problem 2\n",
    "model_early = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_early.parameters(), lr=0.001)\n",
    "# END SOLUTION\n",
    "\n",
    "training_losses_3, val_losses_3 = train_model_with_early_stopping(\n",
    "    model_early, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "plot_losses(training_losses_3, val_losses_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(training_losses_3) < 50, \"Early stopping should stop training before 50 epochs\"\n",
    "assert len(training_losses_3) > 5, \"Training should run for at least a few epochs\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses_3) == len(val_losses_3), \"Loss lists should have same length\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Smaller Model\n",
    "\n",
    "Try reducing the size of your model from Problem 2 to avoid overfitting. Use smaller hidden layer sizes while keeping the same number of layers.\n",
    "\n",
    "**Hint:** Try using 32 neurons in each hidden layer instead of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Use smaller hidden layers to reduce model capacity\n",
    "model_smaller = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_smaller.parameters(), lr=0.001)\n",
    "# END SOLUTION\n",
    "\n",
    "training_losses_4, val_losses_4 = train_model(\n",
    "    model_smaller, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "plot_losses(training_losses_4, val_losses_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert val_losses_4[-1] < 0.5, \"Validation loss should be reasonably low\"\n",
    "assert (\n",
    "    abs(training_losses_4[-1] - val_losses_4[-1]) < 0.2\n",
    "), \"Gap between training and validation loss should be small\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses_4) == 50, \"Should train for exactly 50 epochs\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: L2 Regularization\n",
    "\n",
    "Use L2 regularization (weight decay) to train the same architecture from Problem 2 without overfitting.\n",
    "\n",
    "**Hint:** Use the `weight_decay` parameter in the Adam optimizer. Try a value like 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Use the same architecture as Problem 2 but with L2 regularization via weight_decay\n",
    "model_l2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_l2.parameters(), lr=0.001, weight_decay=0.01)\n",
    "# END SOLUTION\n",
    "\n",
    "training_losses_5, val_losses_5 = train_model(\n",
    "    model_l2, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "plot_losses(training_losses_5, val_losses_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert val_losses_5[-1] < 0.6, \"Validation loss should be reasonable with L2\"\n",
    "assert training_losses_5[-1] > 0.3, \"L2 should prevent training loss from getting too low\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses_5) == 50, \"Should train for exactly 50 epochs\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Dropout\n",
    "\n",
    "Add dropout to the architecture from Problem 2 to avoid overfitting.\n",
    "\n",
    "**Hint:** Add a `nn.Dropout` layer after one or more of the ReLU activations. Try a dropout probability of 0.5 to 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Add dropout after the first ReLU to regularize the model\n",
    "model_dropout = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.Adam(model_dropout.parameters(), lr=0.001)\n",
    "# END SOLUTION\n",
    "\n",
    "training_losses_6, val_losses_6 = train_model(\n",
    "    model_dropout, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "plot_losses(training_losses_6, val_losses_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert val_losses_6[-1] < 0.5, \"Validation loss should be reasonable with dropout\"\n",
    "assert (\n",
    "    training_losses_6[-1] > val_losses_6[-1] - 0.1\n",
    "), \"With dropout, training loss is typically higher than validation loss\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses_6) == 50, \"Should train for exactly 50 epochs\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: L2 and Dropout Combined\n",
    "\n",
    "Use both L2 regularization and dropout together to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Combine dropout and L2 regularization\n",
    "model_l2dropout = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model_l2dropout.parameters(), lr=0.001, weight_decay=0.01)\n",
    "# END SOLUTION\n",
    "\n",
    "training_losses_7, val_losses_7 = train_model(\n",
    "    model_l2dropout, train_loader, val_loader, optimizer, num_epochs=50\n",
    ")\n",
    "plot_losses(training_losses_7, val_losses_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert val_losses_7[-1] < 0.7, \"Validation loss should be reasonable\"\n",
    "assert training_losses_7[-1] > 0.4, \"Combined regularization should prevent low training loss\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(training_losses_7) == 50, \"Should train for exactly 50 epochs\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Run the code below to print the validation and test loss for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_underfit,\n",
    "    model_overfit,\n",
    "    model_early,\n",
    "    model_smaller,\n",
    "    model_l2,\n",
    "    model_dropout,\n",
    "    model_l2dropout,\n",
    "]\n",
    "model_names = [\n",
    "    \"Underfit\",\n",
    "    \"Overfit\",\n",
    "    \"Early Stopping\",\n",
    "    \"Smaller Model\",\n",
    "    \"L2 Regularization\",\n",
    "    \"Dropout\",\n",
    "    \"L2 and Dropout\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_loss(model, loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            outputs = model(batch_x)\n",
    "            batch_loss = criterion(outputs, batch_y)\n",
    "            loss += batch_loss.item()\n",
    "    return loss / len(loader)\n",
    "\n",
    "\n",
    "for model, model_name in zip(models, model_names, strict=True):\n",
    "    val = get_loss(model, val_loader)\n",
    "    test = get_loss(model, test_loader)\n",
    "    print(f\"{model_name:>20}: validation={val:.4f} test={test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: Model Selection\n",
    "\n",
    "If you had to use one of these models, which one would you use and why? Write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION\n",
    "Based on the results, the Early Stopping model typically achieves the best test loss. This makes sense because it stops training at the point where the model generalizes best to unseen data, before overfitting occurs. The Dropout model also performs well by preventing the model from relying too heavily on any single neuron, which improves generalization.\n",
    "\n",
    "Key observations:\n",
    "- The Underfit model has high loss on both training and test, indicating it lacks the capacity to learn the patterns in the data.\n",
    "- The Overfit model has low training loss but higher test loss, showing it has memorized the training data rather than learning generalizable patterns.\n",
    "- Regularization techniques (early stopping, smaller model, L2, dropout) help reduce the gap between training and test performance.\n",
    "# END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
