{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 315, Group Work Assignment 1: PyTorch Tensor Operations and Matplotlib\n",
    "\n",
    "This group work assignment will introduce you to PyTorch tensors and matplotlib.\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. **During lab, feel free to flag down your GSI to ask questions at any point!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## PyTorch Tensor Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "PyTorch is a popular library for scientific computing and machine learning in Python. It provides a high-performance multidimensional array object called a **tensor**, and tools for working with these tensors. PyTorch tensors are similar to NumPy arrays but can also run on GPUs for faster computation.\n",
    "\n",
    "In this assignment, we will explore tensor operations that are essential for building neural networks, including tensor creation, reshaping, advanced indexing, broadcasting, and linear algebra operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let's import PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "A PyTorch tensor is a grid of values, all of the same type, indexed by a tuple of non-negative integers. The number of dimensions is called the *rank* of the tensor; the *shape* is a tuple of integers giving the size along each dimension."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We can initialize tensors from nested Python lists and access elements using square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])  # Create a rank 1 tensor\n",
    "print(type(a), a.shape, a[0], a[1], a[2])\n",
    "a[0] = 5  # Change an element of the tensor\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Create a rank 2 tensor\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.shape)\n",
    "print(b[0, 0], b[0, 1], b[1, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "PyTorch provides many functions to create tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((2, 2))  # Create a tensor of all zeros\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones((1, 2))  # Create a tensor of all ones\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.full((2, 2), 7)  # Create a constant tensor\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.eye(2)  # Create a 2x2 identity matrix\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.rand((2, 2))  # Create a tensor filled with random values\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.arange(6)\n",
    "print(f)\n",
    "print(torch.arange(10, 20))\n",
    "print(torch.arange(10, 20, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Note: Detailed documentation for PyTorch: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "You can stack two tensors vertically or horizontally. Be careful of shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.eye(3)\n",
    "B = torch.full((3, 2), 6.0)\n",
    "print(torch.hstack([A, B]))  # horizontally stack\n",
    "print(torch.vstack([A, B.T]))  # vertically stack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Tensor indexing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "PyTorch offers several ways to index into tensors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Slicing: Similar to Python lists, PyTorch tensors can be sliced. Since tensors may be multidimensional, you must specify a slice for each dimension of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the following rank 2 tensor with shape (3, 4)\n",
    "# [[ 1  2  3  4]\n",
    "#  [ 5  6  7  8]\n",
    "#  [ 9 10 11 12]]\n",
    "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "\n",
    "# Use slicing to pull out the subtensor consisting of the first 2 rows\n",
    "# and columns 1 and 2; b is the following tensor of shape (2, 2):\n",
    "# [[2 3]\n",
    "#  [6 7]]\n",
    "b = a[:2, 1:3]\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "A slice of a tensor is a view into the same data, so modifying it will modify the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0, 1])\n",
    "b[0, 0] = 77  # b[0, 0] is the same piece of data as a[0, 1]\n",
    "print(a[0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "You can also mix integer indexing with slice indexing. However, doing so will yield a tensor of lower rank than the original tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the following rank 2 tensor with shape (3, 4)\n",
    "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Here are two ways of accessing the data in the middle row of the tensor. Mixing integer indexing with slices yields a tensor of lower rank, while using only slices yields a tensor of the same rank as the original tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_r1 = a[1, :]  # Rank 1 view of the second row of a\n",
    "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
    "row_r3 = a[[1], :]  # Rank 2 view of the second row of a\n",
    "print(row_r1, row_r1.shape)\n",
    "print(row_r2, row_r2.shape)\n",
    "print(row_r3, row_r3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make the same distinction when accessing columns of a tensor:\n",
    "col_r1 = a[:, 1]\n",
    "col_r2 = a[:, 1:2]\n",
    "print(col_r1, col_r1.shape)\n",
    "print()\n",
    "print(col_r2, col_r2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Integer tensor indexing: When you index into tensors using slicing, the resulting tensor view will always be a subtensor of the original tensor. In contrast, integer tensor indexing allows you to construct arbitrary tensors using the data from another tensor. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# An example of integer tensor indexing.\n",
    "# The returned tensor will have shape (3,) and\n",
    "print(a[[0, 1, 2], [0, 1, 0]])\n",
    "\n",
    "# The above example of integer tensor indexing is equivalent to this:\n",
    "print(torch.tensor([a[0, 0], a[1, 1], a[2, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using integer tensor indexing, you can reuse the same\n",
    "# element from the source tensor:\n",
    "print(a[[0, 0], [1, 1]])\n",
    "\n",
    "# Equivalent to the previous integer tensor indexing example\n",
    "print(torch.tensor([a[0, 1], a[0, 1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "One useful trick with integer tensor indexing is selecting or mutating one element from each row of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tensor from which we will select elements\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of indices\n",
    "b = torch.tensor([0, 2, 0, 1])\n",
    "\n",
    "# Select one element from each row of a using the indices in b\n",
    "print(a[torch.arange(4), b])  # Prints \"tensor([ 1,  6,  7, 11])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutate one element from each row of a using the indices in b\n",
    "a[torch.arange(4), b] += 10\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Boolean tensor indexing: Boolean tensor indexing lets you pick out arbitrary elements of a tensor. Frequently this type of indexing is used to select the elements of a tensor that satisfy some condition. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "bool_idx = a > 2  # Find the elements of a that are bigger than 2;\n",
    "# this returns a tensor of Booleans of the same\n",
    "# shape as a, where each slot of bool_idx tells\n",
    "# whether that element of a is > 2.\n",
    "\n",
    "print(bool_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use boolean tensor indexing to construct a rank 1 tensor\n",
    "# consisting of the elements of a corresponding to the True values\n",
    "# of bool_idx\n",
    "print(a[bool_idx])\n",
    "\n",
    "# We can do all of the above in a single concise statement:\n",
    "print(a[a > 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "For brevity we have left out a lot of details about tensor indexing; if you want to know more you should read the [documentation](https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### Problem 1: Tensor Creation and Slicing\n",
    "\n",
    "(a) Create a tensor `A` containing the integers 1 through 50 (inclusive), i.e., `[1, 2, 3, ..., 50]`.\n",
    "\n",
    "(b) Slice the tensor `A` to return every 6th element, starting from index 1 (the second element, which is 2). The result `b` should be `[2, 8, 14, 20, 26, 32, 38, 44, 50]`.\n",
    "\n",
    "**Hint:** Use `torch.arange` with appropriate start and stop values. For slicing, recall that `tensor[start:stop:step]` selects elements from `start` to `stop-1` with the given step size.\n",
    "\n",
    "See the [PyTorch documentation on torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "A = torch.arange(1, 51)\n",
    "b = A[1::6]\n",
    "# END SOLUTION\n",
    "\n",
    "print(A)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(A) == 50, \"A should have 50 elements\"\n",
    "assert A[0] == 1, \"A should start with 1\"\n",
    "assert A[-1] == 50, \"A should end with 50\"\n",
    "expected_b = [2, 8, 14, 20, 26, 32, 38, 44, 50]\n",
    "assert b.tolist() == expected_b, \"b should be every 6th element starting from index 1\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert A[24] == 25, \"A[24] should be 25 (middle element)\"\n",
    "assert len(b) == 9, \"b should have 9 elements\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Every tensor is a grid of elements of the **same type**. PyTorch provides a large set of numeric datatypes that you can use to construct tensors. PyTorch tries to guess a datatype when you create a tensor, but functions that construct tensors usually also include an optional argument to explicitly specify the datatype. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2])  # Let PyTorch choose the datatype\n",
    "y = torch.tensor([1.0, 2.0])  # Let PyTorch choose the datatype\n",
    "z = torch.tensor([1, 2], dtype=torch.int64)  # Force a particular datatype\n",
    "\n",
    "print(x.dtype, y.dtype, z.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "You can read all about PyTorch datatypes in the [documentation](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Tensor reshaping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Every tensor has a shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(a.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "The element at position $n$ in the shape tuple tells us the number of elements in the $n$-th dimension. In the above example, there are 3 elements in the first dimension (there are 3 rows) and there are 2 elements in the second dimension (there are 2 elements in each row). We can flatten a tensor or reshape it to convert it into a rank 1 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape(6)\n",
    "print(b, b.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "You can reshape a rank 1 tensor into a 2D or 3D tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(c.reshape(4, 2))\n",
    "print(c.reshape(2, 2, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "We can also reshape this into a rank 2 tensor of a different shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape(2, 3)\n",
    "print(b, b.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "In all these examples, we specified an exact number for each dimension, but for reshaping you can have one unknown dimension for which we pass `-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "d = c.reshape(2, 2, -1)\n",
    "print(d)\n",
    "print(d.reshape(-1))  # Flattening tensors using -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### Problem 2: Reshape and Fancy Indexing\n",
    "\n",
    "(a) Create the tensor\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "1 & 2 & 3 & 4\\\\\n",
    "5 & 6 & 7 & 8\\\\\n",
    "9 & 10 & 11 & 12\\\\\n",
    "\\end{pmatrix}\n",
    "$$ using `torch.arange` and the `reshape` method.\n",
    "\n",
    "(b) Extract a subtensor of $A$ that looks like this:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 4\\\\\n",
    "10 & 12\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "using tensor indexing. Notice you need rows 0 and 2, and columns 1 and 3 (0-indexed).\n",
    "\n",
    "**Hint:** You can use advanced indexing with index tensors: `A[row_indices][:, col_indices]` or `A[row_indices.unsqueeze(1), col_indices]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "A = torch.arange(1, 13).reshape(3, 4)\n",
    "row_indices = torch.tensor([0, 2])\n",
    "col_indices = torch.tensor([1, 3])\n",
    "b = A[row_indices.unsqueeze(1), col_indices]\n",
    "# END SOLUTION\n",
    "\n",
    "print(A)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert A.shape == (3, 4), \"A should have shape (3, 4)\"\n",
    "assert torch.equal(\n",
    "    A, torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "), \"A should be [[1,2,3,4], [5,6,7,8], [9,10,11,12]]\"\n",
    "expected_b = torch.tensor([[2, 4], [10, 12]])\n",
    "assert torch.equal(b, expected_b), \"b should be [[2, 4], [10, 12]]\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert A[1, 2] == 7, \"A[1, 2] should be 7\"\n",
    "assert b.shape == (2, 2), \"b should have shape (2, 2)\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### Tensor math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Basic mathematical functions operate elementwise on tensors, and are available both as operator overloads and as functions in the torch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
    "y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float64)\n",
    "\n",
    "# Elementwise sum; both produce the tensor\n",
    "print(x + y)\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise difference; both produce the tensor\n",
    "print(x - y)\n",
    "print(torch.sub(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise product; both produce the tensor\n",
    "print(x * y)\n",
    "print(torch.mul(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise division; both produce the tensor\n",
    "# [[ 0.2         0.33333333]\n",
    "#  [ 0.42857143  0.5       ]]\n",
    "print(x / y)\n",
    "print(torch.div(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise square root; produces the tensor\n",
    "# [[ 1.          1.41421356]\n",
    "#  [ 1.73205081  2.        ]]\n",
    "print(torch.sqrt(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "Note that `*` is elementwise multiplication, not matrix multiplication. We instead use `torch.matmul` or the `@` operator to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "v = torch.tensor([9, 10])\n",
    "w = torch.tensor([11, 12])\n",
    "\n",
    "# Inner product of vectors; both produce 219\n",
    "print(torch.dot(v, w))\n",
    "print(v @ w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "You can also use the `@` operator which is equivalent to `torch.matmul`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix / vector product; both produce the rank 1 tensor [29 67]\n",
    "print(torch.matmul(x, v))\n",
    "print(x @ v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix / matrix product; both produce the rank 2 tensor\n",
    "# [[19 22]\n",
    "#  [43 50]]\n",
    "print(torch.matmul(x, y))\n",
    "print(x @ y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "PyTorch provides many useful functions for performing computations on tensors; some of the most useful are `sum`, `mean`, `prod`, `max`, `min`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(x)\n",
    "print(torch.sum(x))  # Compute sum of all elements; prints \"tensor(10)\"\n",
    "print(torch.sum(x, dim=0))  # Compute sum of each column; prints \"tensor([4, 6])\"\n",
    "print(torch.sum(x, dim=1))  # Compute sum of each row; prints \"tensor([3, 7])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.mean(x.float()))  # mean of all elements (requires float)\n",
    "print(torch.prod(x, dim=0))  # product of each column\n",
    "print(torch.max(x, dim=1).values)  # row-wise maximum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "You can find the full list of mathematical functions provided by PyTorch in the [documentation](https://pytorch.org/docs/stable/torch.html#math-operations).\n",
    "\n",
    "Apart from computing mathematical functions using tensors, we frequently need to reshape or otherwise manipulate data in tensors. The simplest example of this type of operation is transposing a matrix; to transpose a matrix, simply use the T attribute of a tensor object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(\"transpose\\n\", x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([[1, 2, 3]])\n",
    "print(v)\n",
    "print(\"transpose\\n\", v.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Problem 3: Matrix Arithmetic\n",
    "\n",
    "Compute $A^2 - 2A + 3I$ using **matrix multiplication** (not element-wise), where $I$ is the 2x2 identity matrix:\n",
    "\n",
    "$$A =\n",
    "\begin{pmatrix}\n",
    "1 & -1\\\n",
    "2 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Hint:** Use the `@` operator for matrix multiplication and `torch.eye(2)` for the identity matrix.\n",
    "\n",
    "**Note:** By the Cayley-Hamilton theorem, every square matrix satisfies its own characteristic equation. For this matrix $A$, the characteristic polynomial is $\\lambda^2 - 2\\lambda + 3$, so $A^2 - 2A + 3I = 0$ (the zero matrix).\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "tensor([[0., 0.],\n",
    "        [0., 0.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "A = torch.tensor([[1, -1], [2, 1]], dtype=torch.float32)\n",
    "identity = torch.eye(2)\n",
    "result = A @ A - 2 * A + 3 * identity\n",
    "# END SOLUTION\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Test cases\n",
    "# Note: By the Cayley-Hamilton theorem, A satisfies its characteristic equation.\n",
    "# For A = [[1, -1], [2, 1]], the characteristic polynomial is lambda^2 - 2*lambda + 3\n",
    "# Therefore A^2 - 2A + 3I = 0 (the zero matrix)\n",
    "expected = torch.tensor([[0, 0], [0, 0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert torch.allclose(result, expected), f\"Expected {expected}, got {result}\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert result.shape == (2, 2), \"result should have shape (2, 2)\"\n",
    "assert torch.allclose(result.sum(), torch.tensor(0.0)), \"sum of result should be 0\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "Broadcasting is a powerful mechanism that allows PyTorch to work with tensors of different shapes when performing arithmetic operations. Frequently we have a smaller tensor and a larger tensor, and we want to use the smaller tensor multiple times to perform some operation on the larger tensor.\n",
    "\n",
    "For example, suppose that we want to add a constant vector to each row of a matrix. We could do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add the vector v to each row of the matrix x,\n",
    "# storing the result in the matrix y\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "v = torch.tensor([1, 0, 1])\n",
    "y = torch.empty_like(x)  # Create an empty matrix with the same shape as x\n",
    "\n",
    "# Add the vector v to each row of the matrix x with an explicit loop\n",
    "for i in range(4):\n",
    "    y[i, :] = x[i, :] + v\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "This works; however when the matrix `x` is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the matrix `x` is equivalent to forming a matrix `vv` by stacking multiple copies of `v` vertically, then performing elementwise summation of `x` and `vv`. We could implement this approach like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = v.repeat(4, 1)  # Stack 4 copies of v on top of each other\n",
    "print(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + vv  # Add x and vv elementwise\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "PyTorch broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add the vector v to each row of the matrix x,\n",
    "# storing the result in the matrix y\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "v = torch.tensor([1, 0, 1])\n",
    "y = x + v  # Add v to each row of x using broadcasting\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "The line `y = x + v` works even though `x` has shape `(4, 3)` and `v` has shape `(3,)` due to broadcasting; this line works as if v actually had shape `(4, 3)`, where each row was a copy of `v`, and the sum was performed elementwise.\n",
    "\n",
    "Broadcasting two tensors together follows these rules:\n",
    "\n",
    "1. If the tensors do not have the same rank, prepend the shape of the lower rank tensor with 1s until both shapes have the same length.\n",
    "2. The two tensors are said to be compatible in a dimension if they have the same size in the dimension, or if one of the tensors has size 1 in that dimension.\n",
    "3. The tensors can be broadcast together if they are compatible in all dimensions.\n",
    "4. After broadcasting, each tensor behaves as if it had shape equal to the elementwise maximum of shapes of the two input tensors.\n",
    "5. In any dimension where one tensor had size 1 and the other tensor had size greater than 1, the first tensor behaves as if it were copied along that dimension\n",
    "\n",
    "If this explanation does not make sense, try reading the [PyTorch broadcasting documentation](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
    "\n",
    "Here are some applications of broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outer product of vectors\n",
    "v = torch.tensor([1, 2, 3])  # v has shape (3,)\n",
    "w = torch.tensor([4, 5])  # w has shape (2,)\n",
    "# To compute an outer product, we first reshape v to be a column\n",
    "# vector of shape (3, 1); we can then broadcast it against w to yield\n",
    "# an output of shape (3, 2), which is the outer product of v and w:\n",
    "\n",
    "print(v.reshape(3, 1) * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a vector to each row of a matrix\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n",
    "# giving the following matrix:\n",
    "\n",
    "print(x + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another solution is to reshape w to be a row vector of shape (2, 1);\n",
    "# we can then broadcast it directly against x to produce the same\n",
    "# output.\n",
    "print(x + w.reshape(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply a matrix by a constant:\n",
    "# x has shape (2, 3). PyTorch treats scalars as tensors of shape ();\n",
    "# these can be broadcast together to shape (2, 3), producing the\n",
    "# following tensor:\n",
    "print(x * 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Broadcasting typically makes your code more concise and faster, so you should strive to use it where possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "This brief overview has touched on many of the important things that you need to know about PyTorch, but is far from complete. Check out the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) to find out much more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "**Example: Pairwise Euclidean Distance**\n",
    "\n",
    "Here is a practical problem that can be solved elegantly using broadcasting. Let $X$ be an $(n,d)$ tensor and $Y$ be an $(m,d)$ tensor. Think of $X$ as a collection of $n$ $d$-dimensional observations $x_1,\\dots,x_n$ (where rows of $X$ are the observations), and similarly for $Y$ as a collection of $m$ observations. The task is to compute the matrix $D$ of pairwise distances:\n",
    "\n",
    "$$D_{ij} = \\Vert x_i - y_j\\Vert_2:= \\sqrt{\\sum_{\\ell=1}^d (x_{i\\ell} - y_{j\\ell})^2}.$$\n",
    "\n",
    "How can we compute this using broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "m = 2\n",
    "d = 5\n",
    "X = torch.rand(n, d)\n",
    "Y = torch.rand(m, d)\n",
    "\n",
    "A = (\n",
    "    X[:, None, :] - Y[None, :, :]\n",
    ")  # first one is (n,1,d) and second is (1,m,d) -> hence can be broadcast to (n,m,d)\n",
    "A2 = torch.square(A)  # squaring\n",
    "A2_sum = torch.sum(A2, dim=2)  # summed over last dim, hence now (n,m)\n",
    "D = torch.sqrt(A2_sum)  # square root\n",
    "D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "Pay attention to the use of `None` to reshape the tensors so that they can be broadcast together. We can do the whole computation in a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.sqrt(torch.sum((X[:, None, :] - Y[None, :, :]) ** 2, dim=2))  # one-line with broadcasting\n",
    "\n",
    "# using list comprehension\n",
    "D_comp = torch.tensor(\n",
    "    [[torch.linalg.norm(X[i] - Y[j]) for j in range(Y.shape[0])] for i in range(X.shape[0])]\n",
    ")\n",
    "\n",
    "# checking the (2,1)th element\n",
    "torch.allclose(D_comp, D)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "#### Problem 4: Tensor Slicing Assignment\n",
    "\n",
    "Replace the first row of `A = torch.tensor([[1, 2], [3, 4]])` with `[4, 5]` using slicing.\n",
    "\n",
    "**Hint:** Use `A[row_index, :]` to select an entire row, then assign a new tensor to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "A[0, :] = torch.tensor([4, 5])\n",
    "# END SOLUTION\n",
    "\n",
    "print(A)  # Should print tensor([[4, 5], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert A[0, 0] == 4, f\"A[0, 0] should be 4, got {A[0, 0]}\"\n",
    "assert A[0, 1] == 5, f\"A[0, 1] should be 5, got {A[0, 1]}\"\n",
    "assert A[1, 0] == 3, f\"A[1, 0] should remain 3, got {A[1, 0]}\"\n",
    "assert A[1, 1] == 4, f\"A[1, 1] should remain 4, got {A[1, 1]}\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert A.shape == (2, 2), \"A should still have shape (2, 2)\"\n",
    "assert A.sum() == 16, \"sum of A should be 16 (4+5+3+4)\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All assertions passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "#### Problem 5: Broadcasting with 2D and 3D Tensors\n",
    "\n",
    "Add a 2D tensor to a 3D tensor using different broadcasting methods.\n",
    "\n",
    "**Part 1:** Add `arr_2d` with shape `(4, 5)` to `arr_3d` with shape `(3, 4, 5)` using three methods:\n",
    "1. Automatic broadcasting\n",
    "2. Using `reshape`\n",
    "3. Using `None` indexing or `unsqueeze`\n",
    "\n",
    "**Part 2:** Add `arr_2d` with shape `(3, 5)` to `arr_3d` with shape `(3, 4, 5)` using two methods (automatic broadcasting will not work here):\n",
    "1. Using `reshape`\n",
    "2. Using `None` indexing or `unsqueeze`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_3d = torch.ones((3, 4, 5))\n",
    "arr_2d = torch.ones((4, 5))\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Part 1: Add arr_2d (4, 5) to arr_3d (3, 4, 5)\n",
    "# Method 1a: Automatic broadcasting (arr_2d broadcasts to (3, 4, 5))\n",
    "solution_1a = arr_3d + arr_2d\n",
    "\n",
    "# Method 1b: Using reshape\n",
    "solution_1b = arr_3d + arr_2d.reshape(1, 4, 5)\n",
    "\n",
    "# Method 1c: Using None indexing\n",
    "solution_1c = arr_3d + arr_2d[None, :, :]\n",
    "\n",
    "# Part 2: Add arr_2d (3, 5) to arr_3d (3, 4, 5)\n",
    "arr_3d_p2 = torch.ones((3, 4, 5))\n",
    "arr_2d_p2 = torch.ones((3, 5))\n",
    "\n",
    "# Method 2a: Using reshape to (3, 1, 5)\n",
    "solution_2a = arr_3d_p2 + arr_2d_p2.reshape(3, 1, 5)\n",
    "\n",
    "# Method 2b: Using None indexing\n",
    "solution_2b = arr_3d_p2 + arr_2d_p2[:, None, :]\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test Part 1\n",
    "assert solution_1a.shape == (3, 4, 5), f\"1a: got {solution_1a.shape}\"\n",
    "assert solution_1b.shape == (3, 4, 5), f\"1b: got {solution_1b.shape}\"\n",
    "assert solution_1c.shape == (3, 4, 5), f\"1c: got {solution_1c.shape}\"\n",
    "assert torch.all(solution_1a == 2), \"1a: all values should be 2\"\n",
    "assert torch.all(solution_1b == 2), \"1b: all values should be 2\"\n",
    "assert torch.all(solution_1c == 2), \"1c: all values should be 2\"\n",
    "\n",
    "# Test Part 2\n",
    "assert solution_2a.shape == (3, 4, 5), f\"2a: got {solution_2a.shape}\"\n",
    "assert solution_2b.shape == (3, 4, 5), f\"2b: got {solution_2b.shape}\"\n",
    "assert torch.all(solution_2a == 2), \"2a: all values should be 2\"\n",
    "assert torch.all(solution_2b == 2), \"2b: all values should be 2\"\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert torch.equal(solution_1a, solution_1b), \"1a and 1b should produce identical results\"\n",
    "assert torch.equal(solution_1b, solution_1c), \"1b and 1c should produce identical results\"\n",
    "assert torch.equal(solution_2a, solution_2b), \"2a and 2b should produce identical results\"\n",
    "assert solution_2a.numel() == 60, \"solution_2a should have 60 elements (3*4*5)\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All assertions passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "#### Problem 6: Broadcasting\n",
    "\n",
    "(a) Create a tensor $A$ of shape $(10, 8, 3)$ with elements\n",
    "$$A_{ijk} = A[i,j,k] = \\begin{cases}\n",
    "  i+j, &\\text{if } k=0 \\\\\n",
    "  i+j/2, &\\text{if } k=1 \\\\\n",
    "  i/2+j, &\\text{if } k=2\n",
    "\\end{cases}.$$\n",
    "Divide $A$ by its maximum value to ensure that each entry is between 0 and 1. Call this normalized tensor $B$.\n",
    "\n",
    "**Hint:** Use `torch.arange` to create index tensors and broadcasting to build $A$ without explicit loops. Consider using `None` (or `unsqueeze`) to add dimensions.\n",
    "\n",
    "(b) Create a rank-1 tensor $b = [0.95, 0.9, 0.8]$.\n",
    "\n",
    "(c) Build a new tensor $C$ such that $C_{ijk} = B_{jik} \\times b_k$. \n",
    "\n",
    "**Hint:** First permute $B$ to get shape $(8, 10, 3)$, then multiply by $b$ using broadcasting. The shape of $C$ should be $(8, 10, 3)$.\n",
    "\n",
    "(d) Compute the rank-1 tensor $M$ such that $M_i = \\max_{j,k} C_{ijk}$.\n",
    "\n",
    "**Hint:** Use `.max()` or `torch.amax` with the `dim` parameter to take the maximum over multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# (a) Create tensor A of shape (10, 8, 3)\n",
    "i = torch.arange(10).float()  # shape (10,)\n",
    "j = torch.arange(8).float()  # shape (8,)\n",
    "\n",
    "# A[i,j,0] = i + j\n",
    "# A[i,j,1] = i + j/2\n",
    "# A[i,j,2] = i/2 + j\n",
    "\n",
    "# Create base terms with broadcasting\n",
    "# i[:, None, None] has shape (10, 1, 1)\n",
    "# j[None, :, None] has shape (1, 8, 1)\n",
    "i_expanded = i[:, None, None]  # (10, 1, 1)\n",
    "j_expanded = j[None, :, None]  # (1, 8, 1)\n",
    "\n",
    "# Stack the three channels\n",
    "A_k0 = i_expanded + j_expanded  # i + j\n",
    "A_k1 = i_expanded + j_expanded / 2  # i + j/2\n",
    "A_k2 = i_expanded / 2 + j_expanded  # i/2 + j\n",
    "\n",
    "A = torch.cat([A_k0, A_k1, A_k2], dim=2)  # shape (10, 8, 3)\n",
    "\n",
    "# Normalize A by its maximum value\n",
    "B = A / A.max()\n",
    "\n",
    "# (b) Create rank-1 tensor b\n",
    "b = torch.tensor([0.95, 0.9, 0.8])\n",
    "\n",
    "# (c) Build C such that C[i,j,k] = B[j,i,k] * b[k]\n",
    "# First permute B to shape (8, 10, 3)\n",
    "B_permuted = B.permute(1, 0, 2)\n",
    "C = B_permuted * b  # broadcasting works since b has shape (3,)\n",
    "\n",
    "# (d) Compute M where M[i] = max over j,k of C[i,j,k]\n",
    "M = C.amax(dim=(1, 2))  # shape (8,)\n",
    "# END SOLUTION\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"C shape: {C.shape}\")\n",
    "print(f\"M: {M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert A.shape == (10, 8, 3), \"A should have shape (10, 8, 3)\"\n",
    "assert B.shape == (10, 8, 3), \"B should have shape (10, 8, 3)\"\n",
    "assert torch.allclose(B.max(), torch.tensor(1.0)), \"B should be scaled so max is 1\"\n",
    "assert torch.all(B >= 0), \"B values should be >= 0\"\n",
    "assert torch.all(B <= 1), \"B values should be <= 1\"\n",
    "assert b.shape == (3,), \"b should be 1-rank with 3 elements\"\n",
    "assert C.shape == (8, 10, 3), \"C should have shape (8, 10, 3)\"\n",
    "assert M.shape == (8,), \"M should be 1-rank with 8 elements\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert A.numel() == 240, \"A should have 240 elements (10*8*3)\"\n",
    "assert torch.all(M >= 0) and torch.all(M <= 1), \"M values should be between 0 and 1\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "**Linear Algebra Operations using PyTorch**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "You can use the `linalg` module in PyTorch for various linear algebra operations like computing norm of a vector (or matrix), solutions to systems of linear equations, inverse of a matrix, computing the determinant or eigenvalues, etc.\n",
    "\n",
    "See documentation https://pytorch.org/docs/stable/linalg.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "Let us illustrate some of these with:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "5 & 6 & 2\\\\\n",
    "4 & 7 & 19\\\\\n",
    "0 & 3 & 12\n",
    "\\end{pmatrix}, \\quad\n",
    "b = \\begin{pmatrix}\n",
    "-1\\\\\n",
    "2\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[5, 6, 2], [4, 7, 19], [0, 3, 12]], dtype=torch.float32)\n",
    "b = torch.tensor([3 - 1, 2, 1], dtype=torch.float32)\n",
    "print(torch.linalg.norm(b))  # computing norm of a vector\n",
    "print(torch.linalg.det(A))  # compute determinant\n",
    "print(torch.linalg.inv(A))  # inverse of a matrix\n",
    "print(torch.linalg.solve(A, b))  # solves the system Ax=b for x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "#### Problem 7: Common Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks. Implement functions to compute the following commonly used activation functions:\n",
    "\n",
    "(a) **Sigmoid** activation, defined as\n",
    "$$\\text{Sigmoid}(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "(b) **ReLU** (Rectified Linear Unit) activation, defined as\n",
    "$$\\text{ReLU}(x) = \\max \\{0, x\\}$$\n",
    "\n",
    "(c) **Leaky ReLU**, which takes an additional parameter $c$ (a positive real number), defined as\n",
    "$$\\text{LeakyReLU}(x;c) = \\max\\{x, cx\\}$$\n",
    "\n",
    "Each function must work element-wise on a tensor of arbitrary shape.\n",
    "\n",
    "**Hint:** Use `torch.exp` for the exponential. For element-wise maximum, use `torch.maximum` (not `torch.max`, which returns the maximum value across a dimension).\n",
    "\n",
    "**Expected output:\n",
    "```\n",
    "x = tensor([-3, -2, -1,  0,  1,  2])\n",
    "Sigmoid: tensor([0.0474, 0.1192, 0.2689, 0.5000, 0.7311, 0.8808])\n",
    "ReLU: tensor([0, 0, 0, 0, 1, 2])\n",
    "Leaky ReLU: tensor([-0.3000, -0.2000, -0.1000,  0.0000,  1.0000,  2.0000])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # BEGIN SOLUTION\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    # BEGIN SOLUTION\n",
    "    return torch.maximum(torch.zeros_like(x), x)\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def leaky_relu(x, c=0.1):\n",
    "    # BEGIN SOLUTION\n",
    "    return torch.maximum(c * x, x)\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "x = torch.arange(-3, 3).float()\n",
    "print(f\"x = {x}\")\n",
    "print(f\"Sigmoid: {sigmoid(x)}\")\n",
    "print(f\"ReLU: {relu(x)}\")\n",
    "print(f\"Leaky ReLU: {leaky_relu(x, c=0.1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert torch.allclose(sigmoid(torch.tensor([0.0])), torch.tensor([0.5])), \"sigmoid(0) should be 0.5\"\n",
    "assert torch.allclose(sigmoid(torch.tensor([-100.0])), torch.tensor([0.0]), atol=1e-10)\n",
    "assert torch.allclose(sigmoid(torch.tensor([100.0])), torch.tensor([1.0]), atol=1e-10)\n",
    "assert torch.equal(\n",
    "    relu(torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])),\n",
    "    torch.tensor([0.0, 0.0, 0.0, 1.0, 2.0]),\n",
    "), \"relu test failed\"\n",
    "assert torch.allclose(\n",
    "    leaky_relu(torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0]), c=0.1),\n",
    "    torch.tensor([-0.2, -0.1, 0.0, 1.0, 2.0]),\n",
    "), \"leaky_relu test failed\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert torch.allclose(\n",
    "    sigmoid(torch.tensor([2.0])), torch.tensor([0.8808]), atol=1e-3\n",
    "), \"sigmoid(2) should be approximately 0.8808\"\n",
    "assert torch.equal(\n",
    "    relu(torch.tensor([5.0, -3.0])), torch.tensor([5.0, 0.0])\n",
    "), \"relu([5, -3]) should be [5, 0]\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "## Plotting with Matplotlib\n",
    "\n",
    "The most common module needed is `pyplot`. It is usually imported as follows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "Matplotlib is a plotting library. In this section we give a brief introduction to the `matplotlib.pyplot` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "By running this special iPython command, we will be displaying plots inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "The most important function in `matplotlib` is plot, which allows you to plot 2D data. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the x and y coordinates for points on a sine curve\n",
    "import math\n",
    "\n",
    "x = torch.arange(0, 3 * math.pi, 0.1)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Plot the points using matplotlib\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "With just a little bit of extra work we can easily plot multiple lines at once, and add a title, legend, and axis labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sin = torch.sin(x)\n",
    "y_cos = torch.cos(x)\n",
    "\n",
    "# Plot the points using matplotlib\n",
    "plt.plot(x, y_sin)\n",
    "plt.plot(x, y_cos)\n",
    "plt.xlabel(\"x axis label\")\n",
    "plt.ylabel(\"y axis label\")\n",
    "plt.title(\"Sine and Cosine\")\n",
    "plt.legend([\"Sine\", \"Cosine\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "#### Problem 8: Plotting a Quadratic Function\n",
    "\n",
    "Plot the quadratic function $y = x^2 - 2x + 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-2, 4, 0.1)  # SOLUTION\n",
    "y = x**2 - 2 * x + 1  # SOLUTION\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"y = x - 2x + 1\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(y) == len(x), \"y should have the same length as x\"\n",
    "assert y.min() >= 0, \"y = (x-1)^2 should always be non-negative\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(y) == len(x), \"y should have the same length as x\"\n",
    "assert y.min() >= 0, \"y = (x-1)^2 should always be non-negative\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "#### Subplots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "You can plot different things in the same figure using the subplot function. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the x and y coordinates for points on sine and cosine curves\n",
    "x = torch.arange(0, 3 * math.pi, 0.1)\n",
    "y_sin = torch.sin(x)\n",
    "y_cos = torch.cos(x)\n",
    "\n",
    "# Set up a subplot grid that has height 2 and width 1,\n",
    "# and set the first such subplot as active.\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "# Make the first plot\n",
    "plt.plot(x, y_sin)\n",
    "plt.title(\"Sine\")\n",
    "\n",
    "# Set the second subplot as active, and make the second plot.\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x, y_cos)\n",
    "plt.title(\"Cosine\")\n",
    "\n",
    "# Show the figure.\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "You can read much more about the `subplot` function in the [documentation](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "In the previous example, one had to finish working on a subplot before moving onto the next one.\n",
    "You can acquire finer control over the plots using `plt.subplots` to declare an `Axes` object and modify a subplot whenever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two rows, one column\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "x = torch.arange(0, 3 * math.pi, 0.1)\n",
    "y_sin = torch.sin(x)\n",
    "y_cos = torch.cos(x)\n",
    "\n",
    "# draw the curves first\n",
    "ax[0].plot(x, y_sin)\n",
    "ax[1].plot(x, y_cos)\n",
    "\n",
    "# set the title next\n",
    "ax[0].set_title(\"Sine\")\n",
    "ax[1].set_title(\"Cosine\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "As shown in the example, you can access the desired subplot by indexing `ax`. Note the subtle difference: you must use `ax[0].set_*` instead of `plt.*` to modify a specific subplot (e.g., `ax[0].set_title` instead of `plt.title`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "#### Problem 9: Subplot Grid\n",
    "\n",
    "Using `plt.subplots`, create a 22 grid of plots. Draw sine, cosine, $y = x^2$, and $y = x$ in the four subplots. Include a title for each subplot.\n",
    "\n",
    "**Hint:** For a 22 grid, use `ax[row, col]` to access each subplot. For example, `ax[0, 1]` accesses the top-right subplot. Use `ax[row, col].set_title()` to add titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "x = torch.arange(0, 3 * math.pi, 0.1)\n",
    "y_sin = torch.sin(x)\n",
    "y_cos = torch.cos(x)\n",
    "y_square = x**2\n",
    "y_identity = x\n",
    "\n",
    "# Top-left: sine\n",
    "ax[0, 0].plot(x, y_sin)\n",
    "ax[0, 0].set_title(\"Sine\")\n",
    "\n",
    "# Top-right: cosine\n",
    "ax[0, 1].plot(x, y_cos)\n",
    "ax[0, 1].set_title(\"Cosine\")\n",
    "\n",
    "# Bottom-left: x^2\n",
    "ax[1, 0].plot(x, y_square)\n",
    "ax[1, 0].set_title(\"y = x^2\")\n",
    "\n",
    "# Bottom-right: y = x\n",
    "ax[1, 1].plot(x, y_identity)\n",
    "ax[1, 1].set_title(\"y = x\")\n",
    "# END SOLUTION\n",
    "\n",
    "plt.tight_layout()  # Prevents overlapping titles\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert ax.shape == (2, 2), f\"Expected 2x2 grid, got {ax.shape}\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(ax.flatten()) == 4, \"Should have exactly 4 subplots\"\n",
    "assert ax[0, 0].get_title() != \"\", \"Top-left subplot should have a title\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"Subplot grid created successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "#### Problem 10: Plotting Activation Functions\n",
    "\n",
    "Create a $2\\times 2$ grid of plots. In the first row, show the Sigmoid and ReLU activation functions for $x$ between -10 and 10. In the second row, show the tanh (use `torch.tanh`) and Leaky ReLU activation functions over the same range. For Leaky ReLU, use three different values of $c \\in \\{0.01, 0.05, 0.1\\}$ and show all three curves on the same subplot with a legend.\n",
    "\n",
    "Use `ax.grid()` to show gridlines. Include proper subplot titles.\n",
    "\n",
    "**Hint:** The `ax` object returned by `plt.subplots` when both `nrows` and `ncols` are greater than 1 is indexed like a 2D array. For example, to plot on the first (left) subplot of the second row, use `ax[1, 0].plot(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running it should produce the 4 plots\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "x = torch.arange(-10, 10, 0.1)\n",
    "\n",
    "# Top-left: Sigmoid\n",
    "ax[0, 0].plot(x, sigmoid(x))\n",
    "ax[0, 0].set_title(\"Sigmoid\")\n",
    "ax[0, 0].grid()\n",
    "\n",
    "# Top-right: ReLU\n",
    "ax[0, 1].plot(x, relu(x))\n",
    "ax[0, 1].set_title(\"ReLU\")\n",
    "ax[0, 1].grid()\n",
    "\n",
    "# Bottom-left: tanh\n",
    "ax[1, 0].plot(x, torch.tanh(x))\n",
    "ax[1, 0].set_title(\"tanh\")\n",
    "ax[1, 0].grid()\n",
    "\n",
    "# Bottom-right: Leaky ReLU with different c values\n",
    "for c in [0.01, 0.05, 0.1]:\n",
    "    ax[1, 1].plot(x, leaky_relu(x, c=c), label=f\"c={c}\")\n",
    "ax[1, 1].set_title(\"Leaky ReLU\")\n",
    "ax[1, 1].legend()\n",
    "ax[1, 1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert fig is not None, \"Figure should be created\"\n",
    "assert ax.shape == (2, 2), \"Should create a 2x2 grid of subplots\"\n",
    "assert ax[0, 0].get_title() == \"Sigmoid\", \"Top-left should be Sigmoid\"\n",
    "assert ax[0, 1].get_title() == \"ReLU\", \"Top-right should be ReLU\"\n",
    "assert ax[1, 0].get_title() == \"tanh\", \"Bottom-left should be tanh\"\n",
    "assert ax[1, 1].get_title() == \"Leaky ReLU\", \"Bottom-right should be Leaky ReLU\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    len(ax[1, 1].get_legend().get_texts()) == 3\n",
    "), \"Leaky ReLU should have 3 legend entries for different c values\"\n",
    "assert all(\n",
    "    subplot.get_lines() for subplot in ax.flatten()\n",
    "), \"All subplots should have plotted lines\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "#### Problem 11: Kernel Smoothing\n",
    "\n",
    "Let us take a look at some data (see plot below). We have $n$ observations $(x_i,y_i)$ for $i=1,\\dots,n$. Kernel smoothing estimates a function $f$ such that $f(x_i)\\approx y_i$ for each $i$. This is done by computing $f(x)$ for any $x$ as the weighted average of observed $y_i$ values for neighboring $x_i$'s from the dataset:\n",
    "$$f(x) = \\frac{\\sum_{i=1}^n K(x,x_i) y_i}{\\sum_{i=1}^n K(x,x_i)}$$\n",
    "where $K(x,x_i)$ is an appropriate kernel. Note that the above is a weighted average of $y_i$'s with weights $w_i(x) = K(x,x_i)\\big/\\sum_i K(x,x_i)$.\n",
    "\n",
    "Given the dataset, the smoothing is determined by the choice of kernel. In this problem, we will use a particular type of kernel smoothing.\n",
    "\n",
    "**Gaussian kernel smoothing:** For parameter $b$ (called the lengthscale), the kernel takes the form\n",
    "$$K(x,x_i) = \\exp \\left(-\\frac{(x - x_i)^2}{2b^2}\\right).$$\n",
    "\n",
    "(a) Implement Gaussian kernel smoothing as the following function: `x_train` and `y_train` are the data points (as rank-1 tensors) and `x_test` is another rank-1 tensor on which to compute the kernel-smoothed function. The parameter `lengthscale` is $b$. Return a rank-1 tensor of the same length as `x_test`.\n",
    "\n",
    "**Hint:** Use broadcasting! If `x_test` has shape `(m,)` and `x_train` has shape `(n,)`, reshape them to compute all pairwise differences at once.\n",
    "\n",
    "(b) Plot the Gaussian kernel-smoothed function on 100 equally spaced values between 0 and 1, along with a scatter plot of the data points. Do this for lengthscales $\\{0.01, 0.05, 0.1\\}$ in the same plot (use appropriate legends).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(\n",
    "    [\n",
    "        0.09291784,\n",
    "        0.46809093,\n",
    "        0.93089486,\n",
    "        0.67612654,\n",
    "        0.73441752,\n",
    "        0.86847339,\n",
    "        0.49873225,\n",
    "        0.51083168,\n",
    "        0.18343972,\n",
    "        0.99380898,\n",
    "        0.27840809,\n",
    "        0.38028817,\n",
    "        0.12055708,\n",
    "        0.56715537,\n",
    "        0.92005746,\n",
    "        0.77072270,\n",
    "        0.85278176,\n",
    "        0.05315950,\n",
    "        0.87168699,\n",
    "        0.58858043,\n",
    "    ]\n",
    ")\n",
    "y_train = torch.tensor(\n",
    "    [\n",
    "        -0.15934537,\n",
    "        0.18195445,\n",
    "        0.451270150,\n",
    "        0.13921448,\n",
    "        0.09366691,\n",
    "        0.30567674,\n",
    "        0.372291170,\n",
    "        0.40716968,\n",
    "        -0.08131792,\n",
    "        0.41187806,\n",
    "        0.36943738,\n",
    "        0.3994327,\n",
    "        0.019062570,\n",
    "        0.35820410,\n",
    "        0.452564960,\n",
    "        -0.0183121,\n",
    "        0.02957665,\n",
    "        -0.24354444,\n",
    "        0.148038840,\n",
    "        0.26824970,\n",
    "    ]\n",
    ")\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.xlabel(\"Input $x$\")\n",
    "plt.ylabel(\"Output $y$\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_smoother(x_test, x_train, y_train, lengthscale=1.0):\n",
    "    # BEGIN SOLUTION\n",
    "    # Compute pairwise differences using broadcasting\n",
    "    # x_test has shape (m,), x_train has shape (n,)\n",
    "    # We want to compute (x_test[i] - x_train[j]) for all i, j\n",
    "    diff = x_test[:, None] - x_train[None, :]  # shape (m, n)\n",
    "\n",
    "    # Compute Gaussian kernel weights\n",
    "    kernel = torch.exp(-(diff**2) / (2 * lengthscale**2))  # shape (m, n)\n",
    "\n",
    "    # Compute weighted average\n",
    "    numerator = kernel @ y_train  # shape (m,)\n",
    "    denominator = kernel.sum(dim=1)  # shape (m,)\n",
    "\n",
    "    return numerator / denominator\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "# Part (b): Plot the kernel-smoothed function with different lengthscales\n",
    "# BEGIN SOLUTION\n",
    "x_test = torch.linspace(0, 1, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, label=\"Data points\", color=\"black\", zorder=5)\n",
    "\n",
    "for lengthscale in [0.01, 0.05, 0.1]:\n",
    "    y_pred = gaussian_kernel_smoother(x_test, x_train, y_train, lengthscale=lengthscale)\n",
    "    plt.plot(x_test, y_pred, label=f\"lengthscale={lengthscale}\")\n",
    "\n",
    "plt.xlabel(\"Input $x$\")\n",
    "plt.ylabel(\"Output $y$\")\n",
    "plt.title(\"Gaussian Kernel Smoothing\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# END SOLUTION\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_x = torch.tensor([0.5])\n",
    "test_result = gaussian_kernel_smoother(test_x, x_train, y_train, lengthscale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert test_result.shape == (1,), \"Output should have same length as x_test\"\n",
    "assert torch.isfinite(test_result[0]), \"Output should be finite\"\n",
    "# BEGIN HIDDEN TESTS\n",
    "test_x_multi = torch.tensor([0.2, 0.5, 0.8])\n",
    "test_result_multi = gaussian_kernel_smoother(test_x_multi, x_train, y_train, lengthscale=0.1)\n",
    "assert test_result_multi.shape == (3,), \"Output should have 3 elements for 3 test points\"\n",
    "assert torch.all(torch.isfinite(test_result_multi)), \"All outputs should be finite\"\n",
    "# END HIDDEN TESTS\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "#### Problem 12: Loss Function\n",
    "\n",
    "Implement the least squares loss as a function:\n",
    "$$\text{LeastSquares}(Y, \\hat{Y}) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Hint:** Be careful with shapes! One tensor might have shape `(n,)` while another has shape `(n, 1)`. You may need to flatten or reshape tensors to ensure they have compatible shapes.\n",
    "\n",
    "**Example:** For `y_true = [1, 2, 3]` and `y_pred = [1, 2, 3]`, the loss should be 0. For `y_true = [0, 0]` and `y_pred = [1, 1]`, the loss should be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_loss(y_true, y_pred):\n",
    "    # BEGIN SOLUTION\n",
    "    # Flatten both tensors to ensure they have the same shape\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    return torch.sum((y_true - y_pred) ** 2)\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "# Example usage\n",
    "y_true_example = torch.tensor([1.0, 2.0, 3.0])\n",
    "y_pred_example = torch.tensor([1.5, 2.5, 3.5])\n",
    "example_loss = least_square_loss(y_true_example, y_pred_example)\n",
    "print(f\"Example loss: {example_loss:.3f}\")  # Should be 0.75 = 0.25 + 0.25 + 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert torch.isclose(\n",
    "    least_square_loss(torch.tensor([1.0, 2.0, 3.0]), torch.tensor([1.0, 2.0, 3.0])),\n",
    "    torch.tensor(0.0),\n",
    "), \"Loss should be 0 for identical tensors\"\n",
    "assert torch.isclose(\n",
    "    least_square_loss(torch.tensor([0.0, 0.0]), torch.tensor([1.0, 1.0])),\n",
    "    torch.tensor(2.0),\n",
    "), \"Loss should be 2 for [0,0] vs [1,1]\"\n",
    "assert torch.isclose(\n",
    "    example_loss, torch.tensor(0.75), atol=0.001\n",
    "), f\"Example loss should be 0.75, got {example_loss:.3f}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert torch.isclose(\n",
    "    least_square_loss(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 4.0])),\n",
    "    torch.tensor(5.0),\n",
    "), \"Loss should be 5 for [1,2] vs [2,4]: (1-2)^2 + (2-4)^2 = 1 + 4 = 5\"\n",
    "assert (\n",
    "    least_square_loss(torch.tensor([5.0]), torch.tensor([5.0])) == 0.0\n",
    "), \"Loss should be 0 for single identical values\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "cgVersion": 1,
  "kernelspec": {
   "display_name": "datasci-315",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
