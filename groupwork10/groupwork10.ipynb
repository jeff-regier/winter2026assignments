{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 315, Group Work 10: Transformers\n",
    "\n",
    "In this assignment, you will explore the core components of transformer architectures, which power modern language models like GPT and BERT. You will implement attention mechanisms, understand positional encodings, and build a simple transformer-based model.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand scaled dot-product attention\n",
    "- Implement causal (autoregressive) masking for language modeling\n",
    "- Work with positional encodings\n",
    "- Use PyTorch's transformer modules\n",
    "\n",
    "**Resources:**\n",
    "- [Attention Is All You Need (original paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [PyTorch Transformer Documentation](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Background: Scaled Dot-Product Attention\n",
    "\n",
    "The core operation in transformers is **scaled dot-product attention**. Given queries $Q$, keys $K$, and values $V$, attention is computed as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "where $d_k$ is the dimension of the keys (used for scaling to prevent the dot products from growing too large).\n",
    "\n",
    "**Why scale by $\\sqrt{d_k}$?** When the dimension $d_k$ is large, the dot products $QK^T$ can have large magnitudes, pushing the softmax into regions with very small gradients. Scaling by $\\sqrt{d_k}$ keeps the variance of the dot products roughly constant regardless of dimension.\n",
    "\n",
    "**Masking:** For autoregressive (causal) language modeling, we need to prevent the model from \"seeing the future.\" This is done by adding a mask that sets future positions to $-\\infty$ before the softmax, so they contribute zero weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Problem 1: Implement Scaled Dot-Product Attention\n",
    "\n",
    "Implement the `scaled_dot_product_attention` function that computes attention scores and applies them to the values.\n",
    "\n",
    "**Parameters:**\n",
    "- `query`: Tensor of shape `(batch_size, seq_len, embed_dim)`\n",
    "- `key`: Tensor of shape `(batch_size, seq_len, embed_dim)`\n",
    "- `value`: Tensor of shape `(batch_size, seq_len, embed_dim)`\n",
    "- `mask`: Optional boolean tensor of shape `(seq_len, seq_len)` where `True` indicates positions to mask (set to $-\\infty$)\n",
    "\n",
    "**Returns:**\n",
    "- `output`: Tensor of shape `(batch_size, seq_len, embed_dim)` - the attention-weighted values\n",
    "- `attention_weights`: Tensor of shape `(batch_size, seq_len, seq_len)` - the attention weights after softmax\n",
    "\n",
    "**Worked Example:**\n",
    "\n",
    "Consider a simple case with `batch_size=1`, `seq_len=2`, `embed_dim=2`:\n",
    "\n",
    "```\n",
    "Q = [[1, 0],    K = [[1, 0],    V = [[1, 2],\n",
    "     [0, 1]]         [0, 1]]         [3, 4]]\n",
    "```\n",
    "\n",
    "Step 1: Compute $QK^T$:\n",
    "```\n",
    "QK^T = [[1*1 + 0*0, 1*0 + 0*1],   = [[1, 0],\n",
    "        [0*1 + 1*0, 0*0 + 1*1]]      [0, 1]]\n",
    "```\n",
    "\n",
    "Step 2: Scale by $\\sqrt{d_k} = \\sqrt{2} \\approx 1.414$:\n",
    "```\n",
    "Scaled = [[0.707, 0],\n",
    "          [0, 0.707]]\n",
    "```\n",
    "\n",
    "Step 3: Apply softmax row-wise:\n",
    "```\n",
    "Attention weights = [[0.67, 0.33],\n",
    "                     [0.33, 0.67]]\n",
    "```\n",
    "\n",
    "Step 4: Multiply by V:\n",
    "```\n",
    "Output = [[0.67*1 + 0.33*3, 0.67*2 + 0.33*4],\n",
    "          [0.33*1 + 0.67*3, 0.33*2 + 0.67*4]]\n",
    "       = [[1.67, 2.67],\n",
    "          [2.33, 3.33]]\n",
    "```\n",
    "\n",
    "**Hint:** Use `torch.bmm` for batched matrix multiplication and `F.softmax` with `dim=-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Parameters:\n",
    "    - query: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "    - key: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "    - value: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "    - mask: Optional boolean tensor of shape (seq_len, seq_len) where True = mask out\n",
    "\n",
    "    Returns:\n",
    "    - output: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "    - attention_weights: Tensor of shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Get the embedding dimension for scaling\n",
    "    embed_dim = query.size(-1)\n",
    "    scale = math.sqrt(embed_dim)\n",
    "\n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    # query: (batch, seq_len, embed_dim)\n",
    "    # key.transpose(-2, -1): (batch, embed_dim, seq_len)\n",
    "    # scores: (batch, seq_len, seq_len)\n",
    "    scores = torch.bmm(query, key.transpose(-2, -1)) / scale\n",
    "\n",
    "    # Apply mask if provided (set masked positions to -inf before softmax)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = functional.softmax(scores, dim=-1)\n",
    "\n",
    "    # Apply attention weights to values\n",
    "    output = torch.bmm(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Test basic attention computation\n",
    "torch.manual_seed(42)\n",
    "batch_size, seq_len, embed_dim = 2, 4, 8\n",
    "query = torch.randn(batch_size, seq_len, embed_dim)\n",
    "key = torch.randn(batch_size, seq_len, embed_dim)\n",
    "value = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "assert output.shape == (\n",
    "    batch_size,\n",
    "    seq_len,\n",
    "    embed_dim,\n",
    "), f\"Output shape should be {(batch_size, seq_len, embed_dim)}, got {output.shape}\"\n",
    "expected_attn_shape = (batch_size, seq_len, seq_len)\n",
    "assert (\n",
    "    attention_weights.shape == expected_attn_shape\n",
    "), f\"Attention weights shape should be {expected_attn_shape}, got {attention_weights.shape}\"\n",
    "\n",
    "# Attention weights should sum to 1 along the last dimension\n",
    "weight_sums = attention_weights.sum(dim=-1)\n",
    "assert torch.allclose(\n",
    "    weight_sums, torch.ones_like(weight_sums), atol=1e-5\n",
    "), \"Attention weights should sum to 1 along last dimension\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with identity-like inputs\n",
    "query_id = torch.eye(3).unsqueeze(0)  # (1, 3, 3)\n",
    "key_id = torch.eye(3).unsqueeze(0)\n",
    "value_id = torch.arange(9).float().reshape(1, 3, 3)\n",
    "out_id, weights_id = scaled_dot_product_attention(query_id, key_id, value_id)\n",
    "# With identity Q and K, attention should focus more on diagonal\n",
    "assert weights_id[0, 0, 0] > weights_id[0, 0, 1], \"Diagonal attention should be higher\"\n",
    "\n",
    "# Test that masking works\n",
    "mask = torch.ones(4, 4, dtype=torch.bool)\n",
    "mask[0, 0] = False  # Only allow position 0 to attend to position 0\n",
    "_, masked_weights = scaled_dot_product_attention(query, key, value, mask=mask)\n",
    "assert torch.allclose(\n",
    "    masked_weights[:, 0, 0], torch.ones(batch_size), atol=1e-5\n",
    "), \"With full mask except [0,0], attention should be 1.0 at [0,0]\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Problem 2: Create a Causal Mask\n",
    "\n",
    "For autoregressive language models (like GPT), each position should only attend to previous positions (including itself). This requires a **causal mask** (also called a \"look-ahead mask\").\n",
    "\n",
    "Implement `create_causal_mask` that returns a boolean mask where `True` indicates positions that should be masked out (i.e., future positions).\n",
    "\n",
    "**Parameters:**\n",
    "- `seq_len`: The sequence length\n",
    "\n",
    "**Returns:**\n",
    "- A boolean tensor of shape `(seq_len, seq_len)` where `mask[i, j] = True` if position `i` should NOT attend to position `j` (i.e., if `j > i`)\n",
    "\n",
    "**Example for seq_len=4:**\n",
    "```\n",
    "[[False,  True,  True,  True],   # Position 0 can only see position 0\n",
    " [False, False,  True,  True],   # Position 1 can see positions 0, 1\n",
    " [False, False, False,  True],   # Position 2 can see positions 0, 1, 2\n",
    " [False, False, False, False]]   # Position 3 can see all positions\n",
    "```\n",
    "\n",
    "**Hint:** Use `torch.triu` (upper triangular) with `diagonal=1` to create a matrix of ones above the diagonal, then convert to boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask for autoregressive attention.\n",
    "\n",
    "    Parameters:\n",
    "    - seq_len: Length of the sequence\n",
    "\n",
    "    Returns:\n",
    "    - mask: Boolean tensor of shape (seq_len, seq_len) where True = mask out\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Create upper triangular matrix with ones above the diagonal\n",
    "    # This masks out future positions (j > i)\n",
    "    return torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "mask = create_causal_mask(4)\n",
    "\n",
    "assert mask.shape == (4, 4), f\"Mask shape should be (4, 4), got {mask.shape}\"\n",
    "assert mask.dtype == torch.bool, f\"Mask should be boolean, got {mask.dtype}\"\n",
    "\n",
    "# Check specific values\n",
    "expected = torch.tensor(\n",
    "    [\n",
    "        [False, True, True, True],\n",
    "        [False, False, True, True],\n",
    "        [False, False, False, True],\n",
    "        [False, False, False, False],\n",
    "    ]\n",
    ")\n",
    "assert torch.equal(mask, expected), f\"Mask values incorrect.\\nExpected:\\n{expected}\\nGot:\\n{mask}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with different sizes\n",
    "mask_2 = create_causal_mask(2)\n",
    "expected_2 = torch.tensor([[False, True], [False, False]])\n",
    "assert torch.equal(mask_2, expected_2), \"Causal mask for seq_len=2 is incorrect\"\n",
    "\n",
    "mask_1 = create_causal_mask(1)\n",
    "expected_1 = torch.tensor([[False]])\n",
    "assert torch.equal(mask_1, expected_1), \"Causal mask for seq_len=1 is incorrect\"\n",
    "\n",
    "# Verify diagonal is always False (can attend to self)\n",
    "mask_5 = create_causal_mask(5)\n",
    "assert not mask_5.diagonal().any(), \"Diagonal should be all False (can attend to self)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Problem 3: Verify Causal Masking Works\n",
    "\n",
    "Now let's verify that the causal mask properly prevents attending to future positions.\n",
    "\n",
    "Create a test where you:\n",
    "1. Use your `create_causal_mask` function\n",
    "2. Call `scaled_dot_product_attention` with the mask\n",
    "3. Verify that future positions have zero attention weight\n",
    "\n",
    "Set `causal_attention_weights` to the attention weights from applying causal masking to the provided query, key, and value tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: create test tensors\n",
    "torch.manual_seed(123)\n",
    "test_batch_size, test_seq_len, test_embed_dim = 1, 5, 4\n",
    "test_query = torch.randn(test_batch_size, test_seq_len, test_embed_dim)\n",
    "test_key = torch.randn(test_batch_size, test_seq_len, test_embed_dim)\n",
    "test_value = torch.randn(test_batch_size, test_seq_len, test_embed_dim)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Create causal mask and apply attention\n",
    "causal_mask = create_causal_mask(test_seq_len)\n",
    "_, causal_attention_weights = scaled_dot_product_attention(\n",
    "    test_query, test_key, test_value, mask=causal_mask\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert causal_attention_weights.shape == (\n",
    "    test_batch_size,\n",
    "    test_seq_len,\n",
    "    test_seq_len,\n",
    "), \"Attention weights have incorrect shape\"\n",
    "\n",
    "# Check that upper triangle (future positions) has zero attention\n",
    "upper_triangle = torch.triu(causal_attention_weights[0], diagonal=1)\n",
    "assert torch.allclose(\n",
    "    upper_triangle, torch.zeros_like(upper_triangle), atol=1e-6\n",
    "), \"Future positions should have zero attention weight\"\n",
    "\n",
    "# Check rows still sum to 1\n",
    "row_sums = causal_attention_weights.sum(dim=-1)\n",
    "assert torch.allclose(\n",
    "    row_sums, torch.ones_like(row_sums), atol=1e-5\n",
    "), \"Attention weights should still sum to 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# First position should only attend to itself\n",
    "assert torch.allclose(\n",
    "    causal_attention_weights[0, 0, 0], torch.tensor(1.0), atol=1e-5\n",
    "), \"Position 0 should have attention weight 1.0 on itself\"\n",
    "\n",
    "# Last position should have non-zero weights for all positions\n",
    "last_row = causal_attention_weights[0, -1, :]\n",
    "assert (last_row > 0).all(), \"Last position should attend to all positions\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Background: Positional Encoding\n",
    "\n",
    "Unlike RNNs, transformers process all positions in parallel and have no inherent notion of order. **Positional encodings** are added to the input embeddings to give the model information about the position of each token.\n",
    "\n",
    "The original transformer paper uses sinusoidal positional encodings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension index. This creates a unique encoding for each position that the model can learn to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Problem 4: Implement Sinusoidal Positional Encoding\n",
    "\n",
    "Implement `create_positional_encoding` that generates sinusoidal positional encodings.\n",
    "\n",
    "**Parameters:**\n",
    "- `max_seq_len`: Maximum sequence length\n",
    "- `embed_dim`: Embedding dimension (must be even)\n",
    "\n",
    "**Returns:**\n",
    "- Tensor of shape `(max_seq_len, embed_dim)` containing the positional encodings\n",
    "\n",
    "**Algorithm:**\n",
    "1. Create position indices: `pos = [0, 1, 2, ..., max_seq_len-1]`\n",
    "2. Create dimension indices: `i = [0, 1, 2, ..., embed_dim/2-1]`\n",
    "3. Compute the divisor term: `div_term = 10000^(2i/embed_dim)`\n",
    "4. Apply sine to even indices: `PE[:, 0::2] = sin(pos / div_term)`\n",
    "5. Apply cosine to odd indices: `PE[:, 1::2] = cos(pos / div_term)`\n",
    "\n",
    "**Hint:** Use `torch.arange` and broadcasting. The divisor can be computed as `torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_encoding(max_seq_len, embed_dim):\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encodings.\n",
    "\n",
    "    Parameters:\n",
    "    - max_seq_len: Maximum sequence length\n",
    "    - embed_dim: Embedding dimension (must be even)\n",
    "\n",
    "    Returns:\n",
    "    - pe: Tensor of shape (max_seq_len, embed_dim)\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    # Create position indices (max_seq_len, 1)\n",
    "    position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "\n",
    "    # Compute the divisor term for each dimension pair\n",
    "    # div_term[i] = 1 / 10000^(2i/embed_dim) = exp(-2i * log(10000) / embed_dim)\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "\n",
    "    # Initialize positional encoding matrix\n",
    "    pe = torch.zeros(max_seq_len, embed_dim)\n",
    "\n",
    "    # Apply sine to even indices\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "    # Apply cosine to odd indices\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pe\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "pe = create_positional_encoding(100, 64)\n",
    "\n",
    "assert pe.shape == (100, 64), f\"Shape should be (100, 64), got {pe.shape}\"\n",
    "\n",
    "# Position 0 should have sin(0)=0 for even dims and cos(0)=1 for odd dims\n",
    "assert torch.allclose(\n",
    "    pe[0, 0::2], torch.zeros(32), atol=1e-5\n",
    "), \"Position 0, even dims should be sin(0)=0\"\n",
    "assert torch.allclose(\n",
    "    pe[0, 1::2], torch.ones(32), atol=1e-5\n",
    "), \"Position 0, odd dims should be cos(0)=1\"\n",
    "\n",
    "# Values should be bounded between -1 and 1\n",
    "assert pe.min() >= -1.0 and pe.max() <= 1.0, \"Positional encodings should be in [-1, 1]\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with smaller dimensions\n",
    "pe_small = create_positional_encoding(10, 4)\n",
    "assert pe_small.shape == (10, 4), \"Shape mismatch for small PE\"\n",
    "\n",
    "# Check that different positions have different encodings\n",
    "assert not torch.allclose(\n",
    "    pe[0], pe[1], atol=1e-3\n",
    "), \"Different positions should have different encodings\"\n",
    "\n",
    "# Verify the pattern: low-frequency dims (higher indices) change more slowly\n",
    "# Compare rate of change between position 0->1 for dim 0 vs dim 62\n",
    "diff_low_dim = abs(pe[1, 0] - pe[0, 0])\n",
    "diff_high_dim = abs(pe[1, 62] - pe[0, 62])\n",
    "assert diff_low_dim > diff_high_dim, \"Lower dimension indices should change faster\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Background: Complete Transformer Model\n",
    "\n",
    "Now let's put it all together. A transformer-based language model typically consists of:\n",
    "\n",
    "1. **Token Embedding**: Converts token indices to dense vectors\n",
    "2. **Positional Encoding**: Adds position information\n",
    "3. **Transformer Layers**: Self-attention + feedforward networks\n",
    "4. **Output Projection**: Maps back to vocabulary size\n",
    "\n",
    "PyTorch provides `nn.TransformerEncoder` which handles the transformer layers. We just need to handle the embeddings and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Problem 5: Complete the Transformer Language Model\n",
    "\n",
    "Complete the `TransformerLM` class by implementing the `forward` method. The model should:\n",
    "\n",
    "1. Convert input token indices to embeddings\n",
    "2. Add positional encodings (already stored in `self.positional_encoding`)\n",
    "3. Create a causal mask using your `create_causal_mask` function\n",
    "4. Pass through the transformer encoder with the causal mask\n",
    "5. Project to vocabulary size and return log probabilities\n",
    "\n",
    "**Parameters:**\n",
    "- `x`: Input tensor of shape `(batch_size, seq_len)` containing token indices\n",
    "\n",
    "**Returns:**\n",
    "- Log probabilities of shape `(batch_size, seq_len, vocab_size)`\n",
    "\n",
    "**Important:** PyTorch's `TransformerEncoder` uses a different mask convention. It expects a float mask where masked positions are `-inf` and unmasked positions are `0`. The mask shape should be `(seq_len, seq_len)`. Use `torch.where` to convert your boolean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_len):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer Language Model.\n",
    "\n",
    "        Parameters:\n",
    "        - vocab_size: Number of tokens in vocabulary\n",
    "        - embed_dim: Embedding dimension\n",
    "        - num_heads: Number of attention heads\n",
    "        - hidden_dim: Hidden dimension in feedforward layers\n",
    "        - num_layers: Number of transformer layers\n",
    "        - max_seq_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Store positional encoding as a buffer (not a parameter)\n",
    "        pe = create_positional_encoding(max_seq_len, embed_dim)\n",
    "        self.register_buffer(\"positional_encoding\", pe)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, seq_len) containing token indices\n",
    "\n",
    "        Returns:\n",
    "        - Log probabilities of shape (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # BEGIN SOLUTION\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Step 1: Get token embeddings\n",
    "        token_embeddings = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "\n",
    "        # Step 2: Add positional encodings\n",
    "        # Only use the first seq_len positions\n",
    "        x = token_embeddings + self.positional_encoding[:seq_len, :]\n",
    "\n",
    "        # Step 3: Create causal mask\n",
    "        # PyTorch expects float mask: 0 for attend, -inf for mask\n",
    "        causal_mask = create_causal_mask(seq_len)\n",
    "        attn_mask = torch.where(causal_mask, torch.tensor(float(\"-inf\")), torch.tensor(0.0)).to(\n",
    "            x.device\n",
    "        )\n",
    "\n",
    "        # Step 4: Pass through transformer\n",
    "        x = self.transformer(x, mask=attn_mask)\n",
    "\n",
    "        # Step 5: Project to vocabulary and apply log softmax\n",
    "        logits = self.output_layer(x)\n",
    "        return functional.log_softmax(logits, dim=-1)\n",
    "        # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a small model for testing\n",
    "test_vocab_size = 100\n",
    "test_embed_dim = 32\n",
    "test_num_heads = 4\n",
    "test_hidden_dim = 64\n",
    "test_num_layers = 2\n",
    "test_max_seq_len = 20\n",
    "\n",
    "model = TransformerLM(\n",
    "    vocab_size=test_vocab_size,\n",
    "    embed_dim=test_embed_dim,\n",
    "    num_heads=test_num_heads,\n",
    "    hidden_dim=test_hidden_dim,\n",
    "    num_layers=test_num_layers,\n",
    "    max_seq_len=test_max_seq_len,\n",
    ")\n",
    "\n",
    "# Test with a batch of sequences\n",
    "test_batch = torch.randint(0, test_vocab_size, (4, 10))  # 4 sequences of length 10\n",
    "output = model(test_batch)\n",
    "\n",
    "assert output.shape == (\n",
    "    4,\n",
    "    10,\n",
    "    test_vocab_size,\n",
    "), f\"Output shape should be (4, 10, {test_vocab_size}), got {output.shape}\"\n",
    "\n",
    "# Log probabilities should sum to 0 (in log space, probabilities sum to 1)\n",
    "prob_sums = output.exp().sum(dim=-1)\n",
    "assert torch.allclose(\n",
    "    prob_sums, torch.ones_like(prob_sums), atol=1e-4\n",
    "), \"Probabilities should sum to 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with different sequence lengths\n",
    "test_batch_short = torch.randint(0, test_vocab_size, (2, 5))\n",
    "output_short = model(test_batch_short)\n",
    "assert output_short.shape == (\n",
    "    2,\n",
    "    5,\n",
    "    test_vocab_size,\n",
    "), \"Should handle variable sequence lengths\"\n",
    "\n",
    "# Test that model is deterministic with same input\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out1 = model(test_batch)\n",
    "    out2 = model(test_batch)\n",
    "assert torch.allclose(out1, out2), \"Model should be deterministic in eval mode\"\n",
    "\n",
    "# Test that output values are in valid log probability range\n",
    "assert (output <= 0).all(), \"Log probabilities should be <= 0\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Problem 6: Compute Perplexity\n",
    "\n",
    "**Perplexity** is a common metric for evaluating language models. It measures how \"surprised\" the model is by the test data. Lower perplexity means the model assigns higher probability to the actual next tokens.\n",
    "\n",
    "Perplexity is defined as:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i | w_1, ..., w_{i-1})\\right)$$\n",
    "\n",
    "which is the exponential of the average negative log-likelihood (cross-entropy loss).\n",
    "\n",
    "Implement `compute_perplexity` that calculates the perplexity of a model on a batch of sequences.\n",
    "\n",
    "**Parameters:**\n",
    "- `model`: A TransformerLM model\n",
    "- `sequences`: Tensor of shape `(batch_size, seq_len)` containing token indices\n",
    "\n",
    "**Returns:**\n",
    "- A scalar tensor containing the perplexity\n",
    "\n",
    "**Hint:** The model outputs log probabilities for predicting the next token at each position. For a sequence `[w0, w1, w2, w3]`, position 0's output predicts `w1`, position 1's output predicts `w2`, etc. Use `sequences[:, 1:]` as targets and `log_probs[:, :-1, :]` as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, sequences):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a language model on a batch of sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A TransformerLM model\n",
    "    - sequences: Tensor of shape (batch_size, seq_len)\n",
    "\n",
    "    Returns:\n",
    "    - perplexity: Scalar tensor\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get log probabilities from the model\n",
    "        log_probs = model(sequences)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Shift: use positions 0 to seq_len-2 to predict positions 1 to seq_len-1\n",
    "        # Predictions: log_probs[:, :-1, :] -> (batch, seq_len-1, vocab_size)\n",
    "        # Targets: sequences[:, 1:] -> (batch, seq_len-1)\n",
    "        predictions = log_probs[:, :-1, :]\n",
    "        targets = sequences[:, 1:]\n",
    "\n",
    "        # Gather the log probabilities of the actual next tokens\n",
    "        # targets.unsqueeze(-1) -> (batch, seq_len-1, 1)\n",
    "        target_log_probs = predictions.gather(dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Compute average negative log likelihood\n",
    "        avg_nll = -target_log_probs.mean()\n",
    "\n",
    "        # Perplexity is exp of average NLL\n",
    "        return torch.exp(avg_nll)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use the model from Problem 5\n",
    "test_sequences = torch.randint(0, test_vocab_size, (8, 15))\n",
    "perplexity = compute_perplexity(model, test_sequences)\n",
    "\n",
    "assert perplexity.dim() == 0, \"Perplexity should be a scalar\"\n",
    "assert perplexity > 0, \"Perplexity should be positive\"\n",
    "\n",
    "# For a random untrained model, perplexity should be around vocab_size\n",
    "assert (\n",
    "    perplexity > 50 and perplexity < 200\n",
    "), f\"For untrained model, perplexity ~{test_vocab_size}, got {perplexity:.1f}\"\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Perplexity should be consistent\n",
    "perplexity2 = compute_perplexity(model, test_sequences)\n",
    "assert torch.allclose(perplexity, perplexity2), \"Perplexity should be deterministic\"\n",
    "\n",
    "# Test with a single sequence\n",
    "single_seq = torch.randint(0, test_vocab_size, (1, 10))\n",
    "single_perp = compute_perplexity(model, single_seq)\n",
    "assert single_perp.dim() == 0, \"Should work with single sequence\"\n",
    "assert single_perp > 0, \"Should be positive for single sequence\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this assignment, you implemented:\n",
    "\n",
    "1. **Scaled dot-product attention** - the core attention mechanism\n",
    "2. **Causal masking** - preventing attention to future positions\n",
    "3. **Sinusoidal positional encoding** - adding position information\n",
    "4. **A complete transformer language model** - combining all components\n",
    "5. **Perplexity computation** - evaluating language model quality\n",
    "\n",
    "These are the fundamental building blocks of modern language models like GPT. The actual models used in practice have many more layers, larger dimensions, and are trained on massive datasets, but the core mechanisms are the same as what you implemented here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
