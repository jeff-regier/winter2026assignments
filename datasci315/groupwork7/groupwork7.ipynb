{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQdBxjSzfPi"
   },
   "source": [
    "# DATASCI 315, Group Work 7: High-Dimensional Spaces, the Bias-Variance Trade-Off, Ensemble Methods, and Data Augmentation\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. *During lab, feel free to flag down your GSI to ask questions at any point!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1zS2I9-tnAR"
   },
   "source": [
    "To begin, let's import some packages that we'll use throughout this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqv6wFC-zrrv"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-bright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnFp7yDLz-f0"
   },
   "source": [
    "## Part A: High-Dimensional Space\n",
    "\n",
    "This part investigates the strange properties of high-dimensional spaces. We consider the following two properties of high-dimensional spaces:\n",
    "\n",
    "1. The closeness of random points in a high-dimensional space\n",
    "2. The proportion of a bounding hypercube contained within a hypersphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BjDnxhMzgJc"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1a:** Distance in High-Dimensional Space\n",
    "\n",
    "Given $n$ random points (from a standard multivariate normal distribution) compute the average norm, the minimum, maximum pairwise distance and the ratio. Complete the following function, which returns the average norm and the ratio.\n",
    "\n",
    "Use [`torch.linalg.norm`](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html) to compute vector norms.\n",
    "\n",
    "Note: While computing minimum, ignore the self distances which are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRCFRNnFzmXQ"
   },
   "outputs": [],
   "source": [
    "def distance(n_dim=1, n_data=1000):\n",
    "    # BEGIN SOLUTION\n",
    "    data_points = torch.randn(n_data, n_dim)\n",
    "\n",
    "    # compute the average norm\n",
    "    avg_norm = torch.linalg.norm(data_points, dim=1).mean().item()\n",
    "\n",
    "    # compute pairwise distances\n",
    "    reshaped_data = data_points.reshape(n_data, 1, n_dim)\n",
    "    diff = reshaped_data - reshaped_data.transpose(0, 1)\n",
    "    pairwise_distance = torch.sqrt(torch.sum(diff**2, dim=2))\n",
    "\n",
    "    # compute maximum and minimum (be careful)\n",
    "    maximum_distance = pairwise_distance.max().item()\n",
    "    pairwise_distance.fill_diagonal_(float(\"inf\"))\n",
    "    minimum_distance = pairwise_distance.min().item()\n",
    "\n",
    "    # compute ratio\n",
    "    ratio = maximum_distance / minimum_distance\n",
    "    return avg_norm, ratio\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "avg_norm, ratio = distance(n_dim=10, n_data=100)\n",
    "assert isinstance(avg_norm, float), \"avg_norm should be a float\"\n",
    "assert isinstance(ratio, float), \"ratio should be a float\"\n",
    "assert avg_norm > 0, \"Average norm should be positive\"\n",
    "assert ratio > 1, \"Ratio of max/min distance should be greater than 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "avg_norm_100, ratio_100 = distance(n_dim=100, n_data=500)\n",
    "assert (\n",
    "    9.0 < avg_norm_100 < 11.0\n",
    "), \"Average norm for 100 dimensions should be approximately sqrt(100)\"\n",
    "assert ratio_100 < 2.0, \"Ratio should converge to near 1 in high dimensions\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 7994,
     "status": "ok",
     "timestamp": 1741741798731,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "SYvqHuWcz1cH",
    "outputId": "9138ee7e-df5d-4948-fb92-ac063e3c0097"
   },
   "outputs": [],
   "source": [
    "dim = torch.arange(5, 500, 100)\n",
    "\n",
    "# Run distance function for each dimension\n",
    "norms = []\n",
    "ratios = []\n",
    "for d in dim:\n",
    "    n, r = distance(n_dim=d.item())\n",
    "    norms.append(n)\n",
    "    ratios.append(r)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "ax[0].plot(dim, norms)\n",
    "ax[0].set_xlabel(\"dimension\")\n",
    "ax[0].set_ylabel(\"average norm\")\n",
    "\n",
    "ax[1].plot(dim, ratios)\n",
    "ax[1].set_xlabel(\"dimension\")\n",
    "ax[1].set_ylabel(\"ratio of max/min distance\")\n",
    "\n",
    "ax[2].plot(dim[3:], ratios[3:])\n",
    "ax[2].set_xlabel(\"dimension\")\n",
    "ax[2].set_ylabel(\"ratio of max/min distance\")\n",
    "ax[2].set_ylim(0, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dENe4HFazllv"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1b:** Hypersphere in Bounding Hypercube\n",
    "\n",
    "Consider the hypersphere $B(0,r)=\\{x\\in R^d: \\Vert x\\Vert \\leq r\\}$ of radius $r$ - this is a generalization of the disk (in 2-d) and sphere (in 3-d). Let $B=B(0,1)$ be the standard hypersphere (unit Euclidean ball).\n",
    "\n",
    " This hypersphere $B$ is a subset of the hypercube $H=\\{x\\in R^d: -1 \\leq x_i \\leq 1 \\text{ for all } i=1,\\dots,d\\}$ - this is a generalization of a square (in 2-d) and cube (in 3-d). See visualization in 2-d for reference. $H$ is the smallest possible cube that contains the ball $B$. We are interested in how much of the volume of $H$ is taken up by $B$.\n",
    "\n",
    " The volume of the hypercube is\n",
    "\n",
    "$$V_d(H) = 2^d.$$\n",
    "\n",
    "The volume of the hypersphere is given by\n",
    "\n",
    "$$V_d(B) = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)}$$\n",
    "\n",
    "where $\\Gamma$ is the Gamma function (use `math.gamma` for scalar computations)\n",
    "\n",
    "(i) Complete the function, which takes `n_dim` as input and returns the ratio (volume of the hypersphere $B$) divided by (volume of hypercube $H$) of that dimension.\n",
    "\n",
    "(ii) Plot the ratio for dimensions from 2 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1741741813575,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "uaa6iyCCzlX7",
    "outputId": "76c8d5ee-06ea-4f4b-9fdc-4b8a29248857"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def volume(n_dim):\n",
    "    # compute hypercube volume\n",
    "    cube_volume = 2**n_dim\n",
    "\n",
    "    # compute sphere volume (use math.gamma for the Gamma function)\n",
    "    sphere_volume = math.pi ** (n_dim / 2) / math.gamma(n_dim / 2 + 1)\n",
    "\n",
    "    # ratio\n",
    "    return sphere_volume / cube_volume\n",
    "\n",
    "\n",
    "volume(2)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "vol_ratio_2d = volume(2)\n",
    "assert isinstance(vol_ratio_2d, float), \"volume() should return a float\"\n",
    "assert 0 < vol_ratio_2d < 1, \"Volume ratio should be between 0 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert abs(volume(2) - (math.pi / 4)) < 0.01, \"2D ratio should be pi/4\"\n",
    "assert volume(10) < volume(5), \"Volume ratio should decrease with dimension\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koOlCOze0KPu"
   },
   "source": [
    "In 2-d, the volume of the hypercube (unit square) is 4 and that of the hypersphere (unit disk) is $\\pi\\approx 3.14159$, hence the ratio is about $0.76853982$. Here is a visualization. We are interested in the volume in the square outside the disk (i.e., the red part shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1741741817417,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "AbUcYcms0Knj",
    "outputId": "41f7ce3d-b8a9-4702-b8bd-ec8560a84f5f"
   },
   "outputs": [],
   "source": [
    "circle = plt.Circle((0, 0), 1, fill=True, color=\"blue\", label=\"Unit Circle\", alpha=0.5)\n",
    "\n",
    "# Unit square\n",
    "square = plt.Rectangle((-1, -1), 2, 2, fill=True, color=\"red\", label=\"Unit Square\", alpha=0.5)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.add_artist(square)\n",
    "ax.add_artist(circle)\n",
    "\n",
    "# Set plot limits and aspect ratio\n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Unit Disk and Bounding Unit Square\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjeJWHg10dh8"
   },
   "source": [
    "Write your code for plotting the ratio of volume across dimensions. If correctly implemented, you should note how the ratio basically gets to 0 - the sphere hardly takes up any space within the cube. Most of the volume within the cube is at the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1741741818988,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "Kanqtwqj0ZuZ",
    "outputId": "38065001-c6ba-4c36-f7c1-97d80c078b2e"
   },
   "outputs": [],
   "source": [
    "dim = list(range(2, 20))\n",
    "\n",
    "ratios = [volume(d) for d in dim]\n",
    "plt.plot(dim, ratios)\n",
    "plt.xlabel(\"dimension\")\n",
    "plt.ylabel(\"ratio of volume of sphere/volume of cube\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmU-yz9u06u9"
   },
   "source": [
    "## Part B: Bias-Variance Trade-off and Ensemble Methods\n",
    "\n",
    "In this section, we go over (i) bias-variance trade-off and (ii) ensemble methods using a simple regression problem. Here are some helper codes and plot for the function (domain is $[0,1]$). This follows Section 8.2-8.3 in *Understanding Deep Learning*, using a single layer network (no activation) where the least squares solution has a closed-form expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1741741821333,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "yhxQLbL01Pwx",
    "outputId": "a630ede6-52c3-48df-bec4-1dafb59e941e"
   },
   "outputs": [],
   "source": [
    "# True function we're trying to estimate, defined on [0,1]\n",
    "def true_function(x):\n",
    "    return torch.exp(torch.sin(x * 2 * torch.pi))\n",
    "\n",
    "\n",
    "def generate_data(n_data, sigma=0.3):\n",
    "    \"\"\"Generate n_data points with quasi-uniform x and noisy y.\"\"\"\n",
    "    x = torch.linspace(0, 1, n_data) + torch.rand(n_data) / n_data\n",
    "    x = torch.clamp(x, 0, 1)  # keep in [0,1]\n",
    "    y = true_function(x) + sigma * torch.randn(n_data)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Evaluation grid: 100 evenly-spaced points on [0,1] (constant throughout Part B)\n",
    "X_EVAL = torch.linspace(0, 1, 100)\n",
    "Y_TRUE = true_function(X_EVAL)\n",
    "\n",
    "\n",
    "def plot_regression(x_data=None, y_data=None, y_pred=None, y_std=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot regression results on the evaluation grid X_EVAL.\n",
    "\n",
    "    Args:\n",
    "        x_data, y_data: Training data points\n",
    "        y_pred: Model predictions at X_EVAL\n",
    "        y_std: Prediction standard deviation at X_EVAL (for uncertainty bands)\n",
    "        ax: Matplotlib axis\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    # True function (black line)\n",
    "    ax.plot(X_EVAL, Y_TRUE, \"k-\")\n",
    "\n",
    "    # Model prediction with optional uncertainty band\n",
    "    if y_pred is not None:\n",
    "        if y_std is not None:\n",
    "            ax.fill_between(X_EVAL, y_pred - 2 * y_std, y_pred + 2 * y_std, color=\"lightgray\")\n",
    "        ax.plot(X_EVAL, y_pred, \"-\", color=\"#7fe7de\")\n",
    "\n",
    "    # Training data (orange dots)\n",
    "    if x_data is not None:\n",
    "        ax.scatter(x_data, y_data, color=\"#d18362\", zorder=5)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Generate example data\n",
    "torch.manual_seed(1)\n",
    "x_data, y_data = generate_data(n_data=15, sigma=0.3)\n",
    "\n",
    "# Plot true function and data\n",
    "plot_regression(x_data, y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7X8Mby61xrX"
   },
   "source": [
    "Below we define the model (`network`) and a function to fit it in closed form using [`torch.linalg.lstsq`](https://pytorch.org/docs/stable/generated/torch.linalg.lstsq.html). The model is a piecewise-linear function with `n_hidden` segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1741741822737,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "rqpfKyCQ104T",
    "outputId": "565b5f24-7dbc-4c3d-81eb-3790be073057"
   },
   "outputs": [],
   "source": [
    "def network(x, beta, omega):\n",
    "    \"\"\"Simple piecewise-linear model with n_hidden segments.\"\"\"\n",
    "    n_hidden = omega.shape[0]\n",
    "    y = torch.zeros_like(x)\n",
    "    for i in range(n_hidden):\n",
    "        h = x - i / n_hidden\n",
    "        h = h * (h > 0)  # ReLU-like activation\n",
    "        y = y + omega[i] * h\n",
    "    return y + beta\n",
    "\n",
    "\n",
    "def fit_model(x, y, n_hidden):\n",
    "    \"\"\"Fit the model in closed form using least squares.\"\"\"\n",
    "    n_data = len(x)\n",
    "    design_matrix = torch.ones((n_data, n_hidden + 1))\n",
    "    for i in range(n_data):\n",
    "        for j in range(1, n_hidden + 1):\n",
    "            design_matrix[i, j] = max(x[i] - (j - 1) / n_hidden, 0)\n",
    "\n",
    "    beta_omega = torch.linalg.lstsq(design_matrix, y).solution\n",
    "    return beta_omega[0], beta_omega[1:]  # beta, omega\n",
    "\n",
    "\n",
    "# Fit and plot example model\n",
    "beta, omega = fit_model(x_data, y_data, n_hidden=3)\n",
    "y_pred = network(X_EVAL, beta, omega)\n",
    "\n",
    "plot_regression(x_data, y_data, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYp_MQAS1Te7"
   },
   "source": [
    "### Part B.1: Bias-Variance Trade-Off\n",
    "\n",
    "A very important aspect of machine learning is understanding bias-variance trade-off. As we know, there are three sources of error in our modeling: (i) bias, (ii) variance and (iii) noise (irreducible part).\n",
    "\n",
    "*Rule of thumb: Model with higher complexity will lower bias at cost of higher variance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBm8zMQe17cn"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2a:** Model Mean and Variance Helper Function\n",
    "\n",
    "The function repeats the experiment `n_datasets` times, each time drawing a random dataset of size `n_data` with noise level `sigma` and fitting a model with `n_hidden` hidden units. It computes the mean and standard deviation of model predictions over the evaluation grid `X_EVAL`. This gives an estimate of the bias and variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFxIhJCN19qu"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def get_model_mean_variance(n_data, n_datasets, n_hidden, sigma):\n",
    "    \"\"\"Run n_datasets experiments and return mean/std of predictions at X_EVAL.\"\"\"\n",
    "    all_predictions = torch.zeros((n_datasets, len(X_EVAL)))\n",
    "\n",
    "    for i in range(n_datasets):\n",
    "        # Generate new random dataset\n",
    "        x_data, y_data = generate_data(n_data, sigma)\n",
    "\n",
    "        # Fit model and predict on evaluation grid\n",
    "        beta, omega = fit_model(x_data, y_data, n_hidden)\n",
    "        all_predictions[i] = network(X_EVAL, beta, omega)\n",
    "\n",
    "    return all_predictions.mean(dim=0), all_predictions.std(dim=0)\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "mean_test, std_test = get_model_mean_variance(10, 5, 3, 0.3)\n",
    "assert mean_test.shape == (100,), \"Mean should have 100 elements (matching X_EVAL)\"\n",
    "assert std_test.shape == (100,), \"Std should have 100 elements\"\n",
    "assert torch.all(std_test >= 0), \"Standard deviation should be non-negative\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "torch.manual_seed(42)\n",
    "mean_hidden, std_hidden = get_model_mean_variance(20, 50, 5, 0.3)\n",
    "assert mean_hidden.shape == (100,), \"Mean shape check\"\n",
    "assert 0 < std_hidden.mean().item() < 1, \"Std should be reasonable\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0zBD1rp2Cj9"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2b:** Recreate Figure 8.6 from *Understanding Deep Learning*\n",
    "\n",
    "Use 100 repetitions for each of sample size 6, 10, 100 (using $\\sigma=0.3$ and `n_hidden`=3). Plot the bias and variances in 3 plots side-by-side. Your plots should show how variance decreases as sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1741741826395,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "i7iO7i5N2DIc",
    "outputId": "64802d7e-9791-46fa-d14f-09e65f8d8124"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "torch.manual_seed(1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "for ax, n_data in zip(axes, [6, 10, 100]):\n",
    "    mean_pred, std_pred = get_model_mean_variance(n_data, n_datasets=100, n_hidden=3, sigma=0.3)\n",
    "    plot_regression(y_pred=mean_pred, y_std=std_pred, ax=ax)\n",
    "    ax.set_title(f\"{n_data} samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 2b\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 2b\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxk_d9I52K7i"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2c:** Recreate Figure 8.7 from *Understanding Deep Learning*\n",
    "\n",
    "Use 100 repetitions for each of `n_hidden` 3, 5, 10 (using $\\sigma=0.3$ and `n_data`=10). Plot the bias and variances in 3 plots side-by-side. Your plots should show how variance increases with model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1741741828322,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "O8ZuAv9Y2Le_",
    "outputId": "31f15ca5-8ab8-4d55-c571-7bd3ef79fdeb"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "torch.manual_seed(2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "for ax, n_hidden in zip(axes, [3, 5, 10]):\n",
    "    mean_pred, std_pred = get_model_mean_variance(\n",
    "        n_data=10, n_datasets=100, n_hidden=n_hidden, sigma=0.3\n",
    "    )\n",
    "    plot_regression(y_pred=mean_pred, y_std=std_pred, ax=ax)\n",
    "    ax.set_title(f\"{n_hidden} hidden units\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 2c\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 2c\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2HnzTzh2T3l"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2d:** Recreate Figure 8.9 from *Understanding Deep Learning*\n",
    "\n",
    "Plot bias and variance terms as a function of the model capacity (number of hidden units) in the simplified model using setting from previous problem with `n_data`=15. Use 100 repetitions for each. Your plot should show the classic U-shaped curve where total error (bias + variance) is minimized at an intermediate model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1741741830098,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "w8kYMI1J2UN4",
    "outputId": "2c9b84ef-9296-48d7-d5bc-69a003a0f355"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "torch.manual_seed(1)\n",
    "\n",
    "hidden_units = list(range(1, 13))\n",
    "bias = torch.zeros(len(hidden_units))\n",
    "variance = torch.zeros(len(hidden_units))\n",
    "\n",
    "for i, n_hidden in enumerate(hidden_units):\n",
    "    mean_pred, std_pred = get_model_mean_variance(\n",
    "        n_data=15, n_datasets=100, n_hidden=n_hidden, sigma=0.3\n",
    "    )\n",
    "    variance[i] = torch.mean(std_pred**2)\n",
    "    bias[i] = torch.mean((mean_pred - Y_TRUE) ** 2)\n",
    "\n",
    "# Plot bias-variance tradeoff\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hidden_units, variance, label=\"variance\", color=\"mediumaquamarine\")\n",
    "ax.plot(hidden_units, bias, label=\"bias\", color=\"sandybrown\")\n",
    "ax.plot(hidden_units, variance + bias, \"--\", color=\"gray\", label=\"bias + variance\")\n",
    "ax.set_xlim(1, 12)\n",
    "ax.set_ylim(0, 0.4)\n",
    "ax.set_xlabel(\"Model capacity (hidden units)\")\n",
    "ax.set_ylabel(\"Mean squared error\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 2d\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 2d\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PX8pnXI2bGY"
   },
   "source": [
    "### Part B.2: Ensemble Methods\n",
    "\n",
    "This section investigates how ensembling can improve the performance of models. We'll work with the same ground truth and neural network model as in Part B.1 which we can fit in closed form, and so we can eliminate any errors due to not finding the global maximum.\n",
    "\n",
    "We start with a baseline model using `n_hidden`=14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1741741832016,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "iG1iJ9vd3xXu",
    "outputId": "26315415-f1de-4d5d-a874-a23130b942d4"
   },
   "outputs": [],
   "source": [
    "# Generate training data for ensemble experiments\n",
    "torch.manual_seed(1)\n",
    "n_data = 15\n",
    "x_data, y_data = generate_data(n_data, sigma=0.3)\n",
    "\n",
    "# Fit single model with high capacity (n_hidden=14)\n",
    "beta, omega = fit_model(x_data, y_data, n_hidden=14)\n",
    "y_pred = network(X_EVAL, beta, omega)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plot_regression(x_data, y_data, y_pred, ax=ax)\n",
    "ax.set_title(\"Single Model (n_hidden=14)\")\n",
    "plt.show()\n",
    "\n",
    "mse = torch.mean((y_pred - Y_TRUE) ** 2)\n",
    "print(f\"Mean square error = {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvdI3RJw2-w6"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 3a:** Ensembling 10 Models\n",
    "\n",
    "Let `n_model`=10 be the number of models used. Each model will use the same architecture (in this case controlled via the `n_hidden` parameter, as before) explicitly `n_hidden`=14. However, each model will be trained on a bootstrapped sample (a sample of size $n$ taken *with replacement* from the original training data, also of size $n$).\n",
    "\n",
    "Complete the code chunk below to achieve this and collect the results in `all_y_model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1741741833189,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "W_2rq5Di2_Kw",
    "outputId": "166317c9-c2ce-41c7-a890-c9d5ee2890af"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "n_models = 10\n",
    "all_predictions = torch.zeros((n_models, len(X_EVAL)))\n",
    "\n",
    "for i in range(n_models):\n",
    "    # Bootstrap sample: sample with replacement\n",
    "    indices = torch.randint(0, n_data, (n_data,))\n",
    "    x_boot, y_boot = x_data[indices], y_data[indices]\n",
    "\n",
    "    # Fit model on bootstrap sample\n",
    "    beta, omega = fit_model(x_boot, y_boot, n_hidden=14)\n",
    "    all_predictions[i] = network(X_EVAL, beta, omega)\n",
    "\n",
    "    mse = torch.mean((all_predictions[i] - Y_TRUE) ** 2)\n",
    "    print(f\"Model {i}: MSE = {mse:.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert all_predictions.shape == (n_models, 100), \"all_predictions should have shape (n_models, 100)\"\n",
    "assert not torch.all(\n",
    "    all_predictions[0] == all_predictions[1]\n",
    "), \"Different models should produce different outputs\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert all_predictions.shape[0] == 10, \"Should have 10 models\"\n",
    "assert torch.std(all_predictions, dim=0).mean() > 0, \"Models should have variance\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXVXkcpY45FE"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 3b:** Aggregation\n",
    "\n",
    "Now, we have results from 10 different models. Thus at each $x$, we have 10 different predictions. To aggregate these, one can use **mean** or **median** (for classification task, this can be a majority vote). For both of these aggregation methods, compute the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1741741835172,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "2dD-h8s-45cp",
    "outputId": "78f59217-a391-4397-e1af-32b911820681"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "y_median = torch.median(all_predictions, dim=0).values\n",
    "y_mean = torch.mean(all_predictions, dim=0)\n",
    "\n",
    "mse_median = torch.mean((y_median - Y_TRUE) ** 2)\n",
    "mse_mean = torch.mean((y_mean - Y_TRUE) ** 2)\n",
    "\n",
    "print(f\"MSE for Median ensemble = {mse_median:.3f}\")\n",
    "print(f\"MSE for Mean ensemble = {mse_mean:.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert y_median.shape == (100,), \"Median aggregation should produce 100 values\"\n",
    "assert y_mean.shape == (100,), \"Mean aggregation should produce 100 values\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert not torch.allclose(y_median, y_mean), \"Median and mean should differ\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1741741836614,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "C3zhjSPQ5XSG",
    "outputId": "98ca2c47-32e8-4253-9889-e01fb1b3c3f5"
   },
   "outputs": [],
   "source": [
    "# Plot ensemble results\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "\n",
    "plot_regression(x_data, y_data, y_median, ax=axes[0])\n",
    "axes[0].set_title(\"Median Ensemble\")\n",
    "\n",
    "plot_regression(x_data, y_data, y_mean, ax=axes[1])\n",
    "axes[1].set_title(\"Mean Ensemble\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62v3KBlT6Psp"
   },
   "source": [
    "You should see that both the median and mean models are better than most of the individual models. We have improved our performance at the cost of ten times as much training time, storage, and inference time. Note in the plots how much of the overfitting is also eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mC_f82P0DJP"
   },
   "source": [
    "## Part C: MNIST 1-D Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b1MgDpA1vQ5"
   },
   "source": [
    "The MNIST 1-D Dataset is a 1-dimensional version of MNIST digit dataset - you can check details [here](https://github.com/greydanus/mnist1d). Each digit image is now represented as a vector (1-d) with 40 features. We do not need to get into details about how this was created, rather we take the dataset as given. The only thing to keep in mind is that this is slightly harder dataset compared to the usual MNIST. The first part of the group work focus on this dataset and coming up with a good deep neural classifier for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37416,
     "status": "ok",
     "timestamp": 1741903345642,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "W686MME0z-uQ",
    "outputId": "f7d7613e-0ce6-4f97-9a4c-403bfc8a1514"
   },
   "outputs": [],
   "source": [
    "# Run this if you're in a Colab to install MNIST 1D repository\n",
    "%pip install git+https://github.com/greydanus/mnist1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUS99ot91s1o"
   },
   "source": [
    "Let's generate a training and test dataset using the MNIST1D code. The dataset gets saved as a .pkl file so it doesn't have to be regenerated each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9012,
     "status": "ok",
     "timestamp": 1741741871602,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "lqoZgrcc1pTd",
    "outputId": "fb380266-787f-4a85-f6bc-9cafdb77cff9"
   },
   "outputs": [],
   "source": [
    "import mnist1d\n",
    "\n",
    "args = mnist1d.data.get_dataset_args()\n",
    "data = mnist1d.data.get_dataset(\n",
    "    args, path=\"./data/mnist1d_data.pkl\", download=False, regenerate=False\n",
    ")\n",
    "\n",
    "# The training and test input and outputs are in\n",
    "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
    "print(\"Examples in training set: {}\".format(len(data[\"y\"])))\n",
    "print(\"Examples in test set: {}\".format(len(data[\"y_test\"])))\n",
    "print(\"Length of each example: {}\".format(data[\"x\"].shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYal-fQv3DEC"
   },
   "source": [
    "Let us visualize the dataset in 2-d using PCA and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85139,
     "status": "ok",
     "timestamp": 1741741960404,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "mIUpYeE77fvI",
    "outputId": "d174dfa1-7a1f-4783-d3c1-b5503f17b16d"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist.data = torch.flatten(mnist.data, start_dim=1)\n",
    "\n",
    "mnist_labels = mnist.targets\n",
    "idx = torch.randperm(60000)[:4000]\n",
    "mnist_subset = mnist.data[idx]\n",
    "mnist_labels = mnist_labels[idx]\n",
    "\n",
    "X = data[\"x\"]\n",
    "labels = data[\"y\"]\n",
    "\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "X_tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3).fit_transform(X)\n",
    "mnist_pca = PCA(n_components=2).fit_transform(mnist_subset)\n",
    "mnist_tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3).fit_transform(\n",
    "    mnist_subset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2107,
     "status": "ok",
     "timestamp": 1741741971854,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "z8G_KR8N3mvS",
    "outputId": "dc5db66a-a5e6-4c50-c22a-2c5a6bee06d0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\n",
    "ax[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[0, 1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[1, 0].scatter(mnist_pca[:, 0], mnist_pca[:, 1], c=mnist_labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[1, 1].scatter(mnist_tsne[:, 0], mnist_tsne[:, 1], c=mnist_labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[0, 0].set_ylabel(\"MNIST 1D\")\n",
    "ax[1, 0].set_ylabel(\"MNIST original\")\n",
    "\n",
    "ax[0, 0].set_title(\"PCA\")\n",
    "ax[0, 1].set_title(\"t-SNE\")\n",
    "plt.suptitle(\"2D Visualization of MNIST 1-d and original (color by label)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghr8GdKE7Gbh"
   },
   "source": [
    "As seen above, there is not much separation between the classes (at least in this two-dimensional view). Compare this to that of the original MNIST data (at least the t-SNE) - hence the classification task on the 1-d version is expected to be harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization with He Initialization\n",
    "\n",
    "When training deep neural networks, proper weight initialization is critical. If weights start too large, activations can explode; if too small, gradients vanish during backpropagation. **He initialization** (also called Kaiming initialization) addresses this by setting the initial weights to have variance $\\text{Var}(w) = \\frac{2}{n_{\\text{in}}}$, where $n_{\\text{in}}$ is the number of input units to the layer.\n",
    "\n",
    "This scaling is specifically designed for ReLU-family activations, which zero out half their inputs on average. The factor of 2 compensates for this, keeping the variance of activations roughly constant across layers. See Section 7.5 of *Understanding Deep Learning* for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3tzwfYx1wG1"
   },
   "outputs": [],
   "source": [
    "def weights_init(layer_in):\n",
    "    # Initialize the parameters with He initialization\n",
    "    # Replace this line (see figure 7.8 of book for help)\n",
    "    if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer_in.weight)\n",
    "        layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u7g6zupMWx3"
   },
   "source": [
    "### Performance on MNIST 1-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc2Ps_4nw66i"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4a:** Training Function for MNIST-1D\n",
    "\n",
    "Complete the training function below using the following PyTorch components:\n",
    "- [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for the loss function\n",
    "- [`torch.optim.SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) for the optimizer\n",
    "- [`StepLR`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html) for learning rate scheduling\n",
    "- [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) and [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) for batching\n",
    "\n",
    "The `verbose` parameter can be toggled to either print loss/error through the training process or not.\n",
    "\n",
    "Hint: Scheduler can be used in torch as `StepLR(optimizer, step_size, gamma)` and its function is to reduce the learning rate by fraction `gamma` every `step_size` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Vuxuhl510w4"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def train(\n",
    "    model,\n",
    "    weights_init,\n",
    "    data,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    momentum,\n",
    "    decay=0,\n",
    "    schedule_params=(10, 0.5),\n",
    "    n_epoch=50,\n",
    "    *,\n",
    "    verbose=True,\n",
    "):\n",
    "    # choose cross entropy loss function (equation 5.24)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # construct SGD optimizer and initialize learning rate and momentum\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=decay\n",
    "    )\n",
    "\n",
    "    # object that decreases learning rate by half every 10 epochs\n",
    "    # schedule_params = (step_size, gamma) for StepLR object\n",
    "    scheduler = StepLR(optimizer, step_size=schedule_params[0], gamma=schedule_params[1])\n",
    "\n",
    "    # set up data\n",
    "    x_train = torch.tensor(data[\"x\"].astype(\"float32\"))\n",
    "    y_train = torch.tensor(data[\"y\"].transpose().astype(\"int64\"))\n",
    "    x_test = torch.tensor(data[\"x_test\"].astype(\"float32\"))\n",
    "    y_test = torch.tensor(data[\"y_test\"].astype(\"int64\"))\n",
    "\n",
    "    # load the data into a class that creates the batches\n",
    "    data_loader = DataLoader(\n",
    "        TensorDataset(x_train, y_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Initialize model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # loop over the dataset n_epoch times\n",
    "    # store the loss and the % correct at each epoch\n",
    "    losses_train = torch.zeros(n_epoch)\n",
    "    errors_train = torch.zeros(n_epoch)\n",
    "    losses_test = torch.zeros(n_epoch)\n",
    "    errors_test = torch.zeros(n_epoch)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        # loop over batches\n",
    "        for _i, batch in enumerate(data_loader):\n",
    "            # retrieve inputs and labels for this batch\n",
    "            x_batch, y_batch = batch\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass -- calculate model output\n",
    "            pred = model(x_batch)\n",
    "            # compute the loss\n",
    "            loss = loss_function(pred, y_batch)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # SGD update\n",
    "            optimizer.step()\n",
    "\n",
    "        # Run whole dataset to get statistics -- normally wouldn't do this\n",
    "        pred_train = model(x_train)\n",
    "        pred_test = model(x_test)\n",
    "        _, predicted_train_class = torch.max(pred_train.data, 1)\n",
    "        _, predicted_test_class = torch.max(pred_test.data, 1)\n",
    "        errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(\n",
    "            y_train\n",
    "        )\n",
    "        errors_test[epoch] = 100 - 100 * (predicted_test_class == y_test).float().sum() / len(\n",
    "            y_test\n",
    "        )\n",
    "        losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
    "        losses_test[epoch] = loss_function(pred_test, y_test).item()\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, \"\n",
    "                f\"train error {errors_train[epoch]:3.2f},  \"\n",
    "                f\"test loss {losses_test[epoch]:.6f}, \"\n",
    "                f\"test error {errors_test[epoch]:3.2f}\"\n",
    "            )\n",
    "\n",
    "        # tell scheduler to consider updating learning rate\n",
    "        scheduler.step()\n",
    "    return losses_train, errors_train, losses_test, errors_test\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4a\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4a\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "executionInfo": {
     "elapsed": 8881,
     "status": "ok",
     "timestamp": 1741742502369,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "BaD3P6eC5ZMf",
    "outputId": "9ec290a8-6fc9-4b94-fd49-b4ac2d78c60a"
   },
   "outputs": [],
   "source": [
    "# consider the following baseline\n",
    "D_i = 40  # Input dimensions\n",
    "D_k = 100  # Hidden dimensions\n",
    "D_o = 10  # Output dimensions\n",
    "\n",
    "model = nn.Sequential(nn.Linear(D_i, D_k), nn.ReLU(), nn.Linear(D_k, D_o))\n",
    "\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=data,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.05,\n",
    "    momentum=0.9,\n",
    "    schedule_params=(10, 0.5),\n",
    "    n_epoch=100,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "n_epoch = len(losses_train)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "ax[0].plot(errors_train, \"r-\", label=\"train\")\n",
    "ax[0].plot(errors_test, \"b-\", label=\"test\")\n",
    "ax[0].set_ylim(0, 100)\n",
    "ax[0].set_xlim(0, n_epoch)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Error\")\n",
    "ax[0].set_title(f\"Train Error {errors_train[-1]:3.2f}%, Test Error {errors_test[-1]:3.2f}%\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "ax[1].plot(losses_train, \"r-\", label=\"train\")\n",
    "ax[1].plot(losses_test, \"b-\", label=\"test\")\n",
    "ax[1].set_xlim(0, n_epoch)\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(f\"Train loss {losses_train[-1]:3.2f}, Test loss {losses_test[-1]:3.2f}\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9qRQrKprIQl"
   },
   "source": [
    "There are several tuning knobs in the above for improving the performance:\n",
    "1. Model architecture (number of layers, number of nodes in a layer, activation function, etc.)\n",
    "2. Data (batch size)\n",
    "3. Optimizer choices (learning rate, momentum, decreasing learning rate `scheduler`)\n",
    "4. Regularizer (adding dropout layer or using `weight decay` for $L_2$ regularization)\n",
    "\n",
    "Consider the model above as the baseline, which gives a test error of just above 40% (i.e., test accuracy just below 60%) and test loss of around 1.1. Can you do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 4b:** Tune MLP for 65% Accuracy\n",
    "\n",
    "Note: https://github.com/greydanus/mnist1d mentions for MLP the benchmark is 68% accuracy.\n",
    "\n",
    "Plot the training and test loss and error (as before) - ensure you do not visibly see significant overfitting (aka test loss increasing too much)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51426,
     "status": "ok",
     "timestamp": 1741742091719,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "WLYMrzSs5f-X",
    "outputId": "78780e7a-b61b-4f97-c63a-16249f204786"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "D_k = 500\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_i, D_k), nn.ELU(), nn.Linear(D_k, D_k), nn.ELU(), nn.Linear(D_k, D_o)\n",
    ")\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=data,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.05,\n",
    "    momentum=0.9,\n",
    "    decay=0.001,\n",
    "    schedule_params=(20, 0.7),\n",
    "    n_epoch=150,\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4b\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4b\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1741742097794,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "EuSh0IjvssWP",
    "outputId": "5f1550e0-f727-4eb7-b340-47cc67188abb"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "n_epoch = len(losses_train)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 3))\n",
    "ax[0].plot(errors_train, \"r-\", label=\"train\")\n",
    "ax[0].plot(errors_test, \"b-\", label=\"test\")\n",
    "ax[0].set_ylim(0, 100)\n",
    "ax[0].set_xlim(0, n_epoch)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Error\")\n",
    "ax[0].set_title(f\"Train Error {errors_train[-1]:3.2f}%, Test Error {errors_test[-1]:3.2f}%\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "ax[1].plot(losses_train, \"r-\", label=\"train\")\n",
    "ax[1].plot(losses_test, \"b-\", label=\"test\")\n",
    "ax[1].set_xlim(0, n_epoch)\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(f\"Train loss {losses_train[-1]:3.2f}, Test loss {losses_test[-1]:3.2f}\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVVbJcJUKpRk"
   },
   "source": [
    "### Data Augmentation with MNIST 1-D\n",
    "\n",
    "This part investigates data augmentation for the MNIST-1D model. Data augmentation is a commonly used method for generating more synthetic training samples by applying simple transformations (e.g. translation, inverting, scaling, filters, rotation) - for images at least, given an image of a dog e.g., a human can classify this even if the image is zoomed, rotated, irrespective of location of the dog in the image or any filters applied. This is the intuition behind data augmentation.\n",
    "\n",
    "Again, for baseline model, we use the previously used `baseline` (with 2 linear layers and 100 hidden nodes) - recall, for this model, we achieved a test loss of around 1.14 and a test error of around 42%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSsDfdV5Pe65"
   },
   "outputs": [],
   "source": [
    "D_i = 40  # Input dimensions\n",
    "D_k = 100  # Hidden dimensions\n",
    "D_o = 10  # Output dimensions\n",
    "\n",
    "model = nn.Sequential(nn.Linear(D_i, D_k), nn.ReLU(), nn.Linear(D_k, D_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8XX1uFbMQIX"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4c:** Augment Function\n",
    "\n",
    "Complete the following function which takes a sample $x$ (as `input_vector`) and applies some transformations and returns another vector `data_out`. For this problem, we apply two transformations:\n",
    "\n",
    "1. Shift $K$ places to the right:\n",
    "\n",
    "$$(x_1,x_2,\\dots,x_n) \\mapsto (x_{n-K+1}, \\dots, x_n, x_1, x_2, \\dots, x_{n-K})$$\n",
    "\n",
    "Note that the first coordinate $x_1$ (at python index 0) originally is $(K+1)$-th position (python index $K$) and this is done cyclically, so points that go off the end are added back to the beginning.\n",
    "\n",
    "For example, $n=4, K=2$: $(x_1,x_2,x_3,x_4)\\mapsto (x_3,x_4,x_1,x_2)$.\n",
    "\n",
    "2. Scaling: Scale by a random number drawn from uniform over (0.8, 1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1741742515262,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "JuY5s6_LYhAy",
    "outputId": "192395e8-a139-476a-88de-f7fc374af2bf"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def augment(input_vector):\n",
    "    # Create output vector\n",
    "    input_tensor = (\n",
    "        torch.tensor(input_vector, dtype=torch.float32)\n",
    "        if not isinstance(input_vector, torch.Tensor)\n",
    "        else input_vector.clone()\n",
    "    )\n",
    "    n = len(input_tensor)\n",
    "    data_out = torch.zeros_like(input_tensor)\n",
    "\n",
    "    # TODO:  Shift the input data by a random offset\n",
    "    # (rotating, so points that would go off the end, are added back to the beginning)\n",
    "    k = torch.randint(0, n, (1,)).item()\n",
    "    data_out[k:] = input_tensor[: (n - k)]\n",
    "    data_out[:k] = input_tensor[(n - k) :]\n",
    "\n",
    "    # TODO: Randomly scale data by factor from uniform [0.8, 1.2]\n",
    "    # Replace this line:\n",
    "    scale = 0.8 + 0.4 * torch.rand(1).item()\n",
    "    return data_out * scale\n",
    "\n",
    "\n",
    "# example\n",
    "augment([1, 2, 3, 4])\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(0)\n",
    "test_input = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "test_output = augment(test_input)\n",
    "assert len(test_output) == len(test_input), \"Output length should match input length\"\n",
    "assert isinstance(test_output, torch.Tensor), \"Output should be a torch.Tensor\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check that augmentation applies both shift and scale\n",
    "torch.manual_seed(123)\n",
    "orig = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "aug = augment(orig)\n",
    "assert not torch.allclose(orig, aug), \"Augmentation should modify the input\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLFrF1tkPS8k"
   },
   "source": [
    "Let us construct augment our original training data with such transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVaOKTeEOmxq"
   },
   "outputs": [],
   "source": [
    "n_data_orig = data[\"x\"].shape[0]\n",
    "# We'll double the amount of data\n",
    "n_data_augment = n_data_orig * 2\n",
    "augmented_x = torch.zeros((n_data_augment, D_i))\n",
    "augmented_y = torch.zeros(n_data_augment)\n",
    "# First n_data_orig rows are original data\n",
    "augmented_x[0:n_data_orig, :] = torch.tensor(data[\"x\"], dtype=torch.float32)\n",
    "augmented_y[0:n_data_orig] = torch.tensor(data[\"y\"], dtype=torch.float32)\n",
    "\n",
    "# Fill in rest of with augmented data\n",
    "for c_augment in range(n_data_orig, n_data_augment):\n",
    "    # Choose a data point randomly\n",
    "    random_data_index = torch.randint(0, n_data_orig - 1, (1,)).item()\n",
    "    # Augment the point and store\n",
    "    augmented_x[c_augment, :] = augment(\n",
    "        torch.tensor(data[\"x\"][random_data_index, :], dtype=torch.float32)\n",
    "    )\n",
    "    augmented_y[c_augment] = data[\"y\"][random_data_index]\n",
    "\n",
    "# to use the train function we created above\n",
    "augmented_data = {\n",
    "    \"x\": augmented_x.numpy(),\n",
    "    \"y\": augmented_y.numpy(),\n",
    "    \"x_test\": data[\"x_test\"],\n",
    "    \"y_test\": data[\"y_test\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkKqXtozP_n9"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4d:** Training on Augmented Data\n",
    "\n",
    "Use the `train` function to train the data, using the same tuning knobs as used in the baseline case. What is the test loss and error now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16404,
     "status": "ok",
     "timestamp": 1741742790249,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "reTZvaiSOo86",
    "outputId": "5447a4fa-478f-4ba6-9e72-14eac92414e5"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=augmented_data,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.05,\n",
    "    momentum=0.9,\n",
    "    schedule_params=(10, 0.5),\n",
    "    n_epoch=100,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4d\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4d\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyhNFFBhQttm"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 4e:** Achieve 70% Accuracy\n",
    "\n",
    "Report the final test accuracy - Get it above 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2owV19JTQabO"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def augment_elementwise(input_vector):\n",
    "    # Create output vector\n",
    "    input_tensor = (\n",
    "        torch.tensor(input_vector, dtype=torch.float32)\n",
    "        if not isinstance(input_vector, torch.Tensor)\n",
    "        else input_vector.clone()\n",
    "    )\n",
    "    n = len(input_tensor)\n",
    "    data_out = torch.zeros_like(input_tensor)\n",
    "\n",
    "    k = torch.randint(0, n, (1,)).item()\n",
    "    data_out[k:] = input_tensor[: (n - k)]\n",
    "    data_out[:k] = input_tensor[(n - k) :]\n",
    "    scale = 0.95 + 0.1 * torch.rand(n)\n",
    "    return data_out * scale\n",
    "\n",
    "\n",
    "n_data_orig = data[\"x\"].shape[0]\n",
    "\n",
    "# We'll double the amount of data\n",
    "n_data_augment = int(n_data_orig * 1.5)\n",
    "augmented_x = torch.zeros((n_data_augment, D_i))\n",
    "augmented_y = torch.zeros(n_data_augment)\n",
    "\n",
    "# First n_data_orig rows are original data\n",
    "augmented_x[0:n_data_orig, :] = torch.tensor(data[\"x\"], dtype=torch.float32)\n",
    "augmented_y[0:n_data_orig] = torch.tensor(data[\"y\"], dtype=torch.float32)\n",
    "\n",
    "# Fill in rest of with augmented data\n",
    "for c_augment in range(n_data_orig, n_data_augment):\n",
    "    # Choose a data point randomly\n",
    "    random_data_index = torch.randint(0, n_data_orig - 1, (1,)).item()\n",
    "    # Augment the point and store\n",
    "    augmented_x[c_augment, :] = augment_elementwise(\n",
    "        torch.tensor(data[\"x\"][random_data_index, :], dtype=torch.float32)\n",
    "    )\n",
    "    augmented_y[c_augment] = data[\"y\"][random_data_index]\n",
    "\n",
    "# to use the train function we created above\n",
    "augmented_data = {\n",
    "    \"x\": augmented_x.numpy(),\n",
    "    \"y\": augmented_y.numpy(),\n",
    "    \"x_test\": data[\"x_test\"],\n",
    "    \"y_test\": data[\"y_test\"],\n",
    "}\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4e\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4e\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163910,
     "status": "ok",
     "timestamp": 1741743132268,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "9EgTod6RRD1F",
    "outputId": "6ca3ed68-36c6-4a8f-a5f4-1a4c88860352"
   },
   "outputs": [],
   "source": [
    "D_k = 500\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_i, D_k),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(D_k, D_k),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(D_k, D_k),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(D_k, D_o),\n",
    ")\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=augmented_data,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9,\n",
    "    decay=0.005,\n",
    "    schedule_params=(20, 0.8),\n",
    "    n_epoch=200,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1xKm2zHRKAw-pi7L7EhflRzOI9rq4l1t0",
     "timestamp": 1743176602017
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
