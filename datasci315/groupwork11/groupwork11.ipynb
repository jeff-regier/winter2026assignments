{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DATASCI 315, Group Work 11: LLM Few-Shot Learning and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this group work, we will learn about using open-source LLMs and adapting them to new tasks through few-shot learning and fine-tuning.\n",
    "\n",
    "**Important:** Select GPU as the runtime for this assignment.\n",
    "\n",
    "The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. During lab, feel free to flag down your GSI to ask questions at any point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Run the following cell to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers, datasets, sentencepiece, accelerate, evaluate are provided by the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Part 1: Few-Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "[Few-shot learning](https://huggingface.co/docs/transformers/tasks/prompting#few-shot-prompting) enables a pre-trained model to perform new tasks by providing a few examples directly in the prompt. Unlike fine-tuning, where model weights are updated through training on a dataset, few-shot learning relies on the model's ability to generalize from examples provided at inference time.\n",
    "\n",
    "In this example, we use [FLAN-T5](https://huggingface.co/google/flan-t5-base), an instruction-tuned version of the [T5 (Text-to-Text Transfer Transformer)](https://huggingface.co/docs/transformers/model_doc/t5) model. FLAN-T5 was fine-tuned on a large mixture of tasks described via instructions, making it particularly good at following prompts and few-shot learning. We use the `base` size (250M parameters) which balances capability with inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We set the model to evaluation mode since we are *not* training (updating the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Few-shot learning works by providing examples of the desired task in the prompt, then asking the model to perform the task on a new input. The following prompt demonstrates this for English-to-French translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "Translate English to French:\n",
    "\n",
    "Example 1:\n",
    "Input: I love apples.\n",
    "Output: J'aime les pommes.\n",
    "\n",
    "Example 2:\n",
    "Input: How are you?\n",
    "Output: Comment ça va?\n",
    "\n",
    "Example 3:\n",
    "Input: The weather is good today.\n",
    "Output: Le temps est agréable aujourd'hui.\n",
    "\n",
    "Now translate the following sentence:\n",
    "\n",
    "Input: {new_sentence}\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now we insert the sentence we want to translate into the prompt. The model uses the examples to understand the task and generate a translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"I would like to learn about transformers.\"\n",
    "formatted_prompt = few_shot_prompt.format(new_sentence=new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "To generate inputs in the correct format for the model, we need to tokenize the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(\n",
    "    formatted_prompt, return_tensors=\"pt\", max_length=512, truncation=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Now, let's generate the translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output using beam search\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=64,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=2.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We decode the generated tokens to get the final translation. The `skip_special_tokens=True` argument removes special tokens from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nTranslation:\", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1:** Design Your Own Few-Shot Learning Task\n",
    "\n",
    "Design and implement your own few-shot learning task. You can use the same model or try a different model from [HuggingFace](https://huggingface.co/models).\n",
    "\n",
    "**Requirements:**\n",
    "1. Choose a task different from translation (be creative!)\n",
    "2. Create a few-shot prompt with at least 3 examples\n",
    "3. Demonstrate that the model successfully performs your task on new inputs\n",
    "\n",
    "**Grading criteria:**\n",
    "- **Creativity**: How interesting and original is your chosen task?\n",
    "- **Success demonstration**: Does the model correctly perform the task on new examples?\n",
    "\n",
    "**In your solution, include:**\n",
    "- A description of the task you chose and why it's interesting\n",
    "- Your few-shot prompt\n",
    "- Test examples showing the model's outputs\n",
    "\n",
    "Feel free to experiment with the generation parameters (`max_length`, `num_beams`, `repetition_penalty`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Students design their own few-shot learning task\n",
    "# Example: sentiment analysis, text summarization, etc.\n",
    "\n",
    "# Define few-shot prompt with examples\n",
    "few_shot_prompt = \"\"\"\n",
    "Example few-shot prompt here.\n",
    "\"\"\"\n",
    "\n",
    "# Generate and evaluate results\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "import re\n",
    "\n",
    "# Verify that a few-shot prompt was created\n",
    "assert \"few_shot_prompt\" in dir(), \"few_shot_prompt should be defined\"\n",
    "assert isinstance(few_shot_prompt, str), \"few_shot_prompt should be a string\"\n",
    "assert len(few_shot_prompt) > 100, \"Prompt should be substantial (>100 chars for 3+ examples)\"\n",
    "\n",
    "# Check for example structure (at least 3 examples required)\n",
    "# Count patterns that indicate examples\n",
    "example_patterns = [\n",
    "    r\"example\\s*\\d\",  # \"Example 1\", \"example 2\", etc.\n",
    "    r\"#\\s*\\d\",  # \"# 1\", \"# 2\", etc.\n",
    "    r\"\\d\\s*[.):]\\s*\\w\",  # \"1. \", \"1) \", \"1: \" followed by text\n",
    "]\n",
    "prompt_lower = few_shot_prompt.lower()\n",
    "example_count = sum(len(re.findall(p, prompt_lower)) for p in example_patterns)\n",
    "\n",
    "# Also count input/output pairs as evidence of examples\n",
    "input_count = len(re.findall(r\"input\\s*:\", prompt_lower))\n",
    "output_count = len(re.findall(r\"output\\s*:\", prompt_lower))\n",
    "pair_count = min(input_count, output_count)\n",
    "\n",
    "# Use the max of explicit example markers or input/output pairs\n",
    "detected_examples = max(example_count, pair_count)\n",
    "assert detected_examples >= 3, (\n",
    "    f\"few_shot_prompt should contain at least 3 examples \"\n",
    "    f\"(detected {detected_examples}). Use numbered examples or Input:/Output: pairs.\"\n",
    ")\n",
    "\n",
    "# Verify it's not just the translation example from the walkthrough\n",
    "assert (\n",
    "    \"J'aime les pommes\" not in few_shot_prompt\n",
    "), \"Create your own task - do not reuse the translation example from the walkthrough\"\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify prompt has structure indicating few-shot learning\n",
    "assert len(few_shot_prompt) > 200, \"Prompt should be substantial for 3+ examples\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Part 2: Fine-Tuning for Grammar Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Unlike few-shot learning, [fine-tuning](https://huggingface.co/docs/transformers/training) updates the model's weights by training on a task-specific dataset. This allows even smaller models to achieve strong performance on specific tasks. In this problem, we fine-tune a T5 model to correct grammar mistakes in sentences.\n",
    "\n",
    "The training data consists of sentence pairs:\n",
    "- **Input**: A sentence with grammatical errors, prefixed with `grammar: `\n",
    "- **Output**: The corrected sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Download the data files from Canvas:\n",
    "- [`grammar_train.json`: training set](https://umich.instructure.com/files/40626314/download?download_frd=1)\n",
    "- [`grammar_val.json`: validation set](https://umich.instructure.com/files/40626315/download?download_frd=1)\n",
    "- [`grammar_test.json`: test set](https://umich.instructure.com/files/40626313/download?download_frd=1)\n",
    "\n",
    "You may add your own examples to the training and validation sets to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"./data/grammar_train.json\", \"validation\": \"./data/grammar_val.json\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Examine the first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Load the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files={\"test\": \"./data/grammar_test.json\"})[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "The following function tokenizes the data for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    input_text = example[\"input\"]  # already contains the \"grammar: \" prefix\n",
    "    target_text = example[\"output\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(target_text, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    # Replace pad token IDs in labels with -100 so they are ignored in the loss\n",
    "    model_inputs[\"labels\"] = [\n",
    "        token if token != tokenizer.pad_token_id else -100 for token in labels[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_val = dataset[\"validation\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "The following function computes evaluation metrics:\n",
    "- **exact_match**: Fraction of predictions that exactly match the reference sentence\n",
    "- **bleu**: The [BLEU score](https://en.wikipedia.org/wiki/BLEU), a common metric for evaluating text generation that measures similarity between generated and reference text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in labels with pad_token_id, then decode\n",
    "    labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(\n",
    "        predictions=decoded_preds, references=[[ref] for ref in decoded_labels]\n",
    "    )\n",
    "\n",
    "    # Compute exact match score\n",
    "    exact_matches = [\n",
    "        int(pred.strip() == ref.strip())\n",
    "        for pred, ref in zip(decoded_preds, decoded_labels, strict=True)\n",
    "    ]\n",
    "    exact_match_score = sum(exact_matches) / len(exact_matches)\n",
    "\n",
    "    return {\"bleu\": bleu_result[\"bleu\"], \"exact_match\": exact_match_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The following function generates predictions, similar to what we used for few-shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(example):\n",
    "    input_text = example[\"input\"]\n",
    "\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    ).to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=2.5,\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return {\"prediction\": output_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "For fine-tuning, we use [`t5-small`](https://huggingface.co/google-t5/t5-small) (60M parameters) instead of the larger FLAN-T5 model from Part 1. Since we're updating the model weights to specialize on grammar correction, we don't need the instruction-following capabilities of FLAN-T5. The smaller model trains faster and uses less GPU memory, making it practical for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "First, evaluate the model's BLEU score on the test set *before* fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "predicted_dataset = test_dataset.map(generate_prediction)\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions = predicted_dataset[\"prediction\"]\n",
    "references = test_dataset[\"output\"]\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu_metric = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_result = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "print(f\"Average BLEU score on the test set (before fine-tuning): {bleu_result['bleu']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "A perfect BLEU score is 1.0. Our goal is to achieve a score of 0.9 or higher after fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2: Fine-Tune the Model to Achieve BLEU >= 0.9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Adjust the training parameters below to achieve a BLEU score of 0.9 or higher on the test set.\n",
    "\n",
    "**Hint:** Consider adjusting `learning_rate`, `num_train_epochs`, `per_device_train_batch_size`, and `weight_decay`. You may also add more training examples to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./grammar_corrector\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "The `Seq2SeqTrainingArguments` and `Trainer` classes control model training. Adjust these parameters to achieve the required performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments - adjust these to achieve BLEU >= 0.9\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    report_to=[],  # Disable logging to WandB\n",
    "    learning_rate=5e-5,  # SOLUTION: 3e-5\n",
    "    per_device_train_batch_size=8,  # SOLUTION: 32\n",
    "    per_device_eval_batch_size=8,  # SOLUTION: 32\n",
    "    weight_decay=0.0,  # SOLUTION: 0.01\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=1,  # SOLUTION: 10\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify training_args is configured\n",
    "assert \"training_args\" in dir(), \"training_args should be defined\"\n",
    "assert training_args.num_train_epochs >= 1, \"Should train for at least 1 epoch\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify configuration is reasonable\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Evaluate Performance After Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Test the fine-tuned model on some example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentence(sentence):\n",
    "    # Add task prefix\n",
    "    input_text = \"grammar: \" + sentence\n",
    "\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    ).to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=2.5,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Optional: Load a previously saved fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your fine-tuned model and tokenizer\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"I eated the purple berries.\",\n",
    "    \"He go to school every day.\",\n",
    "    \"Thank you for picking me as your designer. I'd appreciate it.\",\n",
    "    \"The dog were meowing as a cat\",\n",
    "    \"She don't like to play football.\",\n",
    "    \"He is more smarter than his brother.\",\n",
    "    \"I seen the movie yesterday.\",\n",
    "    \"They was going to the store.\",\n",
    "    \"I am not speak  English well\",\n",
    "    \"The mentioned changes have done.\",\n",
    "    \"I'd be more than happy to work with you in another project.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in test_sentences:\n",
    "    corrected = correct_sentence(sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Corrected: {corrected}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "Evaluate on the test set. The goal is a BLEU score of 0.9 or higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_dataset = test_dataset.map(generate_prediction)\n",
    "predictions = predicted_dataset[\"prediction\"]\n",
    "references = test_dataset[\"output\"]\n",
    "bleu_metric = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_result = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "print(f\"Average BLEU score on the test set: {bleu_result['bleu']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "Examine some predictions from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = torch.randperm(len(predicted_dataset))[:5].tolist()\n",
    "for idx in sample_indices:\n",
    "    print(f\"Input: {predicted_dataset[idx]['input']}\")\n",
    "    print(f\"Prediction: {predicted_dataset[idx]['prediction']}\")\n",
    "    if \"output\" in predicted_dataset[idx]:\n",
    "        print(f\"Reference: {predicted_dataset[idx]['output']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 3: Design Your Own Fine-Tuning Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "Design and implement your own fine-tuning task. Use the code from Problem 2 as a starting point.\n",
    "\n",
    "**Requirements:**\n",
    "1. Choose a task different from grammar correction and your few-shot task\n",
    "2. Create training, validation, and test datasets in the same format as Problem 2\n",
    "3. Demonstrate performance improvement after fine-tuning using an appropriate metric\n",
    "\n",
    "**Grading criteria:**\n",
    "- **Creativity**: How interesting is your chosen task? Bonus points for creative tasks!\n",
    "- **Demonstrated improvement**: Does the model perform better after fine-tuning?\n",
    "\n",
    "**Data generation options:**\n",
    "- Generate examples by hand\n",
    "- Generate programmatically (include your generation code)\n",
    "- Use an LLM to help generate examples\n",
    "\n",
    "**In your solution, include:**\n",
    "- A description of your task\n",
    "- How you generated the data (include code if applicable)\n",
    "- Model performance before and after fine-tuning\n",
    "- Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Students design their own fine-tuning task\n",
    "# Example implementation here\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Verify that fine-tuning task was implemented\n",
    "# Note: This problem is open-ended, so we check for reasonable implementation\n",
    "assert True, \"Fine-tuning task should be implemented\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify implementation has required components\n",
    "assert True, \"Verify implementation\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
