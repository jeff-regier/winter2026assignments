{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4U8hhGomCCm"
   },
   "source": [
    "# DATASCI 315, Group Work 4: PyTorch Training Pipeline and FashionMNIST\n",
    "\n",
    "In this group-work assignment, we will start working with PyTorch. The learning objectives for this assignment include gaining familiarity with deep networks in PyTorch.\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. *During lab, feel free to flag down your GSI to ask questions at any point!*\n",
    "\n",
    "**Resources:**\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3wu4DfGm4P7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nrILX5Ydm4C"
   },
   "source": [
    "## Deep Networks for Regression\n",
    "\n",
    "We go over the basic steps to build a (moderately) deep network for a regression problem and train it using PyTorch's autograd. Things to learn in this section:\n",
    "\n",
    "1. Creating a `DataLoader` object (to help with training)\n",
    "2. How to use `torch.nn` layers (linear and activation) and build a sequential model\n",
    "3. How to compute loss and optimize using `torch.optim`\n",
    "\n",
    "Let us first look at a regression problem with synthetic data where the goal is to train a 3-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2005,
     "status": "ok",
     "timestamp": 1739482980266,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "EDofXj8cYpFE",
    "outputId": "45b29cb6-5702-4fd7-a8bc-1b1b25eae1af"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_samples, num_features = 1000, 3\n",
    "torch.manual_seed(10)\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (\n",
    "    2 * torch.sin(3 * X[:, 0])\n",
    "    - 3 * torch.cos(-4 * X[:, 1])\n",
    "    + 2.4 * X[:, 2]\n",
    "    + torch.randn(num_samples)\n",
    ")\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS3gGw6qVw34"
   },
   "source": [
    "### Using DataLoader\n",
    "\n",
    "The `DataLoader` class can be imported from `torch.utils.data` and is a convenient way to create batches while keeping $(X, y)$ pairs together. `DataLoader` takes a `torch` dataset object (which can be created from raw data using `TensorDataset`) or a built-in `torch` dataset (we will see an example later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1739482980266,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "e21xLSFLejGE",
    "outputId": "cca1fc96-c61d-40e4-c60e-e734dc9c60cb"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch_idx, (features, targets) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}: features shape: {features.shape}, targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxZrYb9Mfwwy"
   },
   "source": [
    "Note there are $B=25$ batches now, each of size `batch_size` ($M=32$). In each batch, $X$ has shape $(32, 3)$ where the first dimension corresponds to the number of samples in the batch and the second corresponds to the number of features. The batch also contains the corresponding $y$ values. To see the number of samples in the entire dataset, use the `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739482980266,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "LV0_YUxtQsHS",
    "outputId": "e90cb2a0-b3c3-42f9-a360-2ad3ecbae0b8"
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "we_kThkAgJvT"
   },
   "source": [
    "### Building a Sequential Model\n",
    "\n",
    "Building a sequential model (where the output from one layer is passed as the input to the next layer) is done using `torch.nn.Sequential`.\n",
    "\n",
    "For fully connected layers, you will need:\n",
    "1. Linear layers via `torch.nn.Linear`\n",
    "2. Activation functions from `torch.nn`\n",
    "\n",
    "The `Linear` layer is similar to the class `Linear` we saw in Group Work 2, except that it (i) allows for more general shaped tensors and (ii) parameters associated with this layer enable automatic gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739482980267,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "TaZrjYhOgxp4",
    "outputId": "c7cb67e1-6ae5-4614-b4ca-f57694e4632d"
   },
   "outputs": [],
   "source": [
    "# A single linear layer\n",
    "print(f\"num_features = {num_features}\")\n",
    "layer1 = nn.Linear(in_features=num_features, out_features=10)\n",
    "\n",
    "# Passing the full data through the layer\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "yhat = layer1(X)\n",
    "print(f\"Shape of output of X through this layer: {yhat.shape}\")\n",
    "\n",
    "# Passing a batch through the layer\n",
    "for batch_idx, (features, _targets) in enumerate(dataloader):\n",
    "    yhat = layer1(features)\n",
    "    print(f\"Batch {batch_idx}: shape of output through this layer: {yhat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Vj1FH5iv2_"
   },
   "source": [
    "Such layers can be combined as follows. Suppose we want to design a neural network with 2 hidden layers:\n",
    "\n",
    "$$x \\rightarrow z_1 \\rightarrow z_2 \\rightarrow y$$\n",
    "\n",
    "where (denoting the activation function as $a$ acting coordinate-wise)\n",
    "\n",
    "\\begin{align}\n",
    "  x &\\in \\mathbb{R}^d \\\\\n",
    "  z_1 &= a(b_1 + W_1 x) \\in \\mathbb{R}^{d_1} \\\\\n",
    "  z_2 &= a(b_2 + W_2 z_1) \\in \\mathbb{R}^{d_2} \\\\\n",
    "  y &= b_3 + W_3 z_2 \\in \\mathbb{R}.\n",
    "\\end{align}\n",
    "\n",
    "That is, the first hidden layer has $d_1$ nodes and the second has $d_2$. $W_1, b_1$ are the weights and bias for the first layer, and similarly for the other parameters. Here is how to build this model. Recall the ReLU activation can be used from `torch.nn.ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739482980267,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "bGMwK7GMg87K",
    "outputId": "0ef08bdd-6adc-4e1b-c6c6-cac5a5f2013f"
   },
   "outputs": [],
   "source": [
    "hidden_dim_1 = 10\n",
    "hidden_dim_2 = 5\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(num_features, hidden_dim_1),  # input has d features, output has d_1\n",
    "    nn.ReLU(),  # activation function\n",
    "    nn.Linear(hidden_dim_1, hidden_dim_2),  # input has d_1 features, output has d_2\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim_2, 1),  # input has d_2 features, output has 1 (as y is scalar)\n",
    ")\n",
    "\n",
    "# Let us look at the trainable parameters\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHFzSUf2kjP9"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. The first two are $W_1$ and $b_1$ with shapes $(d_1, d)$ and $(d_1,)$ respectively, associated with the first layer.\n",
    "2. The next two are $W_2$ and $b_2$, and so on.\n",
    "\n",
    "Note that they all have `requires_grad` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739482980267,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "FiYUFkmswSar",
    "outputId": "642cd6cb-8bbe-4639-dddd-524bdeec1ae8"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QU36UUek8jE"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1a:** Shapes of $W_2$ and $b_2$\n",
    "\n",
    "What is the shape of $W_2$ and $b_2$? Store these as tuples in `w2_shape` and `b2_shape`.\n",
    "\n",
    "**Hint:** Recall that for a linear layer mapping from $d_1$ inputs to $d_2$ outputs, the weight matrix $W$ has shape $(d_2, d_1)$ and the bias vector $b$ has shape $(d_2,)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5zHoQ_WlJ1z"
   },
   "outputs": [],
   "source": [
    "# W_2 has shape (d_2, d_1) = (5, 10) and b_2 has shape (d_2,) = (5,).\n",
    "# BEGIN SOLUTION\n",
    "w2_shape = (hidden_dim_2, hidden_dim_1)\n",
    "b2_shape = (hidden_dim_2,)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert w2_shape == (5, 10), f\"Expected W_2 shape (5, 10), got {w2_shape}\"\n",
    "assert b2_shape == (5,), f\"Expected b_2 shape (5,), got {b2_shape}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert w2_shape[0] == hidden_dim_2, \"W_2 rows should equal hidden_dim_2\"\n",
    "assert w2_shape[1] == hidden_dim_1, \"W_2 cols should equal hidden_dim_1\"\n",
    "assert len(b2_shape) == 1, \"b_2 should be 1-dimensional\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1b:** Total Parameters\n",
    "\n",
    "What is the total number of parameters in this model? Store your answer in `total_params`.\n",
    "\n",
    "**Hint:** Each linear layer contributes $(\\text{input dim}) \\times (\\text{output dim})$ weight parameters plus $(\\text{output dim})$ bias parameters. You can also compute this programmatically using `parameter.numel()` for each parameter in `model.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total parameters:\n",
    "#   Layer 1: d * d_1 + d_1 = 3 * 10 + 10 = 40\n",
    "#   Layer 2: d_1 * d_2 + d_2 = 10 * 5 + 5 = 55\n",
    "#   Layer 3: d_2 * 1 + 1 = 5 + 1 = 6\n",
    "#   Total = 40 + 55 + 6 = 101\n",
    "# BEGIN SOLUTION\n",
    "total_params = sum(parameter.numel() for parameter in model.parameters())\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert total_params == 101, f\"Expected 101 total parameters, got {total_params}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify by manual calculation\n",
    "layer1_params = num_features * hidden_dim_1 + hidden_dim_1\n",
    "layer2_params = hidden_dim_1 * hidden_dim_2 + hidden_dim_2\n",
    "layer3_params = hidden_dim_2 * 1 + 1\n",
    "expected_params = layer1_params + layer2_params + layer3_params\n",
    "assert total_params == expected_params, f\"Mismatch: {total_params} vs {expected_params}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH9eNrnnl2YZ"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "The training loop follows a standard pattern: for each batch, we perform a forward pass, compute the loss, backpropagate gradients, and update parameters. Let us see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eL2_QFAXmADo"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 200\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for _epoch in range(num_epochs):\n",
    "    for inputs, target in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Compute the loss for train and test\n",
    "    with torch.no_grad():\n",
    "        y_hat_train = model(X_train)\n",
    "        train_loss.append(loss_fn(y_hat_train, y_train).detach().item())\n",
    "        y_hat_test = model(X_test)\n",
    "        test_loss.append(loss_fn(y_hat_test, y_test).detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739482986726,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "1muREH-krH0y",
    "outputId": "d42f79fa-7a97-4f10-eb3a-c8bcbadfcbe9"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(test_loss, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wig_PVu4bEi"
   },
   "source": [
    "**Note:** One can keep track of the loss computed within each batch as well, but typically these are updated at every batch within an epoch. One must be careful when computing the training loss, depending on what is needed. The method shown here computes the loss on the whole data at the end of each epoch (this might be very costly with large datasets), and hence a better way is to use the batch losses instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePVNoUw8eTNa"
   },
   "source": [
    "## Deep Networks for Classification\n",
    "\n",
    "This section will walk you through training a classification model with one of PyTorch's built-in datasets. We will be using the FashionMNIST dataset, which contains $28 \\times 28$ grayscale images of clothing articles, each with a label (type of clothing). Let us download the dataset and visualize some samples. Note the `ToTensor` transformation applied to each sample, which converts it to a PyTorch tensor object. One could also apply other transforms directly (like `Resize`, `Normalize`, `CenterCrop`, etc.), which can be composed via `torchvision.transforms.Compose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16974,
     "status": "ok",
     "timestamp": 1739490427837,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "KoYY7snja9gq",
    "outputId": "140343be-c2a4-493d-baee-591645e7934b"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1739482989211,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "J2XfcK2gRUlF",
    "outputId": "a49016d9-2432-44a4-942c-04e83b567ba5"
   },
   "outputs": [],
   "source": [
    "num_train = len(training_data)\n",
    "num_test = len(test_data)\n",
    "print(f\"Number of training samples: {num_train}\")\n",
    "print(f\"Number of test samples: {num_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1739482989780,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "K2gUK3BMyrq6",
    "outputId": "35a72327-ce74-4e13-91a8-ff5fbec4dce3"
   },
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYDNr7e7y1EK"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2a:** DataLoaders\n",
    "\n",
    "Create a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) object with the training data, using a batch size of 64. Also create one for the test data. Shuffle the training data but not the test data. Store the DataLoaders in `train_dataloader` and `test_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3sITix0yyI-"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739482989781,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "Ivw3otgEzUWh",
    "outputId": "08c85159-eff6-4cce-dbff-1cf546cfa511"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "num_batches_train = len(train_dataloader)\n",
    "num_batches_test = len(test_dataloader)\n",
    "assert num_batches_train == 938, f\"Expected 938 train batches, got {num_batches_train}\"\n",
    "assert num_batches_test == 157, f\"Expected 157 test batches, got {num_batches_test}\"\n",
    "\n",
    "# Check that dataloader can be iterated over\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "assert train_features.size() == torch.Size(\n",
    "    [64, 1, 28, 28]\n",
    "), f\"Expected feature shape [64, 1, 28, 28], got {train_features.size()}\"\n",
    "assert train_labels.size() == torch.Size(\n",
    "    [64]\n",
    "), f\"Expected label shape [64], got {train_labels.size()}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# Display an image and label for verification\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label} corresponding to {labels_map[label.item()]}\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert train_dataloader.batch_size == 64, \"Train dataloader batch size should be 64\"\n",
    "assert test_dataloader.batch_size == 64, \"Test dataloader batch size should be 64\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSqXTfnTztO3"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2b:** Flattening\n",
    "\n",
    "Before using a sequential model, we need to reshape the features properly. Currently each batch $X$ has shape $(M, 1, 28, 28)$, where $M=64$. We need to flatten each sample to obtain $X$ of shape $(M, 784)$. Use [`nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) (which can be thought of as another layer) to do this. For this problem, initialize this layer as `flatten_layer` and pass the features from a batch through it, storing the result in `train_flat_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSg1G36lzaBR"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "flatten_layer = nn.Flatten()\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_flat_features = flatten_layer(train_features)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1739482990485,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "biXgoQE1Ry8j",
    "outputId": "89b46185-276b-47ae-9402-8ebd86af710d"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert train_features.shape == torch.Size(\n",
    "    [64, 1, 28, 28]\n",
    "), f\"Expected input shape [64, 1, 28, 28], got {train_features.shape}\"\n",
    "assert train_flat_features.shape == torch.Size(\n",
    "    [64, 784]\n",
    "), f\"Expected output shape [64, 784], got {train_flat_features.shape}\"\n",
    "print(f\"Shape of input: {train_features.shape}\")\n",
    "print(f\"Shape of output: {train_flat_features.shape}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert train_flat_features.shape[0] == 64, \"Batch dimension should be preserved\"\n",
    "assert train_flat_features.shape[1] == 28 * 28, \"Features should be flattened to 784\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qFPkNfn1GIK"
   },
   "source": [
    "You can use this as the first layer while building the sequential model. Note that you can think of this as another layer with no trainable parameters (like an activation layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyEDHsaF1Rg2"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2c:** A Sequential Model\n",
    "\n",
    "Write a sequential model with 2 hidden layers, each with 512 nodes. Store your model in `classification_model`.\n",
    "\n",
    "**Notes:**\n",
    "1. The output should have 10 values (there are 10 target classes).\n",
    "2. Make sure you flatten images first using [`nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html).\n",
    "3. Use [`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) activation after each hidden layer.\n",
    "\n",
    "Think about how many total parameters this model has. The test cell will verify that your architecture matches the expected parameter count of 669,706.\n",
    "\n",
    "**Note:** Since we will be using [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), which internally applies log-softmax before computing the loss, we do not need a separate `softmax` operation on the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etFPZk4w0krL"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Architecture: 784 -> 512 -> 512 -> 10\n",
    "# Total params: (784*512 + 512) + (512*512 + 512) + (512*10 + 10) = 669,706\n",
    "classification_model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0721jwq2Y7s"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "with torch.no_grad():\n",
    "    train_features, train_labels = next(iter(train_dataloader))\n",
    "    predicted_logits = classification_model(train_features)\n",
    "    assert predicted_logits.shape == torch.Size(\n",
    "        [64, 10]\n",
    "    ), f\"Expected output shape [64, 10], got {predicted_logits.shape}\"\n",
    "    print(f\"Shape of predicted_logits: {predicted_logits.shape}\")\n",
    "\n",
    "model_num_params = sum(parameter.numel() for parameter in classification_model.parameters())\n",
    "assert model_num_params == 669706, f\"Expected 669,706 parameters, got {model_num_params}\"\n",
    "print(f\"Total number of parameters: {model_num_params}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test that model handles different batch sizes correctly\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(16, 1, 28, 28)\n",
    "    test_output = classification_model(test_input)\n",
    "    assert test_output.shape == torch.Size([16, 10]), \"Model should work with different batch sizes\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wth-WxhS2z-E"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2d:** Loss\n",
    "\n",
    "Initialize the appropriate loss function ([`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) as `classification_loss_fn` and compute the loss based on `predicted_logits` from above and `train_labels` from the same batch. Store the result in `initial_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739482990485,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "C04p9xoL2f_4",
    "outputId": "d02443d7-ce18-4b9c-dbfb-3c35e9b27823"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "with torch.no_grad():\n",
    "    initial_loss = classification_loss_fn(predicted_logits, train_labels)\n",
    "print(f\"Initial loss: {initial_loss.item():.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert initial_loss.item() > 0, \"Loss should be positive\"\n",
    "assert initial_loss.item() < 10, \"Initial loss should be reasonable (< 10)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert initial_loss.shape == torch.Size([]), \"Loss should be a scalar\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDILtowS3Z9e"
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 2e:** Optimizer\n",
    "\n",
    "Initialize an [`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer from `torch.optim` with learning rate $\\eta = 0.001$. Store the optimizer in `classification_optimizer`.\n",
    "\n",
    "**Hint:** Look at the regression example above to see how the optimizer is initialized with the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150311,
     "status": "ok",
     "timestamp": 1739483140792,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "liwcsxMe3VM2",
    "outputId": "bf3d1822-67b4-4567-fcda-6410a38bb0c2"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "classification_optimizer = optim.SGD(classification_model.parameters(), lr=1e-3)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jlg29xJtTwmc"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(classification_optimizer, optim.SGD), \"Optimizer should be SGD\"\n",
    "expected_lr = 1e-3\n",
    "actual_lr = classification_optimizer.defaults[\"lr\"]\n",
    "assert actual_lr == expected_lr, f\"Learning rate should be 0.001, got {actual_lr}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert len(classification_optimizer.param_groups) == 1, \"Should have one param group\"\n",
    "assert len(classification_optimizer.param_groups[0][\"params\"]) > 0, \"Should have parameters\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2f:** A Single Training Step\n",
    "\n",
    "Implement a function `train_one_batch` that performs one gradient descent step on a single batch. The function should:\n",
    "\n",
    "1. Compute the model's predictions (forward pass)\n",
    "2. Compute the loss using `classification_loss_fn`\n",
    "3. Compute gradients (backward pass)\n",
    "4. Update parameters using the optimizer\n",
    "5. Zero the gradients for the next iteration\n",
    "6. Return the loss value (as a Python float, not a tensor)\n",
    "\n",
    "**Hint:** Look at the training loop in the regression example above. The key steps are: `loss.backward()`, `optimizer.step()`, and `optimizer.zero_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def train_one_batch(features, labels):\n",
    "    \"\"\"Perform one training step on a batch and return the loss.\"\"\"\n",
    "    # Forward pass\n",
    "    outputs = classification_model(features)\n",
    "    loss = classification_loss_fn(outputs, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    classification_optimizer.step()\n",
    "\n",
    "    # Zero gradients for next iteration\n",
    "    classification_optimizer.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "# Get a batch and test the function\n",
    "test_features, test_labels = next(iter(train_dataloader))\n",
    "batch_loss = train_one_batch(test_features, test_labels)\n",
    "assert isinstance(batch_loss, float), f\"Expected float, got {type(batch_loss)}\"\n",
    "assert batch_loss > 0, \"Loss should be positive\"\n",
    "print(f\"Batch loss: {batch_loss:.4f}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Verify that training actually updates the model\n",
    "initial_param = next(iter(classification_model.parameters())).clone()\n",
    "_ = train_one_batch(test_features, test_labels)\n",
    "updated_param = next(iter(classification_model.parameters()))\n",
    "assert not torch.allclose(initial_param, updated_param), \"Params should change after training\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2g:** Evaluating on Test Data\n",
    "\n",
    "Implement a function `evaluate_test` that computes the average loss and accuracy on the test set. The function should:\n",
    "\n",
    "1. Loop through all batches in `test_dataloader`\n",
    "2. For each batch, compute the model's predictions and the loss\n",
    "3. Count the number of correct predictions (the predicted class is `outputs.argmax(dim=1)`)\n",
    "4. Return a tuple of (average_loss, accuracy)\n",
    "\n",
    "**Important:** Use `torch.no_grad()` to disable gradient computation during evaluation, since we don't need gradients and this saves memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def evaluate_test():\n",
    "    \"\"\"Compute average loss and accuracy on the test set.\"\"\"\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_dataloader:\n",
    "            outputs = classification_model(batch_features)\n",
    "            loss = classification_loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            num_correct += (outputs.argmax(dim=1) == batch_labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches_test\n",
    "    accuracy = num_correct / num_test\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_loss, test_acc = evaluate_test()\n",
    "assert isinstance(test_loss, float), f\"Expected loss to be float, got {type(test_loss)}\"\n",
    "assert isinstance(test_acc, float), f\"Expected accuracy to be float, got {type(test_acc)}\"\n",
    "assert test_loss > 0, \"Loss should be positive\"\n",
    "assert 0 <= test_acc <= 1, f\"Accuracy should be between 0 and 1, got {test_acc}\"\n",
    "print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert test_acc > 0.05, \"Accuracy should be better than random guessing after some training\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2h:** Full Training Loop\n",
    "\n",
    "Now put it all together! Train the model for 10 epochs using the functions you implemented above. For each epoch:\n",
    "\n",
    "1. Loop through all batches in `train_dataloader` and call `train_one_batch`\n",
    "2. Track the average training loss for the epoch\n",
    "3. Call `evaluate_test` to get the test loss and accuracy\n",
    "\n",
    "Store your results in:\n",
    "- `classification_train_loss`: list of average training loss per epoch\n",
    "- `classification_test_loss`: list of average test loss per epoch\n",
    "- `classification_test_accuracy`: list of test accuracy per epoch\n",
    "\n",
    "Print the progress after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "num_classification_epochs = 10\n",
    "classification_train_loss = []\n",
    "classification_test_loss = []\n",
    "classification_test_accuracy = []\n",
    "\n",
    "for epoch in range(num_classification_epochs):\n",
    "    # Training: loop through all batches\n",
    "    epoch_train_loss = 0\n",
    "    for batch_features, batch_labels in train_dataloader:\n",
    "        batch_loss = train_one_batch(batch_features, batch_labels)\n",
    "        epoch_train_loss += batch_loss\n",
    "\n",
    "    # Record average training loss\n",
    "    classification_train_loss.append(epoch_train_loss / num_batches_train)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc = evaluate_test()\n",
    "    classification_test_loss.append(test_loss)\n",
    "    classification_test_accuracy.append(test_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}: \"\n",
    "        f\"train loss = {classification_train_loss[-1]:.2f}, \"\n",
    "        f\"test loss = {classification_test_loss[-1]:.2f}, \"\n",
    "        f\"test accuracy = {classification_test_accuracy[-1]:.2f}\"\n",
    "    )\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert (\n",
    "    len(classification_train_loss) == 10\n",
    "), f\"Expected 10 training loss values, got {len(classification_train_loss)}\"\n",
    "assert (\n",
    "    len(classification_test_loss) == 10\n",
    "), f\"Expected 10 test loss values, got {len(classification_test_loss)}\"\n",
    "assert (\n",
    "    len(classification_test_accuracy) == 10\n",
    "), f\"Expected 10 test accuracy values, got {len(classification_test_accuracy)}\"\n",
    "assert (\n",
    "    classification_test_accuracy[-1] > 0.5\n",
    "), f\"Expected final accuracy > 0.5, got {classification_test_accuracy[-1]:.2f}\"\n",
    "assert classification_train_loss[-1] < classification_train_loss[0], \"Training loss should decrease\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert all(\n",
    "    0 <= acc <= 1 for acc in classification_test_accuracy\n",
    "), \"Accuracy should be between 0 and 1\"\n",
    "assert all(loss > 0 for loss in classification_train_loss), \"Loss should be positive\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(classification_train_loss, label=\"train loss\")\n",
    "axes[0].plot(classification_test_loss, label=\"test loss\")\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "axes[0].set_ylabel(\"loss\")\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"Loss over epochs\")\n",
    "\n",
    "axes[1].plot(classification_test_accuracy)\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "axes[1].set_ylabel(\"accuracy\")\n",
    "axes[1].set_title(\"Test accuracy over epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
