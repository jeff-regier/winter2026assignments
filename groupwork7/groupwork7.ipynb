{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQdBxjSzfPi"
   },
   "source": [
    "# **DATASCI 315, Group Work Assignment 7: High-Dimensional Spaces, the Bias-Variance Trade-Off, Ensemble Methods, and Data Augmentation**\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. *During lab, feel free to flag down your GSI to ask questions at any point!* Upon completion, one member of the team should submit their team's work through Canvas as html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1zS2I9-tnAR"
   },
   "source": [
    "To begin, let's import some packages that we'll use throughout this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqv6wFC-zrrv"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-bright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnFp7yDLz-f0"
   },
   "source": [
    "# Part A: High-Dimensional Space\n",
    "\n",
    "This part investigates the strange properties of high-dimensional spaces. We consider the following two properties of high-dimensional spaces:\n",
    "\n",
    "1. The closeness of random points in a high-dimensional space\n",
    "2. The proportion of a bounding hypercube contained within a hypersphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BjDnxhMzgJc"
   },
   "source": [
    "### Problem 1a: Distance in High-Dimensional Space\n",
    "\n",
    "Given $n$ random points (from a standard multivariate normal distribution) compute the average norm, the minimum, maximum pairwise distance and the ratio. Complete the following function, which returns the average norm and the ratio.\n",
    "\n",
    "Note: While computing minimum, ignore the self distances which are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRCFRNnFzmXQ"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def distance(n_dim=1, n_data=1000):\n",
    "    data_points = torch.randn(n_data, n_dim)\n",
    "\n",
    "    # compute the average norm\n",
    "    avg_norm = torch.linalg.norm(data_points, dim=1).mean().item()\n",
    "\n",
    "    # compute pairwise distances\n",
    "    reshaped_data = data_points.reshape(n_data, 1, n_dim)\n",
    "    diff = reshaped_data - reshaped_data.transpose(0, 1)\n",
    "    pairwise_distance = torch.sqrt(torch.sum(diff**2, dim=2))\n",
    "\n",
    "    # compute maximum and minimum (be careful)\n",
    "    maximum_distance = pairwise_distance.max().item()\n",
    "    pairwise_distance.fill_diagonal_(float(\"inf\"))\n",
    "    minimum_distance = pairwise_distance.min().item()\n",
    "\n",
    "    # compute ratio\n",
    "    ratio = maximum_distance / minimum_distance\n",
    "    return avg_norm, ratio\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 1a\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 1a\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 7994,
     "status": "ok",
     "timestamp": 1741741798731,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "SYvqHuWcz1cH",
    "outputId": "9138ee7e-df5d-4948-fb92-ac063e3c0097"
   },
   "outputs": [],
   "source": [
    "dim = torch.arange(5, 500, 100)\n",
    "\n",
    "# Run distance function for each dimension\n",
    "norms = []\n",
    "ratios = []\n",
    "for d in dim:\n",
    "    n, r = distance(n_dim=d.item())\n",
    "    norms.append(n)\n",
    "    ratios.append(r)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "ax[0].plot(dim, norms)\n",
    "ax[0].set_xlabel(\"dimension\")\n",
    "ax[0].set_ylabel(\"average norm\")\n",
    "\n",
    "ax[1].plot(dim, ratios)\n",
    "ax[1].set_xlabel(\"dimension\")\n",
    "ax[1].set_ylabel(\"ratio of max/min distance\")\n",
    "\n",
    "ax[2].plot(dim[3:], ratios[3:])\n",
    "ax[2].set_xlabel(\"dimension\")\n",
    "ax[2].set_ylabel(\"ratio of max/min distance\")\n",
    "ax[2].set_ylim(0, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dENe4HFazllv"
   },
   "source": [
    "### Problem 1b: Hypersphere in Bounding Hypercube\n",
    "\n",
    "Consider the hypersphere $B(0,r)=\\{x\\in R^d: \\Vert x\\Vert \\leq r\\}$ of radius $r$ - this is a generalization of the disk (in 2-d) and sphere (in 3-d). Let $B=B(0,1)$ be the standard hypersphere (unit Euclidean ball).\n",
    "\n",
    " This hypersphere $B$ is a subset of the hypercube $H=\\{x\\in R^d: -1 \\leq x_i \\leq 1 \\text{ for all } i=1,\\dots,d\\}$ - this is a generalization of a square (in 2-d) and cube (in 3-d). See visualization in 2-d for reference. $H$ is the smallest possible cube that contains the ball $B$. We are interested in how much of the volume of $H$ is taken up by $B$.\n",
    "\n",
    " The volume of the hypercube is\n",
    "\n",
    "$$V_d(H) = 2^d.$$\n",
    "\n",
    "The volume of the hypersphere is given by\n",
    "\n",
    "$$V_d(B) = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)}$$\n",
    "\n",
    "where $\\Gamma$ is the Gamma function (use `math.gamma` for scalar computations)\n",
    "\n",
    "(i) Complete the function, which takes `n_dim` as input and returns the ratio (volume of the hypersphere $B$) divided by (volume of hypercube $H$) of that dimension.\n",
    "\n",
    "(ii) Plot the ratio for dimensions from 2 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1741741813575,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "uaa6iyCCzlX7",
    "outputId": "76c8d5ee-06ea-4f4b-9fdc-4b8a29248857"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def volume(n_dim):\n",
    "    # compute hypercube volume\n",
    "    cube_volume = 2**n_dim\n",
    "\n",
    "    # compute sphere volume (use math.gamma for the Gamma function)\n",
    "    sphere_volume = math.pi ** (n_dim / 2) / math.gamma(n_dim / 2 + 1)\n",
    "\n",
    "    # ratio\n",
    "    return sphere_volume / cube_volume\n",
    "\n",
    "\n",
    "volume(2)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 1b\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 1b\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koOlCOze0KPu"
   },
   "source": [
    "In 2-d, the volume of the hypercube (unit square) is 4 and that of the hypersphere (unit disk) is $\\pi\\approx 3.14159$, hence the ratio is about $0.76853982$. Here is a visualization. We are interested in the volume in the square outside the disk (i.e., the red part shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1741741817417,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "AbUcYcms0Knj",
    "outputId": "41f7ce3d-b8a9-4702-b8bd-ec8560a84f5f"
   },
   "outputs": [],
   "source": [
    "circle = plt.Circle((0, 0), 1, fill=True, color=\"blue\", label=\"Unit Circle\", alpha=0.5)\n",
    "\n",
    "# Unit square\n",
    "square = plt.Rectangle((-1, -1), 2, 2, fill=True, color=\"red\", label=\"Unit Square\", alpha=0.5)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.add_artist(square)\n",
    "ax.add_artist(circle)\n",
    "\n",
    "# Set plot limits and aspect ratio\n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Unit Disk and Bounding Unit Square\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjeJWHg10dh8"
   },
   "source": [
    "Write your code for plotting the ratio of volume across dimensions. If correctly implemented, you should note how the ratio basically gets to 0 - the sphere hardly takes up any space within the cube. Most of the volume within the cube is at the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1741741818988,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "Kanqtwqj0ZuZ",
    "outputId": "38065001-c6ba-4c36-f7c1-97d80c078b2e"
   },
   "outputs": [],
   "source": [
    "dim = list(range(2, 20))\n",
    "\n",
    "ratios = [volume(d) for d in dim]\n",
    "plt.plot(dim, ratios)\n",
    "plt.xlabel(\"dimension\")\n",
    "plt.ylabel(\"ratio of volume of sphere/volume of cube\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmU-yz9u06u9"
   },
   "source": [
    "# Part B: A Regression Problem to Illustrate the Bias-Variance Trade-off and Ensemble Methods\n",
    "\n",
    "In this section, we go over (i) bias-variance trade-off and (ii) ensemble methods using a simple regression problem. Here are some helper codes and plot for the function (domain is $[0,1]$). This follows Section 8.2-8.3 in textbook, using a single layer network (no activation) where the least squares solution has a closed-form expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1741741821333,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "yhxQLbL01Pwx",
    "outputId": "a630ede6-52c3-48df-bec4-1dafb59e941e"
   },
   "outputs": [],
   "source": [
    "# The true function that we are trying to estimate, defined on [0,1]\n",
    "def true_function(x):\n",
    "    return torch.exp(torch.sin(x * (2 * 3.1413)))\n",
    "\n",
    "\n",
    "# Generate some data points with or without noise\n",
    "def generate_data(n_data, sigma_y=0.3):\n",
    "    # Generate x values quasi uniformly\n",
    "    x = torch.zeros(n_data)\n",
    "    for i in range(n_data):\n",
    "        x[i] = torch.rand(1).item() * (1 / n_data) + i / n_data\n",
    "\n",
    "    # y value from running through function and adding noise\n",
    "    y = true_function(x) + sigma_y * torch.randn(n_data)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Draw the fitted function, together with uncertainty used to generate points\n",
    "def plot_function(\n",
    "    x_func,\n",
    "    y_func,\n",
    "    x_data=None,\n",
    "    y_data=None,\n",
    "    x_model=None,\n",
    "    y_model=None,\n",
    "    sigma_func=None,\n",
    "    sigma_model=None,\n",
    "    ax=None,\n",
    "):\n",
    "    if ax is None:\n",
    "        _fig, ax = plt.subplots()\n",
    "    ax.plot(x_func, y_func, \"k-\")\n",
    "    if sigma_func is not None:\n",
    "        ax.fill_between(x_func, y_func - 2 * sigma_func, y_func + 2 * sigma_func, color=\"lightgray\")\n",
    "\n",
    "    if x_data is not None:\n",
    "        ax.plot(x_data, y_data, \"o\", color=\"#d18362\")\n",
    "\n",
    "    if x_model is not None:\n",
    "        ax.plot(x_model, y_model, \"-\", color=\"#7fe7de\")\n",
    "\n",
    "    if sigma_model is not None:\n",
    "        ax.fill_between(\n",
    "            x_model,\n",
    "            y_model - 2 * sigma_model,\n",
    "            y_model + 2 * sigma_model,\n",
    "            color=\"lightgray\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"Input, \")\n",
    "    ax.set_ylabel(\"Output, \")\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Generate true function\n",
    "x_func = torch.linspace(0, 1.0, 100)\n",
    "y_func = true_function(x_func)\n",
    "\n",
    "# Generate some data points\n",
    "torch.manual_seed(1)\n",
    "sigma_func = 0.3\n",
    "n_data = 15\n",
    "x_data, y_data = generate_data(n_data, sigma_func)\n",
    "\n",
    "# Plot the function, data and uncertainty\n",
    "plot_function(x_func, y_func, x_data, y_data, sigma_func=sigma_func)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7X8Mby61xrX"
   },
   "source": [
    "We also provide codes for the solution for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1741741822737,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "rqpfKyCQ104T",
    "outputId": "565b5f24-7dbc-4c3d-81eb-3790be073057"
   },
   "outputs": [],
   "source": [
    "# Define model -- beta is a scalar and omega has size n_hidden,1\n",
    "def network(x, beta, omega):\n",
    "    # Retrieve number of hidden units\n",
    "    n_hidden = omega.shape[0]\n",
    "\n",
    "    y = torch.zeros_like(x)\n",
    "    for c_hidden in range(n_hidden):\n",
    "        # Evaluate activations based on shifted lines (figure 8.4b-d)\n",
    "        line_vals = x - c_hidden / n_hidden\n",
    "        h = line_vals * (line_vals > 0)\n",
    "        # Weight activations by omega parameters and sum\n",
    "        y = y + omega[c_hidden] * h\n",
    "    # Add bias, beta\n",
    "    return y + beta\n",
    "\n",
    "\n",
    "# This fits the n_hidden+1 parameters (see fig 8.4a) in closed form.\n",
    "# If you have studied linear algebra, then you will know it is a least\n",
    "# squares solution of the form (design_matrix^TA)^-1A^Tb.  If you don't recognize that,\n",
    "# then just take it on trust that this gives you the best possible solution.\n",
    "def fit_model_closed_form(x, y, n_hidden):\n",
    "    n_data = len(x)\n",
    "    design_matrix = torch.ones((n_data, n_hidden + 1))\n",
    "    for i in range(n_data):\n",
    "        for j in range(1, n_hidden + 1):\n",
    "            design_matrix[i, j] = x[i] - (j - 1) / n_hidden\n",
    "            design_matrix[i, j] = max(design_matrix[i, j], 0)\n",
    "\n",
    "    beta_omega = torch.linalg.lstsq(design_matrix, y).solution\n",
    "\n",
    "    beta = beta_omega[0]\n",
    "    omega = beta_omega[1:]\n",
    "\n",
    "    return beta, omega\n",
    "\n",
    "\n",
    "# Closed form solution\n",
    "beta, omega = fit_model_closed_form(x_data, y_data, n_hidden=3)\n",
    "\n",
    "# Get prediction for model across graph range\n",
    "x_model = torch.linspace(0, 1, 100)\n",
    "y_model = network(x_model, beta, omega)\n",
    "\n",
    "# Draw the function and the model\n",
    "plot_function(x_func, y_func, x_data, y_data, x_model, y_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYp_MQAS1Te7"
   },
   "source": [
    "## Part B (i) Bias-Variance Trade-Off\n",
    "\n",
    "A very important aspect of machine learning is understanding bias-variance trade-off. As we know, there are three sources of error in our modeling: (i) bias, (ii) variance and (iii) noise (irreducible part).\n",
    "\n",
    "*Rule of thumb: Model with higher complexity will lower bias at cost of higher variance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBm8zMQe17cn"
   },
   "source": [
    "### Problem 2a: Model Mean and Variance Helper Function\n",
    "\n",
    "The function repeats the experiment `n_datasets` times, each time drawing a random dataset of size `n_data` with noise level `sigma_fun` and fits it with a simple model with `n_hidden` weights and a bias term (no activation). It then computes mean and standard deviation of the estimated model over 100 equispaced points in $[0,1]$ (`x_model`). This gives an estimate of the bias and variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFxIhJCN19qu"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Run the model many times with different datasets and return the mean and variance\n",
    "def get_model_mean_variance(n_data, n_datasets, n_hidden, sigma_func):\n",
    "    # Create array that stores model results in rows\n",
    "    y_model_all = torch.zeros((n_datasets, x_model.shape[0]))\n",
    "\n",
    "    for c_dataset in range(n_datasets):\n",
    "        # TODO -- Generate n_data x,y, pairs with standard deviation sigma_func\n",
    "        # Replace this line\n",
    "        x_data, y_data = generate_data(n_data, sigma_func)\n",
    "\n",
    "        # TODO -- Fit the model\n",
    "        # Replace this line:\n",
    "        beta, omega = fit_model_closed_form(x_data, y_data, n_hidden=n_hidden)\n",
    "\n",
    "        # TODO -- Run the fitted model on x_model\n",
    "        # Replace this line\n",
    "        y_model = network(x_model, beta, omega)\n",
    "\n",
    "        # Store the model results\n",
    "        y_model_all[c_dataset, :] = y_model\n",
    "\n",
    "    # Get mean and standard deviation of model\n",
    "    mean_model = torch.mean(y_model_all, dim=0)\n",
    "    std_model = torch.std(y_model_all, dim=0)\n",
    "\n",
    "    # Return the mean and standard deviation of the fitted model\n",
    "    return mean_model, std_model\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 2a\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 2a\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0zBD1rp2Cj9"
   },
   "source": [
    "### Problem 2b: Recreate Figure 8.6\n",
    "\n",
    "Use 100 repetitions for each of sample size 6, 10, 100 (using $\\sigma=0.3$ and `n_hidden`=3). Plot the bias and variances in 3 plots side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1741741826395,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "i7iO7i5N2DIc",
    "outputId": "64802d7e-9791-46fa-d14f-09e65f8d8124"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Generate N random data sets, fit the model N times\n",
    "n_datasets = 100\n",
    "sigma_func = 0.3\n",
    "n_hidden = 3\n",
    "\n",
    "# Get mean and variance of fitted model\n",
    "torch.manual_seed(1)\n",
    "samples = [6, 10, 100]\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "for i in range(3):\n",
    "    mean_model, std_model = get_model_mean_variance(samples[i], n_datasets, n_hidden, sigma_func)\n",
    "    plot_function(\n",
    "        x_func,\n",
    "        y_func,\n",
    "        x_model=x_model,\n",
    "        y_model=mean_model,\n",
    "        sigma_model=std_model,\n",
    "        ax=ax[i],\n",
    "    )\n",
    "    ax[i].set_title(f\"{samples[i]} samples\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 2b\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 2b\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxk_d9I52K7i"
   },
   "source": [
    "### Problem 2c: Recreate Figure 8.7\n",
    "\n",
    "Use 100 repetitions for each of `n_hidden` 3, 5, 10 (using $\\sigma=0.3$ and `n_data`=10). Plot the bias and variances in 3 plots side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1741741828322,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "O8ZuAv9Y2Le_",
    "outputId": "31f15ca5-8ab8-4d55-c571-7bd3ef79fdeb"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Generate N random data sets, fit the model N times\n",
    "n_datasets = 100\n",
    "sigma_func = 0.3\n",
    "n_data = 20\n",
    "\n",
    "# Get mean and variance of fitted model\n",
    "torch.manual_seed(2)\n",
    "hidden = [3, 5, 10]\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "for i in range(3):\n",
    "    mean_model, std_model = get_model_mean_variance(n_data, n_datasets, hidden[i], sigma_func)\n",
    "    plot_function(\n",
    "        x_func,\n",
    "        y_func,\n",
    "        x_model=x_model,\n",
    "        y_model=y_model,\n",
    "        sigma_model=std_model,\n",
    "        ax=ax[i],\n",
    "    )\n",
    "    ax[i].set_title(f\"{hidden[i]} regions\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 2c\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 2c\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2HnzTzh2T3l"
   },
   "source": [
    "### Problem 2d: Recreate Figure 8.9\n",
    "\n",
    "Plot bias and variance terms as a function of\n",
    "the model capacity (number of hidden units)\n",
    "in the simplified model using setting from previous problem with `n_data`=15. Use 100 repetitions for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1741741830098,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "w8kYMI1J2UN4",
    "outputId": "2c9b84ef-9296-48d7-d5bc-69a003a0f355"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Plot the noise, bias and variance as a function of capacity\n",
    "hidden_variables = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "bias = torch.zeros(len(hidden_variables))\n",
    "variance = torch.zeros(len(hidden_variables))\n",
    "\n",
    "n_datasets = 100\n",
    "n_data = 15\n",
    "sigma_func = 0.3\n",
    "n_hidden = 5\n",
    "\n",
    "# Set random seed so that we get the same result every time\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for c_hidden in range(len(hidden_variables)):\n",
    "    # Get mean and variance of fitted model\n",
    "    mean_model, std_model = get_model_mean_variance(\n",
    "        n_data, n_datasets, hidden_variables[c_hidden], sigma_func\n",
    "    )\n",
    "    # TODO -- Estimate bias and variance\n",
    "    # Replace these lines\n",
    "    # Compute variance (avg squared deviation of fitted models)\n",
    "    variance[c_hidden] = torch.mean(std_model**2)\n",
    "    # Compute bias (average squared deviation of mean fitted model around true function)\n",
    "    bias[c_hidden] = torch.mean((mean_model - y_func) ** 2)\n",
    "\n",
    "# Plot the results\n",
    "_fig, ax = plt.subplots()\n",
    "ax.plot(hidden_variables, variance, label=\"variance\", color=\"mediumaquamarine\")\n",
    "ax.plot(hidden_variables, bias, label=\"bias\", color=\"sandybrown\")\n",
    "ax.plot(\n",
    "    hidden_variables,\n",
    "    variance + bias,\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"gray\",\n",
    "    label=\"bias+variance\",\n",
    ")\n",
    "ax.set_xlim(1, 12)\n",
    "ax.set_ylim(0, 0.4)\n",
    "ax.set_xlabel(\"Model capacity\")\n",
    "ax.set_ylabel(\"Mean squared error\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 2d\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 2d\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PX8pnXI2bGY"
   },
   "source": [
    "## Part B (ii) Ensemble Methods\n",
    "\n",
    "This section investigates how ensembling can improve the performance of models. We'll work with the same ground truth and neural network model as in part B (i) which we can fit in closed form, and so we can eliminate any errors due to not finding the global maximum.\n",
    "\n",
    "We start with a baseline model using `n_hidden`=14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1741741832016,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "iG1iJ9vd3xXu",
    "outputId": "26315415-f1de-4d5d-a874-a23130b942d4"
   },
   "outputs": [],
   "source": [
    "# Generate true function\n",
    "x_func = torch.linspace(0, 1.0, 100)\n",
    "y_func = true_function(x_func)\n",
    "\n",
    "# Generate some data points\n",
    "torch.manual_seed(1)\n",
    "sigma_func = 0.3\n",
    "n_data = 15\n",
    "x_data, y_data = generate_data(n_data, sigma_func)\n",
    "\n",
    "# Closed form solution\n",
    "beta, omega = fit_model_closed_form(x_data, y_data, n_hidden=14)\n",
    "\n",
    "# Get prediction for model across graph range\n",
    "x_model = torch.linspace(0, 1, 100)\n",
    "y_model = network(x_model, beta, omega)\n",
    "\n",
    "# Draw the function and the model\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plot_function(x_func, y_func, x_data, y_data, x_model, y_model, ax=ax)\n",
    "plt.title(\"Single Model\")\n",
    "plt.show()\n",
    "\n",
    "# Compute MSE between fitted model and true curve\n",
    "mean_sq_error = torch.mean((y_model - y_func) * (y_model - y_func))\n",
    "print(f\"Mean square error = {mean_sq_error:3.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvdI3RJw2-w6"
   },
   "source": [
    "### Problem 3a: Ensembling 10 Models\n",
    "\n",
    "Let `n_model`=10 be the number of models used. Each model will use the same architecture (in this case controlled via the `n_hidden` parameter, as before) explicitly `n_hidden`=14. However, each model will be trained on a bootstrapped sample (a sample of size $n$ taken *with replacement* from the original training data, also of size $n$).\n",
    "\n",
    "Complete the code chunk below to achieve this and collect the results in `all_y_model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1741741833189,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "W_2rq5Di2_Kw",
    "outputId": "166317c9-c2ce-41c7-a890-c9d5ee2890af"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Now let's resample the data with replacement four times.\n",
    "n_model = 10\n",
    "# Array to store the prediction from all of our models\n",
    "all_y_model = torch.zeros((n_model, len(y_model)))\n",
    "\n",
    "# For each model\n",
    "for i, c_model in enumerate(range(n_model)):\n",
    "    # TODO Sample data indices with replacement (use torch.randint)\n",
    "    # Replace this line\n",
    "    resampled_indices = torch.randint(0, n_data, (n_data,))\n",
    "\n",
    "    # Extract the resampled x and y data\n",
    "    x_data_resampled = x_data[resampled_indices]\n",
    "    y_data_resampled = y_data[resampled_indices]\n",
    "\n",
    "    # Fit the model\n",
    "    beta, omega = fit_model_closed_form(x_data_resampled, y_data_resampled, n_hidden=14)\n",
    "\n",
    "    # Run the model\n",
    "    y_model_resampled = network(x_model, beta, omega)\n",
    "\n",
    "    # Store the results\n",
    "    all_y_model[c_model, :] = y_model_resampled\n",
    "\n",
    "    # Compute MSE between fitted model and true curve\n",
    "    mean_sq_error = torch.mean((y_model_resampled - y_func) * (y_model_resampled - y_func))\n",
    "    print(f\"Model {i}: Mean square error = {mean_sq_error:3.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 3a\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 3a\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXVXkcpY45FE"
   },
   "source": [
    "### Problem 3b: Aggregation\n",
    "\n",
    "Now, we have results from 10 different models. Thus at each $x$, we have 10 different predictions. To aggregate these, one can use **mean** or **median** (for classification task, this can be a majority vote). For both of these aggregation methods, compute the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1741741835172,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "2dD-h8s-45cp",
    "outputId": "78f59217-a391-4397-e1af-32b911820681"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Replace this line\n",
    "y_model_median = torch.median(all_y_model, dim=0).values\n",
    "y_model_mean = torch.mean(all_y_model, dim=0)\n",
    "\n",
    "# Compute the mean squared error between the fitted model and the true curve\n",
    "mean_sq_error = torch.mean((y_model_median - y_func) * (y_model_median - y_func))\n",
    "print(f\"Mean square error for Median = {mean_sq_error:3.3f}\")\n",
    "\n",
    "mean_sq_error = torch.mean((y_model_mean - y_func) * (y_model_mean - y_func))\n",
    "print(f\"Mean square error for Mean = {mean_sq_error:3.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 3b\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 3b\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1741741836614,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "C3zhjSPQ5XSG",
    "outputId": "98ca2c47-32e8-4253-9889-e01fb1b3c3f5"
   },
   "outputs": [],
   "source": [
    "# Draw the function and the model\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "plot_function(x_func, y_func, x_data, y_data, x_model, y_model_median, ax=ax[0])\n",
    "ax[0].set_title(\"Median\")\n",
    "\n",
    "plot_function(x_func, y_func, x_data, y_data, x_model, y_model_mean, ax=ax[1])\n",
    "ax[1].set_title(\"Mean\")\n",
    "\n",
    "plt.suptitle(\"Ensemble Methods\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62v3KBlT6Psp"
   },
   "source": [
    "You should see that both the median and mean models are better than most of the individual models. We have improved our performance at the cost of ten times as much training time, storage, and inference time. Note in the plots how much of the overfitting is also eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mC_f82P0DJP"
   },
   "source": [
    "# Part C: MNIST 1-D Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b1MgDpA1vQ5"
   },
   "source": [
    "The MNIST 1-D Dataset is a 1-dimensional version of MNIST digit dataset - you can check details [here](https://github.com/greydanus/mnist1d). Each digit image is now represented as a vector (1-d) with 40 features. We do not need to get into details about how this was created, rather we take the dataset as given. The only thing to keep in mind is that this is slightly harder dataset compared to the usual MNIST. The first part of the group work focus on this dataset and coming up with a good deep neural classifier for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37416,
     "status": "ok",
     "timestamp": 1741903345642,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "W686MME0z-uQ",
    "outputId": "f7d7613e-0ce6-4f97-9a4c-403bfc8a1514"
   },
   "outputs": [],
   "source": [
    "# Run this if you're in a Colab to install MNIST 1D repository\n",
    "%pip install git+https://github.com/greydanus/mnist1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUS99ot91s1o"
   },
   "source": [
    "Let's generate a training and test dataset using the MNIST1D code. The dataset gets saved as a .pkl file so it doesn't have to be regenerated each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9012,
     "status": "ok",
     "timestamp": 1741741871602,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "lqoZgrcc1pTd",
    "outputId": "fb380266-787f-4a85-f6bc-9cafdb77cff9"
   },
   "outputs": [],
   "source": [
    "import mnist1d\n",
    "\n",
    "args = mnist1d.data.get_dataset_args()\n",
    "data = mnist1d.data.get_dataset(args, path=\"./mnist1d_data.pkl\", download=False, regenerate=False)\n",
    "\n",
    "# The training and test input and outputs are in\n",
    "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
    "print(\"Examples in training set: {}\".format(len(data[\"y\"])))\n",
    "print(\"Examples in test set: {}\".format(len(data[\"y_test\"])))\n",
    "print(\"Length of each example: {}\".format(data[\"x\"].shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYal-fQv3DEC"
   },
   "source": [
    "Let us visualize the dataset in 2-d using PCA and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85139,
     "status": "ok",
     "timestamp": 1741741960404,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "mIUpYeE77fvI",
    "outputId": "d174dfa1-7a1f-4783-d3c1-b5503f17b16d"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist.data = torch.flatten(mnist.data, start_dim=1)\n",
    "\n",
    "mnist_labels = mnist.targets\n",
    "idx = torch.randperm(60000)[:4000]\n",
    "mnist_subset = mnist.data[idx]\n",
    "mnist_labels = mnist_labels[idx]\n",
    "\n",
    "X = data[\"x\"]\n",
    "labels = data[\"y\"]\n",
    "\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "X_tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3).fit_transform(X)\n",
    "mnist_pca = PCA(n_components=2).fit_transform(mnist_subset)\n",
    "mnist_tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3).fit_transform(\n",
    "    mnist_subset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2107,
     "status": "ok",
     "timestamp": 1741741971854,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "z8G_KR8N3mvS",
    "outputId": "dc5db66a-a5e6-4c50-c22a-2c5a6bee06d0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\n",
    "ax[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[0, 1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[1, 0].scatter(mnist_pca[:, 0], mnist_pca[:, 1], c=mnist_labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[1, 1].scatter(mnist_tsne[:, 0], mnist_tsne[:, 1], c=mnist_labels, alpha=0.3, cmap=\"hsv\")\n",
    "ax[0, 0].set_ylabel(\"MNIST 1D\")\n",
    "ax[1, 0].set_ylabel(\"MNIST original\")\n",
    "\n",
    "ax[0, 0].set_title(\"PCA\")\n",
    "ax[0, 1].set_title(\"t-SNE\")\n",
    "plt.suptitle(\"2D Visualization of MNIST 1-d and original (color by label)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghr8GdKE7Gbh"
   },
   "source": [
    "As seen above, there is not much separation between the classes (at least in this two-dimensional view). Compare this to that of the original MNIST data (at least the t-SNE) - hence the classification task on the 1-d version is expected to be harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3tzwfYx1wG1"
   },
   "outputs": [],
   "source": [
    "def weights_init(layer_in):\n",
    "    # Initialize the parameters with He initialization\n",
    "    # Replace this line (see figure 7.8 of book for help)\n",
    "    if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer_in.weight)\n",
    "        layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u7g6zupMWx3"
   },
   "source": [
    "## Performance on MNSIT 1-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc2Ps_4nw66i"
   },
   "source": [
    "### Problem 4a: Training Function for MNIST-1D\n",
    "\n",
    " The `verbose` parameter can be toggled to either print loss/error through the training process or not.\n",
    "\n",
    "Hint: Scheduler can be used in torch as `StepLR(optimizer, step_size, gamma)` and its function is to reduce the learning rate by fraction `gamma` every `step_size` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Vuxuhl510w4"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def train(\n",
    "    model,\n",
    "    weights_init,\n",
    "    data,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    momentum,\n",
    "    decay=0,\n",
    "    schedule_params=(10, 0.5),\n",
    "    n_epoch=50,\n",
    "    *,\n",
    "    verbose=True,\n",
    "):\n",
    "    # choose cross entropy loss function (equation 5.24)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # construct SGD optimizer and initialize learning rate and momentum\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=decay\n",
    "    )\n",
    "\n",
    "    # object that decreases learning rate by half every 10 epochs\n",
    "    # schedule_params = (step_size, gamma) for StepLR object\n",
    "    scheduler = StepLR(optimizer, step_size=schedule_params[0], gamma=schedule_params[1])\n",
    "\n",
    "    # set up data\n",
    "    x_train = torch.tensor(data[\"x\"].astype(\"float32\"))\n",
    "    y_train = torch.tensor(data[\"y\"].transpose().astype(\"int64\"))\n",
    "    x_test = torch.tensor(data[\"x_test\"].astype(\"float32\"))\n",
    "    y_test = torch.tensor(data[\"y_test\"].astype(\"int64\"))\n",
    "\n",
    "    # load the data into a class that creates the batches\n",
    "    data_loader = DataLoader(\n",
    "        TensorDataset(x_train, y_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Initialize model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # loop over the dataset n_epoch times\n",
    "    # store the loss and the % correct at each epoch\n",
    "    losses_train = torch.zeros(n_epoch)\n",
    "    errors_train = torch.zeros(n_epoch)\n",
    "    losses_test = torch.zeros(n_epoch)\n",
    "    errors_test = torch.zeros(n_epoch)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        # loop over batches\n",
    "        for _i, batch in enumerate(data_loader):\n",
    "            # retrieve inputs and labels for this batch\n",
    "            x_batch, y_batch = batch\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass -- calculate model output\n",
    "            pred = model(x_batch)\n",
    "            # compute the loss\n",
    "            loss = loss_function(pred, y_batch)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # SGD update\n",
    "            optimizer.step()\n",
    "\n",
    "        # Run whole dataset to get statistics -- normally wouldn't do this\n",
    "        pred_train = model(x_train)\n",
    "        pred_test = model(x_test)\n",
    "        _, predicted_train_class = torch.max(pred_train.data, 1)\n",
    "        _, predicted_test_class = torch.max(pred_test.data, 1)\n",
    "        errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(\n",
    "            y_train\n",
    "        )\n",
    "        errors_test[epoch] = 100 - 100 * (predicted_test_class == y_test).float().sum() / len(\n",
    "            y_test\n",
    "        )\n",
    "        losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
    "        losses_test[epoch] = loss_function(pred_test, y_test).item()\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, \"\n",
    "                f\"train error {errors_train[epoch]:3.2f},  \"\n",
    "                f\"test loss {losses_test[epoch]:.6f}, \"\n",
    "                f\"test error {errors_test[epoch]:3.2f}\"\n",
    "            )\n",
    "\n",
    "        # tell scheduler to consider updating learning rate\n",
    "        scheduler.step()\n",
    "    return losses_train, errors_train, losses_test, errors_test\n",
    "\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4a\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4a\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "executionInfo": {
     "elapsed": 8881,
     "status": "ok",
     "timestamp": 1741742502369,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "BaD3P6eC5ZMf",
    "outputId": "9ec290a8-6fc9-4b94-fd49-b4ac2d78c60a"
   },
   "outputs": [],
   "source": [
    "# consider the following baseline\n",
    "D_i = 40  # Input dimensions\n",
    "D_k = 100  # Hidden dimensions\n",
    "D_o = 10  # Output dimensions\n",
    "\n",
    "model = nn.Sequential(nn.Linear(D_i, D_k), nn.ReLU(), nn.Linear(D_k, D_o))\n",
    "\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=data,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.05,\n",
    "    momentum=0.9,\n",
    "    schedule_params=(10, 0.5),\n",
    "    n_epoch=100,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "n_epoch = len(losses_train)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "ax[0].plot(errors_train, \"r-\", label=\"train\")\n",
    "ax[0].plot(errors_test, \"b-\", label=\"test\")\n",
    "ax[0].set_ylim(0, 100)\n",
    "ax[0].set_xlim(0, n_epoch)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Error\")\n",
    "ax[0].set_title(f\"Train Error {errors_train[-1]:3.2f}%, Test Error {errors_test[-1]:3.2f}%\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "ax[1].plot(losses_train, \"r-\", label=\"train\")\n",
    "ax[1].plot(losses_test, \"b-\", label=\"test\")\n",
    "ax[1].set_xlim(0, n_epoch)\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(f\"Train loss {losses_train[-1]:3.2f}, Test loss {losses_test[-1]:3.2f}\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9qRQrKprIQl"
   },
   "source": [
    "There are several tuning knobs in the above for improving the performance:\n",
    "1. Model architecture (number of layers, number of nodes in a layer, activation function, etc.)\n",
    "2. Data (batch size)\n",
    "3. Optimizer choices (learning rate, momentum, decreasing learning rate `scheduler`)\n",
    "4. Regularizer (adding dropout layer or using `weight decay` for $L_2$ regularization)\n",
    "\n",
    "Consider the model above as the baseline, which gives a test error of just above 40% (i.e., test accuracy just below 60%) and test loss of around 1.1. Can you do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4b: Tune MLP for 65% Accuracy\n",
    "\n",
    "Note: https://github.com/greydanus/mnist1d mentions for MLP the benchmark is 68% accuracy.\n",
    "\n",
    "Plot the training and test loss and error (as before) - ensure you do not visibly see significant overfitting (aka test loss increasing too much)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51426,
     "status": "ok",
     "timestamp": 1741742091719,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "WLYMrzSs5f-X",
    "outputId": "78780e7a-b61b-4f97-c63a-16249f204786"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "D_k = 500\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_i, D_k), nn.ELU(), nn.Linear(D_k, D_k), nn.ELU(), nn.Linear(D_k, D_o)\n",
    ")\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=data,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.05,\n",
    "    momentum=0.9,\n",
    "    decay=0.001,\n",
    "    schedule_params=(20, 0.7),\n",
    "    n_epoch=150,\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4b\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4b\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1741742097794,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "EuSh0IjvssWP",
    "outputId": "5f1550e0-f727-4eb7-b340-47cc67188abb"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "n_epoch = len(losses_train)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 3))\n",
    "ax[0].plot(errors_train, \"r-\", label=\"train\")\n",
    "ax[0].plot(errors_test, \"b-\", label=\"test\")\n",
    "ax[0].set_ylim(0, 100)\n",
    "ax[0].set_xlim(0, n_epoch)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Error\")\n",
    "ax[0].set_title(f\"Train Error {errors_train[-1]:3.2f}%, Test Error {errors_test[-1]:3.2f}%\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "ax[1].plot(losses_train, \"r-\", label=\"train\")\n",
    "ax[1].plot(losses_test, \"b-\", label=\"test\")\n",
    "ax[1].set_xlim(0, n_epoch)\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(f\"Train loss {losses_train[-1]:3.2f}, Test loss {losses_test[-1]:3.2f}\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVVbJcJUKpRk"
   },
   "source": [
    "## Augmentation with MNIST 1-D\n",
    "\n",
    "This part investigates data augmentation for the MNIST-1D model. Data augmentation is a commonly used method for generating more synthetic training samples by applying simple transformations (e.g. translation, inverting, scaling, filters, rotation) - for images at least, given an image of a dog e.g., a human can classify this even if the image is zoomed, rotated, irrespective of location of the dog in the image or any filters applied. This is the intuition behind data augmentation.\n",
    "\n",
    "Again, for baseline model, we use the previously used `baseline` (with 2 linear layers and 100 hidden nodes) - recall, for this model, we achieved a test loss of around 1.14 and a test error of around 42%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSsDfdV5Pe65"
   },
   "outputs": [],
   "source": [
    "D_i = 40  # Input dimensions\n",
    "D_k = 100  # Hidden dimensions\n",
    "D_o = 10  # Output dimensions\n",
    "\n",
    "model = nn.Sequential(nn.Linear(D_i, D_k), nn.ReLU(), nn.Linear(D_k, D_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8XX1uFbMQIX"
   },
   "source": [
    "### Problem 4c: Augment Function\n",
    "\n",
    "Complete the following function which takes a sample $x$ (as `input_vector`) and applies some transformations and returns another vector `data_out`. For this problem, we apply two transformations:\n",
    "\n",
    "1. Shift $K$ places to the right:\n",
    "\n",
    "$$(x_1,x_2,\\dots,x_n) \\mapsto (x_{n-K+1}, \\dots, x_n, x_1, x_2, \\dots, x_{n-K})$$\n",
    "\n",
    "Note that the first coordinate $x_1$ (at python index 0) originally is $(K+1)$th position (python index $K$) and this is done cyclically, so points that go off the end are added back to the beginning.\n",
    "\n",
    "For example, $n=4, K=2$: $(x_1,x_2,x_3,x_4)\\mapsto (x_3,x_4,x_1,x_2)$.\n",
    "\n",
    "2. Scaling: Scale by a random number drawn from uniform over (0.8, 1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1741742515262,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "JuY5s6_LYhAy",
    "outputId": "192395e8-a139-476a-88de-f7fc374af2bf"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def augment(input_vector):\n",
    "    # Create output vector\n",
    "    input_tensor = (\n",
    "        torch.tensor(input_vector, dtype=torch.float32)\n",
    "        if not isinstance(input_vector, torch.Tensor)\n",
    "        else input_vector.clone()\n",
    "    )\n",
    "    n = len(input_tensor)\n",
    "    data_out = torch.zeros_like(input_tensor)\n",
    "\n",
    "    # TODO:  Shift the input data by a random offset\n",
    "    # (rotating, so points that would go off the end, are added back to the beginning)\n",
    "    k = torch.randint(0, n, (1,)).item()\n",
    "    data_out[k:] = input_tensor[: (n - k)]\n",
    "    data_out[:k] = input_tensor[(n - k) :]\n",
    "\n",
    "    # TODO: Randomly scale data by factor from uniform [0.8, 1.2]\n",
    "    # Replace this line:\n",
    "    scale = 0.9 + 0.2 * torch.rand(1).item()\n",
    "    return data_out * scale\n",
    "\n",
    "\n",
    "# example\n",
    "augment([1, 2, 3, 4])\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4c\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4c\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLFrF1tkPS8k"
   },
   "source": [
    "Let us construct augment our original training data with such transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVaOKTeEOmxq"
   },
   "outputs": [],
   "source": [
    "n_data_orig = data[\"x\"].shape[0]\n",
    "# We'll double the amount of data\n",
    "n_data_augment = n_data_orig * 2\n",
    "augmented_x = torch.zeros((n_data_augment, D_i))\n",
    "augmented_y = torch.zeros(n_data_augment)\n",
    "# First n_data_orig rows are original data\n",
    "augmented_x[0:n_data_orig, :] = torch.tensor(data[\"x\"], dtype=torch.float32)\n",
    "augmented_y[0:n_data_orig] = torch.tensor(data[\"y\"], dtype=torch.float32)\n",
    "\n",
    "# Fill in rest of with augmented data\n",
    "for c_augment in range(n_data_orig, n_data_augment):\n",
    "    # Choose a data point randomly\n",
    "    random_data_index = torch.randint(0, n_data_orig - 1, (1,)).item()\n",
    "    # Augment the point and store\n",
    "    augmented_x[c_augment, :] = augment(\n",
    "        torch.tensor(data[\"x\"][random_data_index, :], dtype=torch.float32)\n",
    "    )\n",
    "    augmented_y[c_augment] = data[\"y\"][random_data_index]\n",
    "\n",
    "# to use the train function we created above\n",
    "augmented_data = {\n",
    "    \"x\": augmented_x.numpy(),\n",
    "    \"y\": augmented_y.numpy(),\n",
    "    \"x_test\": data[\"x_test\"],\n",
    "    \"y_test\": data[\"y_test\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkKqXtozP_n9"
   },
   "source": [
    "### Problem 4d: Training on Augmented Data\n",
    "\n",
    "Use the `train` function to train the data, using the same tuning knobs as used in the baseline case. What is the test loss and error now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16404,
     "status": "ok",
     "timestamp": 1741742790249,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "reTZvaiSOo86",
    "outputId": "5447a4fa-478f-4ba6-9e72-14eac92414e5"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=augmented_data,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.05,\n",
    "    momentum=0.9,\n",
    "    schedule_params=(10, 0.5),\n",
    "    n_epoch=100,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4d\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4d\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyhNFFBhQttm"
   },
   "source": [
    "### Problem 4e: Achieve 70% Accuracy\n",
    "\n",
    "Report the final test accuracy - Get it above 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2owV19JTQabO"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def augment(input_vector):\n",
    "    # Create output vector\n",
    "    input_tensor = (\n",
    "        torch.tensor(input_vector, dtype=torch.float32)\n",
    "        if not isinstance(input_vector, torch.Tensor)\n",
    "        else input_vector.clone()\n",
    "    )\n",
    "    n = len(input_tensor)\n",
    "    data_out = torch.zeros_like(input_tensor)\n",
    "\n",
    "    k = torch.randint(0, n, (1,)).item()\n",
    "    data_out[k:] = input_tensor[: (n - k)]\n",
    "    data_out[:k] = input_tensor[(n - k) :]\n",
    "    scale = 0.95 + 0.1 * torch.rand(n)\n",
    "    return data_out * scale\n",
    "\n",
    "\n",
    "n_data_orig = data[\"x\"].shape[0]\n",
    "\n",
    "# We'll double the amount of data\n",
    "n_data_augment = int(n_data_orig * 1.5)\n",
    "augmented_x = torch.zeros((n_data_augment, D_i))\n",
    "augmented_y = torch.zeros(n_data_augment)\n",
    "\n",
    "# First n_data_orig rows are original data\n",
    "augmented_x[0:n_data_orig, :] = torch.tensor(data[\"x\"], dtype=torch.float32)\n",
    "augmented_y[0:n_data_orig] = torch.tensor(data[\"y\"], dtype=torch.float32)\n",
    "\n",
    "# Fill in rest of with augmented data\n",
    "for c_augment in range(n_data_orig, n_data_augment):\n",
    "    # Choose a data point randomly\n",
    "    random_data_index = torch.randint(0, n_data_orig - 1, (1,)).item()\n",
    "    # Augment the point and store\n",
    "    augmented_x[c_augment, :] = augment(\n",
    "        torch.tensor(data[\"x\"][random_data_index, :], dtype=torch.float32)\n",
    "    )\n",
    "    augmented_y[c_augment] = data[\"y\"][random_data_index]\n",
    "\n",
    "# to use the train function we created above\n",
    "augmented_data = {\n",
    "    \"x\": augmented_x.numpy(),\n",
    "    \"y\": augmented_y.numpy(),\n",
    "    \"x_test\": data[\"x_test\"],\n",
    "    \"y_test\": data[\"y_test\"],\n",
    "}\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert True, \"Solution implemented for 4e\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Hidden tests for 4e\n",
    "assert True\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163910,
     "status": "ok",
     "timestamp": 1741743132268,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 240
    },
    "id": "9EgTod6RRD1F",
    "outputId": "6ca3ed68-36c6-4a8f-a5f4-1a4c88860352"
   },
   "outputs": [],
   "source": [
    "D_k = 500\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_i, D_k),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(D_k, D_k),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(D_k, D_k),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(D_k, D_o),\n",
    ")\n",
    "losses_train, errors_train, losses_test, errors_test = train(\n",
    "    model=model,\n",
    "    weights_init=weights_init,\n",
    "    data=augmented_data,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9,\n",
    "    decay=0.005,\n",
    "    schedule_params=(20, 0.8),\n",
    "    n_epoch=200,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"Test Accuracy = {(100 - errors_test[-1]):.3f}%, Test loss = {losses_test[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaVZwXwtRJDq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1xKm2zHRKAw-pi7L7EhflRzOI9rq4l1t0",
     "timestamp": 1743176602017
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
