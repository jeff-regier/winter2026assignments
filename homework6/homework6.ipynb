{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 315, Homework 6: Training Models with PyTorch\n",
    "\n",
    "In this homework, you will use PyTorch to train models. There are 3 problems:\n",
    "1. Training a support vector machine (SVM) using PyTorch for simple synthetic data.\n",
    "2. Training a shallow neural network for regression on California housing data.\n",
    "3. Training a deep neural network for classification on digits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Support Vector Machine\n",
    "\n",
    "This problem will test your understanding of using `torch` tensors and automatic differentiation in `torch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1a: Generate Data\n",
    "\n",
    "Generate $n=100$ sample points $(x_i, y_i), i=1,\\dots,n$ where $x_i\\in \\mathbb R^2$ is drawn from the standard bivariate normal distribution. For each such $x_i$, generate $y_i\\in \\{-1,1\\}$ such that\n",
    "\n",
    "$$y_i = \\begin{cases}\n",
    " -1 & x_{i1} > \\frac{3}{4} x_{i2} \\\\\n",
    " +1 & x_{i1} \\leq \\frac{3}{4} x_{i2}.\n",
    "\\end{cases}$$\n",
    "\n",
    "Generate this without using `for`/`while` loops, and without using `numpy`: your code should use PyTorch vectorization. `features` should be of shape $(n,2)$ and `labels` should be of shape $(n,)$.\n",
    "\n",
    "Also, create a scatter plot showing each training sample $x_i$ colored by its response $y_i$.\n",
    "\n",
    "Note: The class labels here are $-1$ and $1$, not $0$ and $1$. The transformation $y\\mapsto 2y-1$ can be used to map 0/1 labels to -1/1 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "n = 100\n",
    "# BEGIN SOLUTION\n",
    "features = torch.randn(n, 2)\n",
    "labels = 2 * (features[:, 0] <= 0.75 * features[:, 1]).int() - 1\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(features[:, 0], features[:, 1], c=labels)\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert features.shape == (\n",
    "    100,\n",
    "    2,\n",
    "), f\"features should have shape (100, 2), got {features.shape}\"\n",
    "assert labels.shape == (100,), f\"labels should have shape (100,), got {labels.shape}\"\n",
    "assert set(labels.unique().tolist()) == {-1, 1}, \"labels should only contain -1 and 1\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert features.dtype == torch.float32, \"features should be float32\"\n",
    "assert (labels == 1).sum() + (labels == -1).sum() == 100, \"All labels should be -1 or 1\"\n",
    "# Check the labeling rule: y_i = 1 iff x_i1 <= 0.75 * x_i2\n",
    "expected_labels = 2 * (features[:, 0] <= 0.75 * features[:, 1]).int() - 1\n",
    "assert torch.all(labels == expected_labels), \"Labels do not follow the specified rule\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b: SVM Loss\n",
    "\n",
    "A support vector machine (SVM) is fitted using *hinge loss,* which is given by\n",
    "\n",
    "$$L(Y, \\widehat{Y}) = \\sum_{i=1}^n \\max \\{0, 1 - y_i \\widehat{y}_i\\}.$$\n",
    "\n",
    "For a linear SVM, the prediction is given as\n",
    "\n",
    "$$\\widehat{y}_i = w_0 + w_1 x_{i1} + w_2 x_{i2}$$\n",
    "\n",
    "Using PyTorch, compute loss for a linear SVM. Your function should take the parameters $w=(w_0,w_1,w_2)$ and data `features`, `labels` as input and output $L(Y, \\hat{Y}(X))$.\n",
    "\n",
    "**Hint:** Be consistent with your shapes for `labels` and $\\widehat{y}$. The parameter $w$ is provided as a $(3,)$ shaped tensor (not a $(3,1)$ tensor). Note `features` is passed as a tensor of shape $(n,2)$.\n",
    "\n",
    "**Hint:** Use `torch.maximum` or `torch.clamp` to compute the max with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(weights, features, labels):\n",
    "    # BEGIN SOLUTION\n",
    "    # Prepend column of ones to features for the bias term\n",
    "    ones_column = torch.ones((features.shape[0], 1))\n",
    "    features_with_bias = torch.cat((ones_column, features), dim=1)\n",
    "    predictions = features_with_bias @ weights\n",
    "    return torch.sum(torch.maximum(1 - labels * predictions, torch.tensor(0.0)))\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "w_test = torch.tensor([0.2, 0.5, -0.3])\n",
    "loss_test = svm_loss(w_test, features, labels)\n",
    "assert loss_test.shape == (), f\"Loss should be a scalar, got shape {loss_test.shape}\"\n",
    "assert loss_test.item() > 0, \"Loss should be positive for this test case\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test with good weights (loss should be relatively small)\n",
    "w_good = torch.tensor([0.0, -0.8, 0.6])\n",
    "loss_good = svm_loss(w_good, features, labels)\n",
    "assert loss_good.item() < 50, \"Loss with good weights should be relatively small\"\n",
    "# Test that loss is non-negative\n",
    "assert loss_test.item() >= 0, \"Hinge loss should be non-negative\"\n",
    "# Check that loss is in reasonable range for these weights\n",
    "assert 100 < loss_test.item() < 200, f\"Loss should be in range (100, 200), got {loss_test.item()}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1c: Training via Gradient Descent\n",
    "\n",
    "Write the following `train_svm` function, taking `features` and `labels` as input and using gradient descent to train the parameters (here just $w$). Use automatic differentiation to get gradients. The function also takes as input `eta` (learning rate / step size) and `epochs` (the number of iterations). There is no need to check for convergence. Return the final value of parameter $w$ and a list of the loss evaluated at each iteration.\n",
    "\n",
    "Do not use `torch.optim` for this problem. Instead, implement gradient descent directly.\n",
    "\n",
    "**Hint:** Set up the parameters to enable gradient computations with `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(features, labels, eta=1e-1, epochs=1000):\n",
    "    _n, d = features.shape\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    # Initialize weights randomly with gradient tracking\n",
    "    weights = torch.randn(d + 1, requires_grad=True)\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for _i in range(epochs):\n",
    "        # Compute loss\n",
    "        loss = svm_loss(weights, features, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Gradient descent update (no gradient tracking)\n",
    "        with torch.no_grad():\n",
    "            weights -= eta * weights.grad\n",
    "            weights.grad.zero_()\n",
    "        # Record loss\n",
    "        losses.append(loss.detach().item())\n",
    "    # END SOLUTION\n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "torch.manual_seed(42)\n",
    "w_trained, losses = train_svm(features, labels)\n",
    "assert len(losses) == 1000, f\"Should have 1000 loss values, got {len(losses)}\"\n",
    "assert losses[-1] < losses[0], \"Loss should decrease during training\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert w_trained.shape == (3,), f\"w should have shape (3,), got {w_trained.shape}\"\n",
    "assert w_trained.requires_grad, \"w should have requires_grad=True\"\n",
    "# Check that final loss is reasonably small\n",
    "assert losses[-1] < 10, f\"Final loss should be small, got {losses[-1]}\"\n",
    "# Check that the learned weights approximate the true boundary\n",
    "w_norm = w_trained.detach() / torch.linalg.norm(w_trained.detach())\n",
    "# The true boundary is x1 = 0.75 * x2, so w should be proportional to [0, -0.8, 0.6]\n",
    "assert abs(w_norm[1].item() + 0.8) < 0.2, \"w1 should be close to -0.8 (normalized)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify trained weights\n",
    "w_trained, losses = train_svm(features, labels)\n",
    "print(f\"first loss = {losses[0]:.3f} and last loss = {losses[-1]:.3f}\")\n",
    "w_detached = w_trained.detach()\n",
    "print(f\"optimal w (normalized) = {w_detached / torch.linalg.norm(w_detached)}\")\n",
    "\n",
    "w_true = torch.tensor([0.0, -4.0, 3.0])\n",
    "print(f\"original w (normalized) = {w_true / torch.linalg.norm(w_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Shallow Network Regression\n",
    "\n",
    "In this problem, you will train a shallow neural network on the California housing dataset using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "california_housing = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "housing_features = california_housing[0]\n",
    "housing_prices = california_housing[1]\n",
    "train_features_unscaled, test_features_unscaled, train_prices, test_prices = train_test_split(\n",
    "    housing_features, housing_prices, test_size=0.3, random_state=42\n",
    ")\n",
    "sc = StandardScaler()\n",
    "train_features_scaled = sc.fit_transform(train_features_unscaled)\n",
    "test_features_scaled = sc.transform(test_features_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to tensors\n",
    "train_features = torch.tensor(train_features_scaled, dtype=torch.float32)\n",
    "test_features = torch.tensor(test_features_scaled, dtype=torch.float32)\n",
    "train_prices = torch.tensor(train_prices.values, dtype=torch.float32)\n",
    "test_prices = torch.tensor(test_prices.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a: Sizes\n",
    "\n",
    "Fill in the input dimension and output dimension of the shallow neural network for regression on this dataset. For this homework, we will choose a hidden layer with dimension double that of the input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "input_dim = train_features.shape[1]\n",
    "hidden_dim = 2 * input_dim\n",
    "output_dim = 1\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert input_dim == 8, f\"Input dimension should be 8, got {input_dim}\"\n",
    "assert hidden_dim == 16, f\"Hidden dimension should be 16, got {hidden_dim}\"\n",
    "assert output_dim == 1, f\"Output dimension should be 1 for regression, got {output_dim}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert input_dim == train_features.shape[1], \"input_dim should match number of features\"\n",
    "assert hidden_dim == 2 * input_dim, \"hidden_dim should be 2 * input_dim\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: Initialization\n",
    "\n",
    "Define `torch` tensors for weights `W1`, `b1`, `W2`, and `b2`. Initialize both biases `b1` and `b2` to be zero vectors and initialize `W1` and `W2` by picking values uniformly at random from the interval $[0,1]$.\n",
    "\n",
    "Note: All variables should enable gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "W1 = torch.rand((input_dim, hidden_dim), requires_grad=True)\n",
    "W2 = torch.rand((hidden_dim, output_dim), requires_grad=True)\n",
    "b1 = torch.zeros(hidden_dim, requires_grad=True)\n",
    "b2 = torch.zeros(output_dim, requires_grad=True)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert W1.shape == (\n",
    "    input_dim,\n",
    "    hidden_dim,\n",
    "), f\"W1 shape should be ({input_dim}, {hidden_dim})\"\n",
    "assert W2.shape == (\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "), f\"W2 shape should be ({hidden_dim}, {output_dim})\"\n",
    "assert b1.shape == (hidden_dim,), f\"b1 shape should be ({hidden_dim},)\"\n",
    "assert b2.shape == (output_dim,), f\"b2 shape should be ({output_dim},)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert W1.requires_grad, \"W1 should have requires_grad=True\"\n",
    "assert W2.requires_grad, \"W2 should have requires_grad=True\"\n",
    "assert b1.requires_grad, \"b1 should have requires_grad=True\"\n",
    "assert b2.requires_grad, \"b2 should have requires_grad=True\"\n",
    "assert torch.all(b1 == 0), \"b1 should be initialized to zeros\"\n",
    "assert torch.all(b2 == 0), \"b2 should be initialized to zeros\"\n",
    "assert torch.all((W1 >= 0) & (W1 <= 1)), \"W1 values should be in [0, 1]\"\n",
    "assert torch.all((W2 >= 0) & (W2 <= 1)), \"W2 values should be in [0, 1]\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c: Model and Loss\n",
    "\n",
    "(i) Complete the `model` function below to define a shallow network. The `inputs` parameter is a matrix of shape $n \\times d$, where the $i$th row contains $x_i^\\intercal$. It should return a `torch` tensor of shape $(n,)$ containing the output of the network.\n",
    "\n",
    "**Hint:** Use `torch.nn.ReLU()` for activation.\n",
    "\n",
    "(ii) Write a function that computes mean-squared error given the predictions and targets. Make sure to use `torch` operations only for autodiff to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs):\n",
    "    # BEGIN SOLUTION\n",
    "    hidden = torch.nn.ReLU()(inputs @ W1 + b1)\n",
    "    return hidden @ W2 + b2\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def mse(predictions, targets):\n",
    "    # BEGIN SOLUTION\n",
    "    sq_error = torch.square(predictions.squeeze() - targets)\n",
    "    return torch.mean(sq_error)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "test_output = model(train_features[:10])\n",
    "assert test_output.shape[0] == 10, f\"Model output should have 10 rows, got {test_output.shape[0]}\"\n",
    "test_mse = mse(torch.tensor([1.0, 2.0, 3.0]), torch.tensor([1.0, 2.0, 4.0]))\n",
    "assert abs(test_mse.item() - 1 / 3) < 0.01, f\"MSE calculation incorrect, got {test_mse.item()}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Test model output shape\n",
    "full_output = model(train_features)\n",
    "assert full_output.shape[0] == train_features.shape[0], \"Model should output one value per input\"\n",
    "# Test MSE with known values\n",
    "mse_test = mse(torch.tensor([0.0, 0.0]), torch.tensor([1.0, 1.0]))\n",
    "assert abs(mse_test.item() - 1.0) < 0.01, \"MSE of [0,0] vs [1,1] should be 1.0\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2d: Training via Batches\n",
    "\n",
    "For this problem, complete the following code for training the model via batches. We are not using `DataLoader` to create batches—instead, we will shuffle the training data at each epoch and go through it $B$ samples at a time (where $B=500$ is the batch size). Use Stochastic Gradient Descent from `torch` to perform the optimization by filling in the following code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "num_samples = train_features.shape[0]\n",
    "num_batches = num_samples // batch_size  # how many iterations within each epoch\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.SGD(params=[W1, b1, W2, b2], lr=learning_rate)\n",
    "# END SOLUTION\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle data\n",
    "    random_shuffle_idx = torch.randperm(num_samples)\n",
    "    train_features = train_features[random_shuffle_idx]\n",
    "    train_prices = train_prices[random_shuffle_idx]\n",
    "\n",
    "    # Training within epoch through batches\n",
    "    # BEGIN SOLUTION\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        pred = model(train_features[batch_start:batch_end, :])\n",
    "        loss = mse(pred, train_prices[batch_start:batch_end])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # END SOLUTION\n",
    "\n",
    "    # Evaluation\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss = mse(model(train_features), train_prices)\n",
    "            print(f\"Loss at step {epoch}: {loss.detach().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "with torch.no_grad():\n",
    "    final_train_loss = mse(model(train_features), train_prices).item()\n",
    "assert (\n",
    "    final_train_loss < 3.0\n",
    "), f\"Training loss should be below 3.0 after training, got {final_train_loss}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert final_train_loss < 2.0, f\"Training loss should be below 2.0, got {final_train_loss}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2e: Evaluation on Test Data\n",
    "\n",
    "Compute the loss on the test data and print the value. Make sure no gradient computations are done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_features)\n",
    "    test_loss = mse(predictions, test_prices)\n",
    "    print(f\"Loss on test data = {test_loss.detach().item():.4f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert test_loss.item() < 3.0, f\"Test loss should be below 3.0, got {test_loss.item()}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert test_loss.item() < 2.0, f\"Test loss should be below 2.0, got {test_loss.item()}\"\n",
    "assert test_loss.item() > 0.1, f\"Test loss suspiciously low: {test_loss.item()}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Deep Network Classification\n",
    "\n",
    "In this problem, you will train a two-layer network using `torch` for classification on the `digits` dataset, which consists of handwritten digits (as $8\\times 8$ grayscale images) and their labels (0–9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the digits dataset\n",
    "digit_images, digit_labels = datasets.load_digits(return_X_y=True)\n",
    "print(f\"Number of samples = {len(digit_images)}\")\n",
    "print(f\"Target classes = {torch.unique(digit_labels)}\")\n",
    "digit_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(digit_images[i].reshape(8, 8), cmap=\"gray\")\n",
    "    ax.set_title(f\"Sample: {digit_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "digit_images = sc.fit_transform(digit_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3a: DataLoader\n",
    "\n",
    "Split the data into train and test sets using `sklearn` (keep 30% data for test). Set up dataloader objects for batch-training. Use batch size $32$. Keep `shuffle=True` so that each epoch the training data are shuffled.\n",
    "\n",
    "**Hint:** `DataLoader` takes a `torch` `TensorDataset` object—so first need to create that. For that, convert everything to proper `torch` tensors (of correct data types—for `nn.CrossEntropyLoss` the target must have data type `torch.long`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "digit_images = torch.tensor(digit_images, dtype=torch.float32)\n",
    "digit_labels = torch.tensor(digit_labels, dtype=torch.long)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    digit_images, digit_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "training_data = TensorDataset(train_images, train_labels)\n",
    "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert len(train_images) == 1257, f\"train_images should have 1257 samples, got {len(train_images)}\"\n",
    "assert len(test_images) == 540, f\"test_images should have 540 samples, got {len(test_images)}\"\n",
    "assert (\n",
    "    train_labels.dtype == torch.long\n",
    "), f\"train_labels should be torch.long, got {train_labels.dtype}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    train_images.dtype == torch.float32\n",
    "), f\"train_images should be float32, got {train_images.dtype}\"\n",
    "assert len(training_data) == len(train_images), \"TensorDataset length should match train_images\"\n",
    "# Check dataloader batch size\n",
    "first_batch = next(iter(train_dataloader))\n",
    "assert first_batch[0].shape[0] == 32, f\"Batch size should be 32, got {first_batch[0].shape[0]}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3b: Setting up Model\n",
    "\n",
    "Build a `torch` model using `nn.Sequential` by adding appropriate layers (Linear and activation). For this problem use ReLU activation. The first hidden layer should have $128$ units and the second should have $64$ units.\n",
    "\n",
    "**Hint:** Are the images already flattened?\n",
    "\n",
    "**Hint:** Set up the shapes correctly. The target has 10 classes.\n",
    "\n",
    "**How many total parameters are there in the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=64, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64, out_features=10),\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "# END SOLUTION\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert total_params == 17226, f\"Total parameters should be 17226, got {total_params}\"\n",
    "test_input = torch.randn(5, 64)\n",
    "test_out = model(test_input)\n",
    "assert test_out.shape == (\n",
    "    5,\n",
    "    10,\n",
    "), f\"Model output shape should be (5, 10), got {test_out.shape}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check model structure\n",
    "assert len(list(model.children())) == 5, \"Model should have 5 layers (3 Linear + 2 ReLU)\"\n",
    "# Check first linear layer\n",
    "first_layer = next(iter(model.children()))\n",
    "assert first_layer.in_features == 64, \"First layer input should be 64\"\n",
    "assert first_layer.out_features == 128, \"First layer output should be 128\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3c: Setting up Loss and Optimizer\n",
    "\n",
    "Set up the appropriate loss function (from `torch`) and initialize the SGD optimizer with correct trainable parameters. Use learning rate 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert isinstance(loss_fn, nn.CrossEntropyLoss), \"Should use CrossEntropyLoss for classification\"\n",
    "assert isinstance(optimizer, optim.SGD), \"Should use SGD optimizer\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "# Check learning rate\n",
    "assert (\n",
    "    optimizer.defaults[\"lr\"] == 0.1\n",
    "), f\"Learning rate should be 0.1, got {optimizer.defaults['lr']}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3d: Training\n",
    "\n",
    "Train the model for 100 epochs by filling in parts of the following code chunk. It should display the average loss over the training data every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    epoch_training_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    # BEGIN SOLUTION\n",
    "    for inputs, target in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss value\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    avg_train_loss = epoch_training_loss / len(train_dataloader)\n",
    "    # END SOLUTION\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: average training loss = {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert avg_train_loss < 0.1, f\"Final average training loss should be < 0.1, got {avg_train_loss}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    avg_train_loss < 0.05\n",
    "), f\"Training loss should be very small after 100 epochs, got {avg_train_loss}\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3e: Test Accuracy\n",
    "\n",
    "Compute the test accuracy in evaluation mode (i.e., no gradient computations). Accuracy is defined as the proportion of correct predictions (this is not the same as the Cross Entropy loss that you were using as the loss function for optimization above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_images)\n",
    "    correct = (outputs.argmax(dim=1) == test_labels).sum().item()\n",
    "    test_accuracy = correct / len(test_labels)\n",
    "    print(f\"Test accuracy = {100 * test_accuracy:.2f}%\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert test_accuracy > 0.90, f\"Test accuracy should be > 90%, got {100 * test_accuracy:.2f}%\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert test_accuracy > 0.95, f\"Test accuracy should be > 95%, got {100 * test_accuracy:.2f}%\"\n",
    "assert test_accuracy <= 1.0, \"Test accuracy cannot exceed 100%\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
