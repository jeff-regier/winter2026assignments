{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4U8hhGomCCm"
   },
   "source": [
    "# DATASCI 315, Group Work 4: PyTorch Training Pipeline and FashionMNIST\n",
    "\n",
    "In this group-work assignment, we will start working with PyTorch. The learning objectives for this assignment include gaining familiarity with:\n",
    "\n",
    "1. Tensors in PyTorch\n",
    "2. Automatic differentiation (autograd) in PyTorch\n",
    "3. Deep networks in PyTorch\n",
    "\n",
    "**Instructions:** During lab section, and afterward as necessary, you will collaborate in two-person teams (assigned by the GSI) to complete the problems that are interspersed below. The GSI will help individual teams encountering difficulty, make announcements addressing common issues, and help ensure progress for all teams. *During lab, feel free to flag down your GSI to ask questions at any point!* Upon completion, one member of the team should submit their team's work through Canvas as HTML.\n",
    "\n",
    "**Resources:**\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3wu4DfGm4P7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nrILX5Ydm4C"
   },
   "source": [
    "## Deep Networks for Regression\n",
    "\n",
    "We go over the basic steps to build a (moderately) deep network for a regression problem and train it using PyTorch's autograd. Things to learn in this section:\n",
    "\n",
    "1. Creating a `DataLoader` object (to help with training)\n",
    "2. How to use `torch.nn` layers (linear and activation) and build a sequential model\n",
    "3. How to compute loss and optimize using `torch.optim`\n",
    "\n",
    "Let us first look at a regression problem with synthetic data where the goal is to train a 3-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2005,
     "status": "ok",
     "timestamp": 1739482980266,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "EDofXj8cYpFE",
    "outputId": "45b29cb6-5702-4fd7-a8bc-1b1b25eae1af"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_samples, num_features = 1000, 3\n",
    "torch.manual_seed(10)\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (\n",
    "    2 * torch.sin(3 * X[:, 0])\n",
    "    - 3 * torch.cos(-4 * X[:, 1])\n",
    "    + 2.4 * X[:, 2]\n",
    "    + torch.randn(num_samples)\n",
    ")\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS3gGw6qVw34"
   },
   "source": [
    "### Using DataLoader\n",
    "\n",
    "The `DataLoader` class can be imported from `torch.utils.data` and is a convenient way to create batches while keeping $(X, y)$ pairs together. `DataLoader` takes a `torch` dataset object (which can be created from raw data using `TensorDataset`) or a built-in `torch` dataset (we will see an example later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1739482980266,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "e21xLSFLejGE",
    "outputId": "cca1fc96-c61d-40e4-c60e-e734dc9c60cb"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch_idx, (features, targets) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}: features shape: {features.shape}, targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxZrYb9Mfwwy"
   },
   "source": [
    "Note there are $B=25$ batches now, each of size `batch_size` ($M=32$). In each batch, $X$ has shape $(32, 3)$ where the first dimension corresponds to the number of samples in the batch and the second corresponds to the number of features. The batch also contains the corresponding $y$ values. To see the number of samples in the entire dataset, use the `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739482980266,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "LV0_YUxtQsHS",
    "outputId": "e90cb2a0-b3c3-42f9-a360-2ad3ecbae0b8"
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "we_kThkAgJvT"
   },
   "source": [
    "### Building a Sequential Model\n",
    "\n",
    "Building a sequential model (where the output from one layer is passed as the input to the next layer) is done using `torch.nn.Sequential`.\n",
    "\n",
    "For fully connected layers, you will need:\n",
    "1. Linear layers via `torch.nn.Linear`\n",
    "2. Activation functions from `torch.nn`\n",
    "\n",
    "The `Linear` layer is similar to the class `Linear` we saw in Group Work 2, except that it (i) allows for more general shaped tensors and (ii) parameters associated with this layer enable automatic gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739482980267,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "TaZrjYhOgxp4",
    "outputId": "c7cb67e1-6ae5-4614-b4ca-f57694e4632d"
   },
   "outputs": [],
   "source": [
    "# A single linear layer\n",
    "print(f\"num_features = {num_features}\")\n",
    "layer1 = nn.Linear(in_features=num_features, out_features=10)\n",
    "\n",
    "# Passing the full data through the layer\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "yhat = layer1(X)\n",
    "print(f\"Shape of output of X through this layer: {yhat.shape}\")\n",
    "\n",
    "# Passing a batch through the layer\n",
    "for batch_idx, (features, _targets) in enumerate(dataloader):\n",
    "    yhat = layer1(features)\n",
    "    print(f\"Batch {batch_idx}: shape of output through this layer: {yhat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Vj1FH5iv2_"
   },
   "source": [
    "Such layers can be combined as follows. Suppose we want to design a neural network with 2 hidden layers:\n",
    "\n",
    "$$x \\rightarrow z_1 \\rightarrow z_2 \\rightarrow y$$\n",
    "\n",
    "where (denoting the activation function as $a$ acting coordinate-wise)\n",
    "\n",
    "\\begin{align}\n",
    "  x &\\in \\mathbb{R}^d \\\\\n",
    "  z_1 &= a(b_1 + W_1 x) \\in \\mathbb{R}^{d_1} \\\\\n",
    "  z_2 &= a(b_2 + W_2 z_1) \\in \\mathbb{R}^{d_2} \\\\\n",
    "  y &= b_3 + W_3 z_2 \\in \\mathbb{R}.\n",
    "\\end{align}\n",
    "\n",
    "That is, the first hidden layer has $d_1$ nodes and the second has $d_2$. $W_1, b_1$ are the weights and bias for the first layer, and similarly for the other parameters. Here is how to build this model. Recall the ReLU activation can be used from `torch.nn.ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739482980267,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "bGMwK7GMg87K",
    "outputId": "0ef08bdd-6adc-4e1b-c6c6-cac5a5f2013f"
   },
   "outputs": [],
   "source": [
    "hidden_dim_1 = 10\n",
    "hidden_dim_2 = 5\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(num_features, hidden_dim_1),  # input has d features, output has d_1\n",
    "    nn.ReLU(),  # activation function\n",
    "    nn.Linear(hidden_dim_1, hidden_dim_2),  # input has d_1 features, output has d_2\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim_2, 1),  # input has d_2 features, output has 1 (as y is scalar)\n",
    ")\n",
    "\n",
    "# Let us look at the trainable parameters\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHFzSUf2kjP9"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. The first two are $W_1$ and $b_1$ with shapes $(d_1, d)$ and $(d_1,)$ respectively, associated with the first layer.\n",
    "2. The next two are $W_2$ and $b_2$, and so on.\n",
    "\n",
    "Note that they all have `requires_grad` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739482980267,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "FiYUFkmswSar",
    "outputId": "642cd6cb-8bbe-4639-dddd-524bdeec1ae8"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QU36UUek8jE"
   },
   "source": [
    "### Problem 1: Shapes and Parameters\n",
    "\n",
    "(a) What is the shape of $W_2$ and $b_2$?\n",
    "\n",
    "(b) What is the total number of parameters in this model?\n",
    "\n",
    "Explain your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5zHoQ_WlJ1z"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# (a) W_2 has shape (d_2, d_1) = (5, 10) and b_2 has shape (d_2,) = (5,).\n",
    "#\n",
    "# (b) Total parameters:\n",
    "#   Layer 1: d * d_1 + d_1 = 3 * 10 + 10 = 40\n",
    "#   Layer 2: d_1 * d_2 + d_2 = 10 * 5 + 5 = 55\n",
    "#   Layer 3: d_2 * 1 + 1 = 5 + 1 = 6\n",
    "#   Total = 40 + 55 + 6 = 101\n",
    "\n",
    "w2_shape = (hidden_dim_2, hidden_dim_1)\n",
    "b2_shape = (hidden_dim_2,)\n",
    "total_params = sum(parameter.numel() for parameter in model.parameters())\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert w2_shape == (5, 10), f\"Expected W_2 shape (5, 10), got {w2_shape}\"\n",
    "assert b2_shape == (5,), f\"Expected b_2 shape (5,), got {b2_shape}\"\n",
    "assert total_params == 101, f\"Expected 101 total parameters, got {total_params}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert w2_shape[0] == hidden_dim_2, \"W_2 rows should equal hidden_dim_2\"\n",
    "assert w2_shape[1] == hidden_dim_1, \"W_2 cols should equal hidden_dim_1\"\n",
    "assert len(b2_shape) == 1, \"b_2 should be 1-dimensional\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH9eNrnnl2YZ"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "This is similar to the optimization routine we saw above for simple linear regression. The only difference is letting the optimizer know which parameters to optimize over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eL2_QFAXmADo"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 200\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for _epoch in range(num_epochs):\n",
    "    for inputs, target in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Compute the loss for train and test\n",
    "    with torch.no_grad():\n",
    "        y_hat_train = model(X_train)\n",
    "        train_loss.append(loss_fn(y_hat_train, y_train).detach().item())\n",
    "        y_hat_test = model(X_test)\n",
    "        test_loss.append(loss_fn(y_hat_test, y_test).detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739482986726,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "1muREH-krH0y",
    "outputId": "d42f79fa-7a97-4f10-eb3a-c8bcbadfcbe9"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(test_loss, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wig_PVu4bEi"
   },
   "source": [
    "**Note:** One can keep track of the loss computed within each batch as well, but typically these are updated at every batch within an epoch. One must be careful when computing the training loss, depending on what is needed. The method shown here computes the loss on the whole data at the end of each epoch (this might be very costly with large datasets), and hence a better way is to use the batch losses instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePVNoUw8eTNa"
   },
   "source": [
    "## Deep Networks for Classification\n",
    "\n",
    "This section will walk you through training a classification model with one of PyTorch's built-in datasets. We will be using the FashionMNIST dataset, which contains $28 \\times 28$ grayscale images of clothing articles, each with a label (type of clothing). Let us download the dataset and visualize some samples. Note the `ToTensor` transformation applied to each sample, which converts it to a PyTorch tensor object. One could also apply other transforms directly (like `Resize`, `Normalize`, `CenterCrop`, etc.), which can be composed via `torchvision.transforms.Compose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16974,
     "status": "ok",
     "timestamp": 1739490427837,
     "user": {
      "displayName": "Sunrit Chakraborty",
      "userId": "12695564417859604069"
     },
     "user_tz": 300
    },
    "id": "KoYY7snja9gq",
    "outputId": "140343be-c2a4-493d-baee-591645e7934b"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1739482989211,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "J2XfcK2gRUlF",
    "outputId": "a49016d9-2432-44a4-942c-04e83b567ba5"
   },
   "outputs": [],
   "source": [
    "num_train = len(training_data)\n",
    "num_test = len(test_data)\n",
    "print(f\"Number of training samples: {num_train}\")\n",
    "print(f\"Number of test samples: {num_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1739482989780,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "K2gUK3BMyrq6",
    "outputId": "35a72327-ce74-4e13-91a8-ff5fbec4dce3"
   },
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYDNr7e7y1EK"
   },
   "source": [
    "#### Problem 2a: DataLoaders\n",
    "\n",
    "Create a `DataLoader` object with the training data, using a batch size of 64. Also create one for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3sITix0yyI-"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739482989781,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "Ivw3otgEzUWh",
    "outputId": "08c85159-eff6-4cce-dbff-1cf546cfa511"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "num_batches_train = len(train_dataloader)\n",
    "num_batches_test = len(test_dataloader)\n",
    "assert num_batches_train == 938, f\"Expected 938 train batches, got {num_batches_train}\"\n",
    "assert num_batches_test == 157, f\"Expected 157 test batches, got {num_batches_test}\"\n",
    "\n",
    "# Check that dataloader can be iterated over\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "assert train_features.size() == torch.Size(\n",
    "    [64, 1, 28, 28]\n",
    "), f\"Expected feature shape [64, 1, 28, 28], got {train_features.size()}\"\n",
    "assert train_labels.size() == torch.Size(\n",
    "    [64]\n",
    "), f\"Expected label shape [64], got {train_labels.size()}\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# Display an image and label for verification\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label} corresponding to {labels_map[label.item()]}\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert train_dataloader.batch_size == 64, \"Train dataloader batch size should be 64\"\n",
    "assert test_dataloader.batch_size == 64, \"Test dataloader batch size should be 64\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSqXTfnTztO3"
   },
   "source": [
    "#### Problem 2b: Flattening\n",
    "\n",
    "Before using a sequential model, we need to reshape the features properly. Currently each batch $X$ has shape $(M, 1, 28, 28)$, where $M=64$. We need to flatten each sample to obtain $X$ of shape $(M, 784)$. Use `nn.Flatten` (which can be thought of as another layer) to do this. For this problem, initialize this layer and pass the features from a batch through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSg1G36lzaBR"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "flatten_layer = nn.Flatten()\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_flat_features = flatten_layer(train_features)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1739482990485,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "biXgoQE1Ry8j",
    "outputId": "89b46185-276b-47ae-9402-8ebd86af710d"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert train_features.shape == torch.Size(\n",
    "    [64, 1, 28, 28]\n",
    "), f\"Expected input shape [64, 1, 28, 28], got {train_features.shape}\"\n",
    "assert train_flat_features.shape == torch.Size(\n",
    "    [64, 784]\n",
    "), f\"Expected output shape [64, 784], got {train_flat_features.shape}\"\n",
    "print(f\"Shape of input: {train_features.shape}\")\n",
    "print(f\"Shape of output: {train_flat_features.shape}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert train_flat_features.shape[0] == 64, \"Batch dimension should be preserved\"\n",
    "assert train_flat_features.shape[1] == 28 * 28, \"Features should be flattened to 784\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qFPkNfn1GIK"
   },
   "source": [
    "You can use this as the first layer while building the sequential model. Note that you can think of this as another layer with no trainable parameters (like an activation layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyEDHsaF1Rg2"
   },
   "source": [
    "#### Problem 2c: A Sequential Model\n",
    "\n",
    "Write a sequential model with 2 hidden layers, each with 512 nodes.\n",
    "\n",
    "**Notes:**\n",
    "1. The output should have 10 values (there are 10 target classes).\n",
    "2. Make sure you flatten images first.\n",
    "3. Use ReLU activation after each hidden layer.\n",
    "\n",
    "What is the total number of parameters in this model?\n",
    "\n",
    "**Note:** Since we will be using `CrossEntropyLoss`, which automatically normalizes the outputs, we do not need a separate `softmax` operation on the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etFPZk4w0krL"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Architecture: 784 -> 512 -> 512 -> 10\n",
    "# Total params: (784*512 + 512) + (512*512 + 512) + (512*10 + 10) = 669,706\n",
    "classification_model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0721jwq2Y7s"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "with torch.no_grad():\n",
    "    train_features, train_labels = next(iter(train_dataloader))\n",
    "    predicted_logits = classification_model(train_features)\n",
    "    assert predicted_logits.shape == torch.Size(\n",
    "        [64, 10]\n",
    "    ), f\"Expected output shape [64, 10], got {predicted_logits.shape}\"\n",
    "    print(f\"Shape of predicted_logits: {predicted_logits.shape}\")\n",
    "\n",
    "model_num_params = sum(parameter.numel() for parameter in classification_model.parameters())\n",
    "assert model_num_params == 669706, f\"Expected 669,706 parameters, got {model_num_params}\"\n",
    "print(f\"Total number of parameters: {model_num_params}\")\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert (\n",
    "    len(list(classification_model.parameters())) == 6\n",
    "), \"Model should have 6 parameter tensors (3 weight + 3 bias)\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wth-WxhS2z-E"
   },
   "source": [
    "#### Problem 2d: Loss\n",
    "\n",
    "Initialize the appropriate loss function (`CrossEntropyLoss`) and compute the loss based on `predicted_logits` from above and `train_labels` from the same batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739482990485,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "C04p9xoL2f_4",
    "outputId": "d02443d7-ce18-4b9c-dbfb-3c35e9b27823"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "with torch.no_grad():\n",
    "    initial_loss = classification_loss_fn(predicted_logits, train_labels)\n",
    "print(f\"Initial loss: {initial_loss.item():.3f}\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert initial_loss.item() > 0, \"Loss should be positive\"\n",
    "assert initial_loss.item() < 10, \"Initial loss should be reasonable (< 10)\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert initial_loss.shape == torch.Size([]), \"Loss should be a scalar\"\n",
    "# END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDILtowS3Z9e"
   },
   "source": [
    "#### Problem 2e: Training\n",
    "\n",
    "Use SGD optimizer from `torch.optim` to train the model for 10 epochs. Use learning rate $\\eta = 0.001$. Keep track of both the training and test loss. Also compute the average classification accuracy on the test data.\n",
    "\n",
    "**Hint:** When computing test loss, ensure you use `torch.no_grad()` since we do not want these operations to modify gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150311,
     "status": "ok",
     "timestamp": 1739483140792,
     "user": {
      "displayName": "Andrej Leban",
      "userId": "09327691438298822730"
     },
     "user_tz": 300
    },
    "id": "liwcsxMe3VM2",
    "outputId": "bf3d1822-67b4-4567-fcda-6410a38bb0c2"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "classification_optimizer = optim.SGD(classification_model.parameters(), lr=1e-3)\n",
    "\n",
    "num_classification_epochs = 10\n",
    "classification_train_loss = []\n",
    "classification_test_loss = []\n",
    "classification_test_accuracy = []\n",
    "\n",
    "for epoch in range(num_classification_epochs):\n",
    "    epoch_training_loss = 0\n",
    "    epoch_test_loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_features, batch_labels in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = classification_model(batch_features)\n",
    "        loss = classification_loss_fn(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        classification_optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "        classification_optimizer.zero_grad()\n",
    "\n",
    "        # Accumulate loss\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    # Test evaluation loop\n",
    "    for batch_features, batch_labels in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = classification_model(batch_features)\n",
    "            loss = classification_loss_fn(outputs, batch_labels)\n",
    "            epoch_test_loss += loss.detach().item()\n",
    "            num_correct += (outputs.argmax(dim=1) == batch_labels).sum().item()\n",
    "\n",
    "    classification_train_loss.append(epoch_training_loss / num_batches_train)\n",
    "    classification_test_loss.append(epoch_test_loss / num_batches_test)\n",
    "    classification_test_accuracy.append(num_correct / num_test)\n",
    "    print(\n",
    "        f\"Epoch {epoch}: \"\n",
    "        f\"train loss = {classification_train_loss[-1]:.2f}, \"\n",
    "        f\"test loss = {classification_test_loss[-1]:.2f}, \"\n",
    "        f\"test accuracy = {classification_test_accuracy[-1]:.2f}\"\n",
    "    )\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jlg29xJtTwmc"
   },
   "outputs": [],
   "source": [
    "# Test assertions\n",
    "assert (\n",
    "    len(classification_train_loss) == 10\n",
    "), f\"Expected 10 training loss values, got {len(classification_train_loss)}\"\n",
    "assert (\n",
    "    len(classification_test_loss) == 10\n",
    "), f\"Expected 10 test loss values, got {len(classification_test_loss)}\"\n",
    "assert (\n",
    "    len(classification_test_accuracy) == 10\n",
    "), f\"Expected 10 test accuracy values, got {len(classification_test_accuracy)}\"\n",
    "assert (\n",
    "    classification_test_accuracy[-1] > 0.5\n",
    "), f\"Expected final accuracy > 0.5, got {classification_test_accuracy[-1]:.2f}\"\n",
    "assert classification_train_loss[-1] < classification_train_loss[0], \"Training loss should decrease\"\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "# BEGIN HIDDEN TESTS\n",
    "assert all(\n",
    "    0 <= acc <= 1 for acc in classification_test_accuracy\n",
    "), \"Accuracy should be between 0 and 1\"\n",
    "assert all(loss > 0 for loss in classification_train_loss), \"Loss should be positive\"\n",
    "# END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
